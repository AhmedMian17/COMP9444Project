{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# FLAPPY BIRD GRAPHICS FILES FROM https://github.com/yenchenlin/DeepLearningFlappyBird\n",
    "\n",
    "import pygame\n",
    "import sys\n",
    "def load():\n",
    "    # path of player with different states\n",
    "    PLAYER_PATH = (\n",
    "            'assets/sprites/redbird-upflap.png',\n",
    "            'assets/sprites/redbird-midflap.png',\n",
    "            'assets/sprites/redbird-downflap.png'\n",
    "    )\n",
    "\n",
    "    # path of background\n",
    "    BACKGROUND_PATH = 'assets/sprites/background-black.png'\n",
    "\n",
    "    # path of pipe\n",
    "    PIPE_PATH = 'assets/sprites/pipe-green.png'\n",
    "\n",
    "    IMAGES, SOUNDS, HITMASKS = {}, {}, {}\n",
    "\n",
    "    # numbers sprites for score display\n",
    "    IMAGES['numbers'] = (\n",
    "        pygame.image.load('assets/sprites/0.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/1.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/2.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/3.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/4.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/5.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/6.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/7.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/8.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/9.png').convert_alpha()\n",
    "    )\n",
    "\n",
    "    # base (ground) sprite\n",
    "    IMAGES['base'] = pygame.image.load('assets/sprites/base.png').convert_alpha()\n",
    "\n",
    "    # sounds\n",
    "    if 'win' in sys.platform:\n",
    "        soundExt = '.wav'\n",
    "    else:\n",
    "        soundExt = '.ogg'\n",
    "\n",
    "    SOUNDS['die']    = pygame.mixer.Sound('assets/audio/die' + soundExt)\n",
    "    SOUNDS['hit']    = pygame.mixer.Sound('assets/audio/hit' + soundExt)\n",
    "    SOUNDS['point']  = pygame.mixer.Sound('assets/audio/point' + soundExt)\n",
    "    SOUNDS['swoosh'] = pygame.mixer.Sound('assets/audio/swoosh' + soundExt)\n",
    "    SOUNDS['wing']   = pygame.mixer.Sound('assets/audio/wing' + soundExt)\n",
    "\n",
    "    # select random background sprites\n",
    "    IMAGES['background'] = pygame.image.load(BACKGROUND_PATH).convert()\n",
    "\n",
    "    # select random player sprites\n",
    "    IMAGES['player'] = (\n",
    "        pygame.image.load(PLAYER_PATH[0]).convert_alpha(),\n",
    "        pygame.image.load(PLAYER_PATH[1]).convert_alpha(),\n",
    "        pygame.image.load(PLAYER_PATH[2]).convert_alpha(),\n",
    "    )\n",
    "\n",
    "    # select random pipe sprites\n",
    "    IMAGES['pipe'] = (\n",
    "        pygame.transform.rotate(\n",
    "            pygame.image.load(PIPE_PATH).convert_alpha(), 180),\n",
    "        pygame.image.load(PIPE_PATH).convert_alpha(),\n",
    "    )\n",
    "\n",
    "    # hismask for pipes\n",
    "    HITMASKS['pipe'] = (\n",
    "        getHitmask(IMAGES['pipe'][0]),\n",
    "        getHitmask(IMAGES['pipe'][1]),\n",
    "    )\n",
    "\n",
    "    # hitmask for player\n",
    "    HITMASKS['player'] = (\n",
    "        getHitmask(IMAGES['player'][0]),\n",
    "        getHitmask(IMAGES['player'][1]),\n",
    "        getHitmask(IMAGES['player'][2]),\n",
    "    )\n",
    "\n",
    "    return IMAGES, SOUNDS, HITMASKS\n",
    "\n",
    "def getHitmask(image):\n",
    "    \"\"\"returns a hitmask using an image's alpha.\"\"\"\n",
    "    mask = []\n",
    "    for x in range(image.get_width()):\n",
    "        mask.append([])\n",
    "        for y in range(image.get_height()):\n",
    "            mask[x].append(bool(image.get_at((x,y))[3]))\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# FLAPPY BIRD CODE FROM https://github.com/yenchenlin/DeepLearningFlappyBird\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import random\n",
    "import pygame\n",
    "import pygame.surfarray as surfarray\n",
    "from pygame.locals import *\n",
    "from itertools import cycle\n",
    "\n",
    "FPS = 30\n",
    "SCREENWIDTH  = 288\n",
    "SCREENHEIGHT = 512\n",
    "\n",
    "pygame.init()\n",
    "FPSCLOCK = pygame.time.Clock()\n",
    "SCREEN = pygame.display.set_mode((SCREENWIDTH, SCREENHEIGHT))\n",
    "pygame.display.set_caption('Flappy Bird')\n",
    "\n",
    "IMAGES, SOUNDS, HITMASKS = load()\n",
    "PIPEGAPSIZE = 100 # gap between upper and lower part of pipe\n",
    "BASEY = SCREENHEIGHT * 0.79\n",
    "\n",
    "PLAYER_WIDTH = IMAGES['player'][0].get_width()\n",
    "PLAYER_HEIGHT = IMAGES['player'][0].get_height()\n",
    "PIPE_WIDTH = IMAGES['pipe'][0].get_width()\n",
    "PIPE_HEIGHT = IMAGES['pipe'][0].get_height()\n",
    "BACKGROUND_WIDTH = IMAGES['background'].get_width()\n",
    "\n",
    "PLAYER_INDEX_GEN = cycle([0, 1, 2, 1])\n",
    "\n",
    "\n",
    "class GameState:\n",
    "    def __init__(self):\n",
    "        self.score = self.playerIndex = self.loopIter = 0\n",
    "        self.playerx = int(SCREENWIDTH * 0.2)\n",
    "        self.playery = int((SCREENHEIGHT - PLAYER_HEIGHT) / 2)\n",
    "        self.basex = 0\n",
    "        self.baseShift = IMAGES['base'].get_width() - BACKGROUND_WIDTH\n",
    "\n",
    "        newPipe1 = getRandomPipe()\n",
    "        newPipe2 = getRandomPipe()\n",
    "        self.upperPipes = [\n",
    "            {'x': SCREENWIDTH, 'y': newPipe1[0]['y']},\n",
    "            {'x': SCREENWIDTH + (SCREENWIDTH / 2), 'y': newPipe2[0]['y']},\n",
    "        ]\n",
    "        self.lowerPipes = [\n",
    "            {'x': SCREENWIDTH, 'y': newPipe1[1]['y']},\n",
    "            {'x': SCREENWIDTH + (SCREENWIDTH / 2), 'y': newPipe2[1]['y']},\n",
    "        ]\n",
    "\n",
    "        # player velocity, max velocity, downward accleration, accleration on flap\n",
    "        self.pipeVelX = -4\n",
    "        self.playerVelY    =  0    # player's velocity along Y, default same as playerFlapped\n",
    "        self.playerMaxVelY =  10   # max vel along Y, max descend speed\n",
    "        self.playerMinVelY =  -8   # min vel along Y, max ascend speed\n",
    "        self.playerAccY    =   1   # players downward accleration\n",
    "        self.playerFlapAcc =  -9   # players speed on flapping\n",
    "        self.playerFlapped = False # True when player flaps\n",
    "\n",
    "    def frame_step(self, flap):\n",
    "        pygame.event.pump()\n",
    "\n",
    "        reward = 0.1\n",
    "        terminal = False\n",
    "\n",
    "        if flap:\n",
    "            if self.playery > -2 * PLAYER_HEIGHT:\n",
    "                self.playerVelY = self.playerFlapAcc\n",
    "                self.playerFlapped = True\n",
    "                #SOUNDS['wing'].play()\n",
    "\n",
    "        # check for score\n",
    "        playerMidPos = self.playerx + PLAYER_WIDTH / 2\n",
    "        for pipe in self.upperPipes:\n",
    "            pipeMidPos = pipe['x'] + PIPE_WIDTH / 2\n",
    "            if pipeMidPos <= playerMidPos < pipeMidPos + 4:\n",
    "                self.score += 1\n",
    "                #SOUNDS['point'].play()\n",
    "                reward = 1\n",
    "\n",
    "        # playerIndex basex change\n",
    "        if (self.loopIter + 1) % 3 == 0:\n",
    "            self.playerIndex = next(PLAYER_INDEX_GEN)\n",
    "        self.loopIter = (self.loopIter + 1) % 30\n",
    "        self.basex = -((-self.basex + 100) % self.baseShift)\n",
    "\n",
    "        # player's movement\n",
    "        if self.playerVelY < self.playerMaxVelY and not self.playerFlapped:\n",
    "            self.playerVelY += self.playerAccY\n",
    "        if self.playerFlapped:\n",
    "            self.playerFlapped = False\n",
    "        self.playery += min(self.playerVelY, BASEY - self.playery - PLAYER_HEIGHT)\n",
    "        if self.playery < 0:\n",
    "            self.playery = 0\n",
    "\n",
    "        # move pipes to left\n",
    "        for uPipe, lPipe in zip(self.upperPipes, self.lowerPipes):\n",
    "            uPipe['x'] += self.pipeVelX\n",
    "            lPipe['x'] += self.pipeVelX\n",
    "\n",
    "        # add new pipe when first pipe is about to touch left of screen\n",
    "        if 0 < self.upperPipes[0]['x'] < 5:\n",
    "            newPipe = getRandomPipe()\n",
    "            self.upperPipes.append(newPipe[0])\n",
    "            self.lowerPipes.append(newPipe[1])\n",
    "\n",
    "        # remove first pipe if its out of the screen\n",
    "        if self.upperPipes[0]['x'] < -PIPE_WIDTH:\n",
    "            self.upperPipes.pop(0)\n",
    "            self.lowerPipes.pop(0)\n",
    "\n",
    "        # check if crash here\n",
    "        isCrash= checkCrash({'x': self.playerx, 'y': self.playery,\n",
    "                             'index': self.playerIndex},\n",
    "                            self.upperPipes, self.lowerPipes)\n",
    "        if isCrash:\n",
    "            #SOUNDS['hit'].play()\n",
    "            #SOUNDS['die'].play()\n",
    "            terminal = True\n",
    "            self.__init__()\n",
    "            reward = -1\n",
    "\n",
    "        # draw sprites\n",
    "        SCREEN.blit(IMAGES['background'], (0,0))\n",
    "\n",
    "        for uPipe, lPipe in zip(self.upperPipes, self.lowerPipes):\n",
    "            SCREEN.blit(IMAGES['pipe'][0], (uPipe['x'], uPipe['y']))\n",
    "            SCREEN.blit(IMAGES['pipe'][1], (lPipe['x'], lPipe['y']))\n",
    "\n",
    "        SCREEN.blit(IMAGES['base'], (self.basex, BASEY))\n",
    "        # print score so player overlaps the score\n",
    "        # showScore(self.score)\n",
    "        SCREEN.blit(IMAGES['player'][self.playerIndex],\n",
    "                    (self.playerx, self.playery))\n",
    "\n",
    "        image_data = pygame.surfarray.array3d(pygame.display.get_surface())\n",
    "        pygame.display.update()\n",
    "        FPSCLOCK.tick(FPS)\n",
    "        #print self.upperPipes[0]['y'] + PIPE_HEIGHT - int(BASEY * 0.2)\n",
    "        return image_data, reward, terminal\n",
    "\n",
    "def getRandomPipe():\n",
    "    \"\"\"returns a randomly generated pipe\"\"\"\n",
    "    # y of gap between upper and lower pipe\n",
    "    gapYs = [20, 30, 40, 50, 60, 70, 80, 90]\n",
    "    index = random.randint(0, len(gapYs)-1)\n",
    "    gapY = gapYs[index]\n",
    "\n",
    "    gapY += int(BASEY * 0.2)\n",
    "    pipeX = SCREENWIDTH + 10\n",
    "\n",
    "    return [\n",
    "        {'x': pipeX, 'y': gapY - PIPE_HEIGHT},  # upper pipe\n",
    "        {'x': pipeX, 'y': gapY + PIPEGAPSIZE},  # lower pipe\n",
    "    ]\n",
    "\n",
    "\n",
    "def showScore(score):\n",
    "    \"\"\"displays score in center of screen\"\"\"\n",
    "    scoreDigits = [int(x) for x in list(str(score))]\n",
    "    totalWidth = 0 # total width of all numbers to be printed\n",
    "\n",
    "    for digit in scoreDigits:\n",
    "        totalWidth += IMAGES['numbers'][digit].get_width()\n",
    "\n",
    "    Xoffset = (SCREENWIDTH - totalWidth) / 2\n",
    "\n",
    "    for digit in scoreDigits:\n",
    "        SCREEN.blit(IMAGES['numbers'][digit], (Xoffset, SCREENHEIGHT * 0.1))\n",
    "        Xoffset += IMAGES['numbers'][digit].get_width()\n",
    "\n",
    "\n",
    "def checkCrash(player, upperPipes, lowerPipes):\n",
    "    \"\"\"returns True if player collders with base or pipes.\"\"\"\n",
    "    pi = player['index']\n",
    "    player['w'] = IMAGES['player'][0].get_width()\n",
    "    player['h'] = IMAGES['player'][0].get_height()\n",
    "\n",
    "    # if player crashes into ground\n",
    "    if player['y'] + player['h'] >= BASEY - 1:\n",
    "        return True\n",
    "    else:\n",
    "\n",
    "        playerRect = pygame.Rect(player['x'], player['y'],\n",
    "                      player['w'], player['h'])\n",
    "\n",
    "        for uPipe, lPipe in zip(upperPipes, lowerPipes):\n",
    "            # upper and lower pipe rects\n",
    "            uPipeRect = pygame.Rect(uPipe['x'], uPipe['y'], PIPE_WIDTH, PIPE_HEIGHT)\n",
    "            lPipeRect = pygame.Rect(lPipe['x'], lPipe['y'], PIPE_WIDTH, PIPE_HEIGHT)\n",
    "\n",
    "            # player and upper/lower pipe hitmasks\n",
    "            pHitMask = HITMASKS['player'][pi]\n",
    "            uHitmask = HITMASKS['pipe'][0]\n",
    "            lHitmask = HITMASKS['pipe'][1]\n",
    "\n",
    "            # if bird collided with upipe or lpipe\n",
    "            uCollide = pixelCollision(playerRect, uPipeRect, pHitMask, uHitmask)\n",
    "            lCollide = pixelCollision(playerRect, lPipeRect, pHitMask, lHitmask)\n",
    "\n",
    "            if uCollide or lCollide:\n",
    "                return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def pixelCollision(rect1, rect2, hitmask1, hitmask2):\n",
    "    \"\"\"Checks if two objects collide and not just their rects\"\"\"\n",
    "    rect = rect1.clip(rect2)\n",
    "\n",
    "    if rect.width == 0 or rect.height == 0:\n",
    "        return False\n",
    "\n",
    "    x1, y1 = rect.x - rect1.x, rect.y - rect1.y\n",
    "    x2, y2 = rect.x - rect2.x, rect.y - rect2.y\n",
    "\n",
    "    for x in range(rect.width):\n",
    "        for y in range(rect.height):\n",
    "            if hitmask1[x1+x][y1+y] and hitmask2[x2+x][y2+y]:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flappy no graphics for DQL\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import random\n",
    "import pygame\n",
    "import game.flappy_bird_utils as flappy_bird_utils\n",
    "import pygame.surfarray as surfarray\n",
    "from pygame.locals import *\n",
    "from itertools import cycle\n",
    "\n",
    "FPS = 30\n",
    "SCREENWIDTH  = 288\n",
    "SCREENHEIGHT = 512\n",
    "\n",
    "pygame.init()\n",
    "FPSCLOCK = pygame.time.Clock()\n",
    "SCREEN = pygame.display.set_mode((SCREENWIDTH, SCREENHEIGHT))\n",
    "pygame.display.set_caption('Flappy Bird')\n",
    "\n",
    "IMAGES, SOUNDS, HITMASKS = flappy_bird_utils.load()\n",
    "PIPEGAPSIZE = 100 # gap between upper and lower part of pipe\n",
    "BASEY = SCREENHEIGHT * 0.79\n",
    "\n",
    "PLAYER_WIDTH = IMAGES['player'][0].get_width()\n",
    "PLAYER_HEIGHT = IMAGES['player'][0].get_height()\n",
    "PIPE_WIDTH = IMAGES['pipe'][0].get_width()\n",
    "PIPE_HEIGHT = IMAGES['pipe'][0].get_height()\n",
    "BACKGROUND_WIDTH = IMAGES['background'].get_width()\n",
    "\n",
    "PLAYER_INDEX_GEN = cycle([0, 1, 2, 1])\n",
    "\n",
    "\n",
    "class GameNoGraphics:\n",
    "    def __init__(self):\n",
    "        self.score = self.playerIndex = self.loopIter = 0\n",
    "        self.playerx = int(SCREENWIDTH * 0.2)\n",
    "        self.playery = int((SCREENHEIGHT - PLAYER_HEIGHT) / 2)\n",
    "        self.basex = 0\n",
    "        self.baseShift = IMAGES['base'].get_width() - BACKGROUND_WIDTH\n",
    "\n",
    "        newPipe1 = getRandomPipe()\n",
    "        newPipe2 = getRandomPipe()\n",
    "        self.upperPipes = [\n",
    "            {'x': SCREENWIDTH, 'y': newPipe1[0]['y']},\n",
    "            {'x': SCREENWIDTH + (SCREENWIDTH / 2), 'y': newPipe2[0]['y']},\n",
    "        ]\n",
    "        self.lowerPipes = [\n",
    "            {'x': SCREENWIDTH, 'y': newPipe1[1]['y']},\n",
    "            {'x': SCREENWIDTH + (SCREENWIDTH / 2), 'y': newPipe2[1]['y']},\n",
    "        ]\n",
    "\n",
    "        # player velocity, max velocity, downward accleration, accleration on flap\n",
    "        self.pipeVelX = -4\n",
    "        self.playerVelY    =  0    # player's velocity along Y, default same as playerFlapped\n",
    "        self.playerMaxVelY =  10   # max vel along Y, max descend speed\n",
    "        self.playerMinVelY =  -8   # min vel along Y, max ascend speed\n",
    "        self.playerAccY    =   1   # players downward accleration\n",
    "        self.playerFlapAcc =  -9   # players speed on flapping\n",
    "        self.playerFlapped = False # True when player flaps\n",
    "\n",
    "    def get_next_pipe_index(self):\n",
    "        playerMidPos = self.playerx + PLAYER_WIDTH / 2\n",
    "        distance = [0, 0]\n",
    "        idx = 0\n",
    "        for pipe in self.upperPipes:\n",
    "            if (idx == 2):\n",
    "                break\n",
    "            pipeMidPos = pipe['x'] + PIPE_WIDTH / 2\n",
    "            if pipeMidPos >= playerMidPos:\n",
    "                distance[idx] = pipeMidPos - playerMidPos\n",
    "            else:\n",
    "                distance[idx] = 999\n",
    "            idx += 1\n",
    "        return distance.index(min(distance))\n",
    "\n",
    "\n",
    "    def frame_step(self, flap):\n",
    "        pygame.event.pump()\n",
    "\n",
    "        reward = 0.4\n",
    "        terminal = False\n",
    "\n",
    "        if flap:\n",
    "            if self.playery > -2 * PLAYER_HEIGHT:\n",
    "                self.playerVelY = self.playerFlapAcc\n",
    "                self.playerFlapped = True\n",
    "                #SOUNDS['wing'].play()\n",
    "\n",
    "        next_pipe_idx = self.get_next_pipe_index()\n",
    "        playerMidPos = self.playery + PLAYER_HEIGHT/2\n",
    "        pipeUpperMidPos = self.upperPipes[next_pipe_idx]['y']\n",
    "        pipeLowerMidPos = self.lowerPipes[next_pipe_idx]['y']\n",
    "        # if playerMidPos < pipeUpperMidPos + 1 and playerMidPos > pipeLowerMidPos - 1:\n",
    "        #     reward = 0.2\n",
    "        # if playerMidPos > pipeLowerMidPos - 100 and playerMidPos < pipeLowerMidPos:\n",
    "        #     # print(\"within\")\n",
    "        #     reward = 0.2\n",
    "        if playerMidPos <= 0.2*SCREENHEIGHT:\n",
    "            # print(\"too high\")\n",
    "            reward = -0.2\n",
    "        if playerMidPos <= 0.1*SCREENHEIGHT:\n",
    "            # print(\"too high\")\n",
    "            reward = -0.4\n",
    "\n",
    "\n",
    "        # check for score\n",
    "        playerMidPos = self.playerx + PLAYER_WIDTH / 2\n",
    "        for pipe in self.upperPipes:\n",
    "            pipeMidPos = pipe['x'] + PIPE_WIDTH / 2\n",
    "            if pipeMidPos <= playerMidPos < pipeMidPos + 4:\n",
    "                self.score += 1\n",
    "                #SOUNDS['point'].play()\n",
    "                reward = 10\n",
    "\n",
    "        # playerIndex basex change\n",
    "        if (self.loopIter + 1) % 3 == 0:\n",
    "            self.playerIndex = next(PLAYER_INDEX_GEN)\n",
    "        self.loopIter = (self.loopIter + 1) % 30\n",
    "        self.basex = -((-self.basex + 100) % self.baseShift)\n",
    "\n",
    "        # player's movement\n",
    "        if self.playerVelY < self.playerMaxVelY and not self.playerFlapped:\n",
    "            self.playerVelY += self.playerAccY\n",
    "        if self.playerFlapped:\n",
    "            self.playerFlapped = False\n",
    "        self.playery += min(self.playerVelY, BASEY - self.playery - PLAYER_HEIGHT)\n",
    "        if self.playery < 0:\n",
    "            self.playery = 0\n",
    "\n",
    "        # move pipes to left\n",
    "        for uPipe, lPipe in zip(self.upperPipes, self.lowerPipes):\n",
    "            uPipe['x'] += self.pipeVelX\n",
    "            lPipe['x'] += self.pipeVelX\n",
    "\n",
    "        # add new pipe when first pipe is about to touch left of screen\n",
    "        if 0 < self.upperPipes[0]['x'] < 5:\n",
    "            newPipe = getRandomPipe()\n",
    "            self.upperPipes.append(newPipe[0])\n",
    "            self.lowerPipes.append(newPipe[1])\n",
    "\n",
    "        # remove first pipe if its out of the screen\n",
    "        if self.upperPipes[0]['x'] < -PIPE_WIDTH:\n",
    "            self.upperPipes.pop(0)\n",
    "            self.lowerPipes.pop(0)\n",
    "\n",
    "        # check if crash here\n",
    "        isCrash= checkCrash({'x': self.playerx, 'y': self.playery,\n",
    "                             'index': self.playerIndex},\n",
    "                            self.upperPipes, self.lowerPipes)\n",
    "        if isCrash:\n",
    "            #SOUNDS['hit'].play()\n",
    "            #SOUNDS['die'].play()\n",
    "            terminal = True\n",
    "            self.__init__()\n",
    "            reward = -5\n",
    "\n",
    "        # # draw sprites\n",
    "        # SCREEN.blit(IMAGES['background'], (0,0))\n",
    "\n",
    "        # for uPipe, lPipe in zip(self.upperPipes, self.lowerPipes):\n",
    "        #     SCREEN.blit(IMAGES['pipe'][0], (uPipe['x'], uPipe['y']))\n",
    "        #     SCREEN.blit(IMAGES['pipe'][1], (lPipe['x'], lPipe['y']))\n",
    "\n",
    "        # SCREEN.blit(IMAGES['base'], (self.basex, BASEY))\n",
    "        # # print score so player overlaps the score\n",
    "        # # showScore(self.score)\n",
    "        # SCREEN.blit(IMAGES['player'][self.playerIndex],\n",
    "        #             (self.playerx, self.playery))\n",
    "\n",
    "        # image_data = pygame.surfarray.array3d(pygame.display.get_surface())\n",
    "        # pygame.display.update()\n",
    "        # FPSCLOCK.tick(FPS)\n",
    "        #print self.upperPipes[0]['y'] + PIPE_HEIGHT - int(BASEY * 0.2)\n",
    "        return 0, reward, terminal\n",
    "\n",
    "def getRandomPipe():\n",
    "    \"\"\"returns a randomly generated pipe\"\"\"\n",
    "    # y of gap between upper and lower pipe\n",
    "    gapYs = [20, 30, 40, 50, 60, 70, 80, 90]\n",
    "    index = random.randint(0, len(gapYs)-1)\n",
    "    gapY = gapYs[index]\n",
    "\n",
    "    gapY += int(BASEY * 0.2)\n",
    "    pipeX = SCREENWIDTH + 10\n",
    "\n",
    "    return [\n",
    "        {'x': pipeX, 'y': gapY - PIPE_HEIGHT},  # upper pipe\n",
    "        {'x': pipeX, 'y': gapY + PIPEGAPSIZE},  # lower pipe\n",
    "    ]\n",
    "\n",
    "\n",
    "def showScore(score):\n",
    "    \"\"\"displays score in center of screen\"\"\"\n",
    "    scoreDigits = [int(x) for x in list(str(score))]\n",
    "    totalWidth = 0 # total width of all numbers to be printed\n",
    "\n",
    "    for digit in scoreDigits:\n",
    "        totalWidth += IMAGES['numbers'][digit].get_width()\n",
    "\n",
    "    Xoffset = (SCREENWIDTH - totalWidth) / 2\n",
    "\n",
    "    for digit in scoreDigits:\n",
    "        SCREEN.blit(IMAGES['numbers'][digit], (Xoffset, SCREENHEIGHT * 0.1))\n",
    "        Xoffset += IMAGES['numbers'][digit].get_width()\n",
    "\n",
    "\n",
    "def checkCrash(player, upperPipes, lowerPipes):\n",
    "    \"\"\"returns True if player collders with base or pipes.\"\"\"\n",
    "    pi = player['index']\n",
    "    player['w'] = IMAGES['player'][0].get_width()\n",
    "    player['h'] = IMAGES['player'][0].get_height()\n",
    "\n",
    "    # if player crashes into ground\n",
    "    if player['y'] + player['h'] >= BASEY - 1:\n",
    "        return True\n",
    "    else:\n",
    "\n",
    "        playerRect = pygame.Rect(player['x'], player['y'],\n",
    "                      player['w'], player['h'])\n",
    "\n",
    "        for uPipe, lPipe in zip(upperPipes, lowerPipes):\n",
    "            # upper and lower pipe rects\n",
    "            uPipeRect = pygame.Rect(uPipe['x'], uPipe['y'], PIPE_WIDTH, PIPE_HEIGHT)\n",
    "            lPipeRect = pygame.Rect(lPipe['x'], lPipe['y'], PIPE_WIDTH, PIPE_HEIGHT)\n",
    "\n",
    "            # player and upper/lower pipe hitmasks\n",
    "            pHitMask = HITMASKS['player'][pi]\n",
    "            uHitmask = HITMASKS['pipe'][0]\n",
    "            lHitmask = HITMASKS['pipe'][1]\n",
    "\n",
    "            # if bird collided with upipe or lpipe\n",
    "            uCollide = pixelCollision(playerRect, uPipeRect, pHitMask, uHitmask)\n",
    "            lCollide = pixelCollision(playerRect, lPipeRect, pHitMask, lHitmask)\n",
    "\n",
    "            if uCollide or lCollide:\n",
    "                return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def pixelCollision(rect1, rect2, hitmask1, hitmask2):\n",
    "    \"\"\"Checks if two objects collide and not just their rects\"\"\"\n",
    "    rect = rect1.clip(rect2)\n",
    "\n",
    "    if rect.width == 0 or rect.height == 0:\n",
    "        return False\n",
    "\n",
    "    x1, y1 = rect.x - rect1.x, rect.y - rect1.y\n",
    "    x2, y2 = rect.x - rect2.x, rect.y - rect2.y\n",
    "\n",
    "    for x in range(rect.width):\n",
    "        for y in range(rect.height):\n",
    "            if hitmask1[x1+x][y1+y] and hitmask2[x2+x][y2+y]:\n",
    "                return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# FLAPPY BIRD CODE WITH GRAPHICS REMOVED\n",
    "\n",
    "import random\n",
    "import pygame\n",
    "from itertools import cycle\n",
    "\n",
    "SCREENWIDTH  = 288\n",
    "SCREENHEIGHT = 512\n",
    "\n",
    "IMAGES, SOUNDS, HITMASKS = load()\n",
    "PIPEGAPSIZE = 100 # gap between upper and lower part of pipe\n",
    "BASEY = SCREENHEIGHT * 0.79\n",
    "\n",
    "PLAYER_WIDTH = IMAGES['player'][0].get_width()\n",
    "PLAYER_HEIGHT = IMAGES['player'][0].get_height()\n",
    "PIPE_WIDTH = IMAGES['pipe'][0].get_width()\n",
    "PIPE_HEIGHT = IMAGES['pipe'][0].get_height()\n",
    "BACKGROUND_WIDTH = IMAGES['background'].get_width()\n",
    "\n",
    "PLAYER_INDEX_GEN = cycle([0, 1, 2, 1])\n",
    "\n",
    "\n",
    "class GameStateNoGraphics:\n",
    "    def __init__(self):\n",
    "        self.score = self.playerIndex = self.loopIter = 0\n",
    "        self.playerx = int(SCREENWIDTH * 0.2)\n",
    "        self.playery = int((SCREENHEIGHT - PLAYER_HEIGHT) / 2)\n",
    "        self.basex = 0\n",
    "        self.baseShift = IMAGES['base'].get_width() - BACKGROUND_WIDTH\n",
    "\n",
    "        newPipe1 = getRandomPipe()\n",
    "        newPipe2 = getRandomPipe()\n",
    "        self.upperPipes = [\n",
    "            {'x': SCREENWIDTH, 'y': newPipe1[0]['y']},\n",
    "            {'x': SCREENWIDTH + (SCREENWIDTH / 2), 'y': newPipe2[0]['y']},\n",
    "        ]\n",
    "        self.lowerPipes = [\n",
    "            {'x': SCREENWIDTH, 'y': newPipe1[1]['y']},\n",
    "            {'x': SCREENWIDTH + (SCREENWIDTH / 2), 'y': newPipe2[1]['y']},\n",
    "        ]\n",
    "\n",
    "        # player velocity, max velocity, downward accleration, accleration on flap\n",
    "        self.pipeVelX = -4\n",
    "        self.playerVelY    =  0    # player's velocity along Y, default same as playerFlapped\n",
    "        self.playerMaxVelY =  10   # max vel along Y, max descend speed\n",
    "        self.playerMinVelY =  -8   # min vel along Y, max ascend speed\n",
    "        self.playerAccY    =   1   # players downward accleration\n",
    "        self.playerFlapAcc =  -9   # players speed on flapping\n",
    "        self.playerFlapped = False # True when player flaps\n",
    "\n",
    "    def frame_step(self, flap):\n",
    "        if flap:\n",
    "            if self.playery > -2 * PLAYER_HEIGHT:\n",
    "                self.playerVelY = self.playerFlapAcc\n",
    "                self.playerFlapped = True\n",
    "\n",
    "        # playerIndex basex change\n",
    "        if (self.loopIter + 1) % 3 == 0:\n",
    "            self.playerIndex = next(PLAYER_INDEX_GEN)\n",
    "        self.loopIter = (self.loopIter + 1) % 30\n",
    "        self.basex = -((-self.basex + 100) % self.baseShift)\n",
    "\n",
    "        # player's movement\n",
    "        if self.playerVelY < self.playerMaxVelY and not self.playerFlapped:\n",
    "            self.playerVelY += self.playerAccY\n",
    "        if self.playerFlapped:\n",
    "            self.playerFlapped = False\n",
    "        self.playery += min(self.playerVelY, BASEY - self.playery - PLAYER_HEIGHT)\n",
    "        if self.playery < 0:\n",
    "            self.playery = 0\n",
    "\n",
    "        # move pipes to left\n",
    "        for uPipe, lPipe in zip(self.upperPipes, self.lowerPipes):\n",
    "            uPipe['x'] += self.pipeVelX\n",
    "            lPipe['x'] += self.pipeVelX\n",
    "\n",
    "        # add new pipe when first pipe is about to touch left of screen\n",
    "        if 0 < self.upperPipes[0]['x'] < 5:\n",
    "            newPipe = getRandomPipe()\n",
    "            self.upperPipes.append(newPipe[0])\n",
    "            self.lowerPipes.append(newPipe[1])\n",
    "\n",
    "        # remove first pipe if it's out of the screen\n",
    "        if self.upperPipes[0]['x'] < -PIPE_WIDTH:\n",
    "            self.upperPipes.pop(0)\n",
    "            self.lowerPipes.pop(0)\n",
    "\n",
    "        # check if crash here\n",
    "        isCrash = checkCrash({'x': self.playerx, 'y': self.playery, 'index': self.playerIndex}, self.upperPipes, self.lowerPipes)\n",
    "        if isCrash:\n",
    "            self.__init__()\n",
    "\n",
    "        return isCrash\n",
    "\n",
    "\n",
    "def getRandomPipe():\n",
    "    \"\"\"returns a randomly generated pipe\"\"\"\n",
    "    # y of gap between upper and lower pipe\n",
    "    gapYs = [20, 30, 40, 50, 60, 70, 80, 90]\n",
    "    index = random.randint(0, len(gapYs)-1)\n",
    "    gapY = gapYs[index]\n",
    "\n",
    "    gapY += int(BASEY * 0.2)\n",
    "    pipeX = SCREENWIDTH + 10\n",
    "\n",
    "    return [\n",
    "        {'x': pipeX, 'y': gapY - PIPE_HEIGHT},  # upper pipe\n",
    "        {'x': pipeX, 'y': gapY + PIPEGAPSIZE},  # lower pipe\n",
    "    ]\n",
    "\n",
    "\n",
    "def checkCrash(player, upperPipes, lowerPipes):\n",
    "    \"\"\"returns True if player collders with base or pipes.\"\"\"\n",
    "    pi = player['index']\n",
    "    player['w'] = IMAGES['player'][0].get_width()\n",
    "    player['h'] = IMAGES['player'][0].get_height()\n",
    "\n",
    "    # if player crashes into ground\n",
    "    if player['y'] + player['h'] >= BASEY - 1:\n",
    "        return True\n",
    "    else:\n",
    "\n",
    "        playerRect = pygame.Rect(player['x'], player['y'],\n",
    "                      player['w'], player['h'])\n",
    "\n",
    "        for uPipe, lPipe in zip(upperPipes, lowerPipes):\n",
    "            # upper and lower pipe rects\n",
    "            uPipeRect = pygame.Rect(uPipe['x'], uPipe['y'], PIPE_WIDTH, PIPE_HEIGHT)\n",
    "            lPipeRect = pygame.Rect(lPipe['x'], lPipe['y'], PIPE_WIDTH, PIPE_HEIGHT)\n",
    "\n",
    "            # player and upper/lower pipe hitmasks\n",
    "            pHitMask = HITMASKS['player'][pi]\n",
    "            uHitmask = HITMASKS['pipe'][0]\n",
    "            lHitmask = HITMASKS['pipe'][1]\n",
    "\n",
    "            # if bird collided with upipe or lpipe\n",
    "            uCollide = pixelCollision(playerRect, uPipeRect, pHitMask, uHitmask)\n",
    "            lCollide = pixelCollision(playerRect, lPipeRect, pHitMask, lHitmask)\n",
    "\n",
    "            if uCollide or lCollide:\n",
    "                return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def pixelCollision(rect1, rect2, hitmask1, hitmask2):\n",
    "    \"\"\"Checks if two objects collide and not just their rects\"\"\"\n",
    "    rect = rect1.clip(rect2)\n",
    "\n",
    "    if rect.width == 0 or rect.height == 0:\n",
    "        return False\n",
    "\n",
    "    x1, y1 = rect.x - rect1.x, rect.y - rect1.y\n",
    "    x2, y2 = rect.x - rect2.x, rect.y - rect2.y\n",
    "\n",
    "    for x in range(rect.width):\n",
    "        for y in range(rect.height):\n",
    "            if hitmask1[x1+x][y1+y] and hitmask2[x2+x][y2+y]:\n",
    "                return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# HELPER FUNCTION TO GET INPUT FROM FLAPPY BIRD GAME\n",
    "\n",
    "import torch\n",
    "\n",
    "def get_gamestate_info(game_state):\n",
    "    \"\"\"\n",
    "    gets coordinates of the two pipes\n",
    "    usage:          pipe_info = get_pipes_info(game_state)\n",
    "                    pipe_info[\"pipe0\"][\"upper\"][\"x\"] = x coordinate of the upper pipe of the first pipe\n",
    "    @args:          game_state\n",
    "    @returns:         \n",
    "        \"pipe0\": {\n",
    "            \"upper\": {\n",
    "                \"x\":\n",
    "                \"y\": \n",
    "            },\n",
    "            \"lower\": {\n",
    "                \"x\": \n",
    "                \"y\": \n",
    "            }\n",
    "        },\n",
    "        \"pipe1\": {\n",
    "            \"upper\": {\n",
    "                \"x\": ,\n",
    "                \"y\": \n",
    "            },\n",
    "            \"lower\": {\n",
    "                \"x\": \n",
    "                \"y\": \n",
    "            }\n",
    "        }, \n",
    "        \"player\": {\n",
    "            \"x\": \n",
    "            \"y\": \n",
    "            \"VelY\":\n",
    "            \"AccY\": \n",
    "            \"Flapped\": \n",
    "        }\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"pipe0\": {\n",
    "            \"upper\": {\n",
    "                \"x\": game_state.upperPipes[0]['x'],\n",
    "                \"y\": game_state.upperPipes[0]['y']\n",
    "            },\n",
    "            \"lower\": {\n",
    "                \"x\": game_state.lowerPipes[0]['x'],\n",
    "                \"y\": game_state.lowerPipes[0]['y']\n",
    "            }\n",
    "        },\n",
    "        \"pipe1\": {\n",
    "            \"upper\": {\n",
    "                \"x\": game_state.upperPipes[1]['x'],\n",
    "                \"y\": game_state.upperPipes[1]['y']\n",
    "            },\n",
    "            \"lower\": {\n",
    "                \"x\": game_state.lowerPipes[1]['x'],\n",
    "                \"y\": game_state.lowerPipes[1]['y']\n",
    "            }\n",
    "        },\n",
    "        \"player\": {\n",
    "            \"x\": game_state.playerx,\n",
    "            \"y\": game_state.playery,\n",
    "            \"VelY\": game_state.playerVelY,\n",
    "            \"AccY\": game_state.playerAccY,\n",
    "            \"Flapped\": game_state.playerFlapped,\n",
    "        }\n",
    "    }\n",
    "\n",
    "def get_input_layer(game_state):\n",
    "    \"\"\"\n",
    "    gets gamestate but returns it as a tensor. Use when feeding into ML algorithm\n",
    "    Arguments: game_state\n",
    "    Returns: tensor of shape (6, 1) containing same information as get_gamestate_info, but without the dictionary.\n",
    "    \"\"\"\n",
    "    return torch.tensor([game_state.lowerPipes[0]['x'], game_state.lowerPipes[0]['y'], \n",
    "                         game_state.lowerPipes[1]['x'], game_state.lowerPipes[1]['y'], \n",
    "                        game_state.playery, game_state.playerVelY, game_state.playerFlapped])\n",
    "\n",
    "def get_input_layer_2(game_state):\n",
    "    \"\"\"\n",
    "    gets gamestate but returns it as a np array. Use when feeding into ML algorithm\n",
    "    Arguments: game_state\n",
    "    Returns: tensor of shape (7, 1) containing same information as get_gamestate_info, but without the dictionary.\n",
    "    \"\"\"\n",
    "    # print(\"pipe height: \", game_state.lowerPipes[0]['y'])\n",
    "    return np.array([game_state.lowerPipes[0]['x'], game_state.lowerPipes[0]['y'], game_state.lowerPipes[0]['y'] - 100,\n",
    "                         game_state.lowerPipes[1]['x'], game_state.lowerPipes[1]['y'], game_state.lowerPipes[0]['y'] - 100,\n",
    "                        game_state.playery])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'neat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_30500\\1226115634.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mneat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mNEATModel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'neat'"
     ]
    }
   ],
   "source": [
    "# NEAT MODEL\n",
    "\n",
    "import os.path\n",
    "import pickle\n",
    "import neat\n",
    "\n",
    "class NEATModel:\n",
    "    def __init__(self):\n",
    "        configFile = os.path.join(os.path.abspath(''), 'NEATConfig')\n",
    "\n",
    "        self.config = neat.Config(neat.DefaultGenome, neat.DefaultReproduction,\n",
    "                                  neat.DefaultSpeciesSet, neat.DefaultStagnation,\n",
    "                                  configFile)\n",
    "\n",
    "        self.population = neat.Population(self.config)\n",
    "\n",
    "        self.population.add_reporter(neat.StdOutReporter(True))\n",
    "        stats = neat.StatisticsReporter()\n",
    "        self.population.add_reporter(stats)\n",
    "        self.population.add_reporter(neat.Checkpointer(20))\n",
    "        self.bestGenome = None\n",
    "        self.gameState = None\n",
    "\n",
    "    def run(self, generations, checkpointFileName=\"\"):\n",
    "        if checkpointFileName != \"\":\n",
    "            self.population = neat.Checkpointer.restore_checkpoint(checkpointFileName)\n",
    "        self.bestGenome = self.population.run(self.evaluateGenomes, generations)\n",
    "        with open(\"NEATBestGenome.pkl\", \"wb\") as f:\n",
    "            pickle.dump(self.bestGenome, f)\n",
    "            f.close()\n",
    "\n",
    "    def loadBest(self):\n",
    "        with open(\"NEATBestGenome.pkl\", \"rb\") as f:\n",
    "            self.bestGenome = pickle.load(f)\n",
    "\n",
    "    def playGame(self):\n",
    "        self.gameState = GameState()\n",
    "        network = neat.nn.FeedForwardNetwork.create(self.bestGenome, self.config)\n",
    "        go = True\n",
    "        while go:\n",
    "            networkInput = get_input_layer(self.gameState)\n",
    "            networkOutput = network.activate(networkInput)[0]\n",
    "            flap = networkOutput > 0.5  # sigmoid activation, output should be between 0 and 1\n",
    "            _, _, terminal = self.gameState.frame_step(flap)\n",
    "            if terminal:\n",
    "                go = False\n",
    "\n",
    "    def testBest(self, runs):\n",
    "        self.gameState = GameStateNoGraphics()\n",
    "        network = neat.nn.FeedForwardNetwork.create(self.bestGenome, self.config)\n",
    "        fitnesses = []\n",
    "        for i in range(runs):\n",
    "            thisRunFitness = 0\n",
    "            go = True\n",
    "            while go:\n",
    "                thisRunFitness += 1\n",
    "                networkInput = get_input_layer(self.gameState)\n",
    "                networkOutput = network.activate(networkInput)[0]\n",
    "                flap = networkOutput > 0.5  # sigmoid activation, output should be between 0 and 1\n",
    "                if self.gameState.frame_step(flap) or thisRunFitness > 10000:\n",
    "                    go = False\n",
    "                    fitnesses.append(thisRunFitness)\n",
    "                    if (i+1) % 100 == 0:\n",
    "                        print(\"Finished run: \" + str(i+1) + \"/\" + str(runs))\n",
    "        print(fitnesses)\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluateGenomes(genomes, config):\n",
    "        gameState = GameStateNoGraphics()\n",
    "        for genome_id, genome in genomes:\n",
    "            network = neat.nn.FeedForwardNetwork.create(genome, config)\n",
    "            runs = 10\n",
    "            averageFitness = 0\n",
    "            for i in range(runs):\n",
    "                thisRunFitness = 0\n",
    "                go = True\n",
    "                while go:\n",
    "                    thisRunFitness += 1\n",
    "                    networkInput = get_input_layer(gameState)\n",
    "                    networkOutput = network.activate(networkInput)[0]\n",
    "                    flap = networkOutput > 0.5  # sigmoid activation, output should be between 0 and 1\n",
    "                    if gameState.frame_step(flap) or thisRunFitness > 10000:\n",
    "                        go = False\n",
    "                        averageFitness += thisRunFitness / runs\n",
    "            genome.fitness = averageFitness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run: 100/1000\n",
      "Finished run: 200/1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m neatModel\u001b[38;5;241m.\u001b[39mloadBest()  \u001b[38;5;66;03m# Load the best model from training\u001b[39;00m\n\u001b[0;32m      6\u001b[0m neatModel\u001b[38;5;241m.\u001b[39mplayGame()  \u001b[38;5;66;03m# Watch the best model play the game (won't work in Jupyter Lab)\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mneatModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtestBest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Test performance of the best model\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 58\u001b[0m, in \u001b[0;36mNEATModel.testBest\u001b[1;34m(self, runs)\u001b[0m\n\u001b[0;32m     56\u001b[0m thisRunFitness \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     57\u001b[0m networkInput \u001b[38;5;241m=\u001b[39m get_input_layer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgameState)\n\u001b[1;32m---> 58\u001b[0m networkOutput \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetworkInput\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     59\u001b[0m flap \u001b[38;5;241m=\u001b[39m networkOutput \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m  \u001b[38;5;66;03m# sigmoid activation, output should be between 0 and 1\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgameState\u001b[38;5;241m.\u001b[39mframe_step(flap) \u001b[38;5;129;01mor\u001b[39;00m thisRunFitness \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m10000\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\neat\\nn\\feed_forward.py:16\u001b[0m, in \u001b[0;36mFeedForwardNetwork.activate\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_nodes) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs):\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;132;01m{0:n}\u001b[39;00m\u001b[38;5;124m inputs, got \u001b[39m\u001b[38;5;132;01m{1:n}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_nodes), \u001b[38;5;28mlen\u001b[39m(inputs)))\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues[k] \u001b[38;5;241m=\u001b[39m v\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node, act_func, agg_func, bias, response, links \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_evals:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\tensor.py:594\u001b[0m, in \u001b[0;36mTensor.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state():\n\u001b[0;32m    590\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIterating over a tensor might cause the trace to be incorrect. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    591\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPassing a tensor of different shape won\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mt change the number of \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    592\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miterations executed (and might lead to errors or silently give \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    593\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mincorrect results).\u001b[39m\u001b[38;5;124m'\u001b[39m, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mRuntimeWarning\u001b[39;00m)\n\u001b[1;32m--> 594\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munbind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAINING AND TESTING THE NEAT MODEL\n",
    "\n",
    "neatModel = NEATModel()\n",
    "# neatModel.run(300)  # Train the model, will take quite a long time!\n",
    "neatModel.loadBest()  # Load the best model from training\n",
    "neatModel.playGame()  # Watch the best model play the game (look at your taskbar it won't popup automatically)\n",
    "neatModel.testBest(1000)  # Test performance of the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "# from utils import get_input_layer_2 as input\n",
    "# import game.flappyNoGraphics as Game\n",
    "# import game.wrapped_flappy_bird as GameVisual\n",
    "from collections import deque\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the neural net of the agent\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, lr):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.n_actions = 2\n",
    "        self.hid_1 = 128\n",
    "        self.hid_2 = 128\n",
    "        self.hid_3 = 128\n",
    "        self.inputs = 7 * 4\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(self.inputs, self.hid_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hid_1, self.hid_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hid_2, self.hid_3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hid_3, self.n_actions),\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss = nn.SmoothL1Loss()\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model(x.type(torch.FloatTensor).to(self.device))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a class to manage the state. Each state is a stack of 4 \"frame\" of the game\n",
    "# which provides the agent on information of the bird's movement.\n",
    "\n",
    "from collections import deque\n",
    "import torch\n",
    "# from utils import get_input_layer_2 as input\n",
    "import numpy as np\n",
    "\n",
    "class StateManager(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "        starting_state = [0, 0, 0, 0, 0, 0, 0]\n",
    "        for _ in range(capacity):\n",
    "            self.memory.append(starting_state)\n",
    "\n",
    "    def push(self, game):\n",
    "        \"\"\"Save a frame, \n",
    "            returns tensor of flattened state frames\n",
    "        \"\"\"\n",
    "        state_frame = get_input_layer_2(game)\n",
    "        self.memory.popleft()\n",
    "        self.memory.append(state_frame)\n",
    "        tensor_list = []\n",
    "        for i in range(4):\n",
    "            tensor_list.append(self.memory[i])\n",
    "        return np.array(tensor_list).flatten()\n",
    "    \n",
    "    def get(self):\n",
    "        # return np array of state frames\n",
    "        tensor_list = []\n",
    "        for i in range(4):\n",
    "            tensor_list.append(self.memory[i])\n",
    "        return np.array(tensor_list).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the class for the agent.\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Porperties:\n",
    "            gamma (float): Future reward discount rate.\n",
    "            epsilon (float): Probability for choosing random policy.\n",
    "            epsilon_decay (float): Rate at which epsilon decays toward zero.\n",
    "            learning_rate (float): Learning rate for Adam optimizer.\n",
    "\n",
    "        Returns:\n",
    "            Agent\n",
    "        \"\"\"\n",
    "        # constant parameters\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.996\n",
    "        self.lr = 0.00005\n",
    "        self.batch_size = 64\n",
    "        self.max_mem_size = 10000\n",
    "        # self.input_dims = 7 * 4\n",
    "\n",
    "        #variable parameters\n",
    "        self.epsilon = 0.01\n",
    "        self.mem_cntr = 0\n",
    "        self.mem_cntr_successful = 0\n",
    "\n",
    "        # initializing memory\n",
    "        self.memory = deque(maxlen=self.max_mem_size)\n",
    "        self.memory_successful = deque(maxlen=1000)\n",
    "        self.episodic_memory = []\n",
    "\n",
    "        #initialize networks\n",
    "        self.network = Network(self.lr)\n",
    "\n",
    "    def save_experience(self):\n",
    "        with open('Models/DQL/experience.pickle', 'wb') as handle:\n",
    "            pickle.dump(self.memory, handle)\n",
    "        with open('Models/DQL/experience_successful.pickle', 'wb') as handle:\n",
    "            pickle.dump(self.memory_successful, handle)\n",
    "\n",
    "    def load_experience(self):\n",
    "        with open('Models/DQL/experience.pickle', 'rb') as handle:\n",
    "            self.memory = pickle.load(handle)\n",
    "        with open('Models/DQL/experience_successful.pickle', 'rb') as handle:\n",
    "            self.memory_successful = pickle.load(handle)\n",
    "\n",
    "    def getMemory(self):\n",
    "        return self.memory\n",
    "\n",
    "    def nextEpisode(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def getepsilon(self):\n",
    "        return self.epsilon\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, game_over, score, next_reward):\n",
    "        if (self.mem_cntr >= self.max_mem_size - 2):\n",
    "            for i in range(self.max_mem_size - 3000):\n",
    "                self.memory.popleft()\n",
    "            self.mem_cntr = len(self.memory) - 1\n",
    "\n",
    "        memory = [state, action, reward, next_state, game_over, score, next_reward]\n",
    "        self.memory.append(memory)\n",
    "\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def remember_successful(self, state, action, reward, next_state, game_over, score, next_reward):\n",
    "        if (self.mem_cntr_successful >= 1000 - 20):\n",
    "            for i in range(1000 - 500):\n",
    "                self.memory_successful.popleft()\n",
    "            self.mem_cntr_successful = len(self.memory_successful) - 1\n",
    "\n",
    "        memory = [state, action, reward, next_state, game_over, score, next_reward]\n",
    "        self.memory_successful.append(memory)\n",
    "\n",
    "        self.mem_cntr_successful += 1\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            # exploration\n",
    "\n",
    "            # 2 in 30 = averages about 1 press every 0.5 seconds which is in the ballpark of whats required to play the game. \n",
    "            # Gives bot best start possible (as it actually has a chance of making it through the first block!)\n",
    "            # in flappy bird a flap changes the gamestate a lot more than a no-flap.\n",
    "            determiner = np.random.randint(0, 30);\n",
    "            if (determiner <= 2):\n",
    "                return 1\n",
    "            return 0\n",
    "        else:\n",
    "            # exploitation, select epsilon-greedy action.\n",
    "                state_tensor = torch.tensor([state]).to(self.network.device, dtype=torch.int32)\n",
    "                action = torch.argmax(self.network.forward(state_tensor)).item()\n",
    "                \n",
    "        return action\n",
    "    \n",
    "    def updateEpsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        learn from a random batch of experiences\n",
    "        \"\"\"\n",
    "        if self.mem_cntr < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        self.network.optimizer.zero_grad()\n",
    "        max_mem = min(self.mem_cntr, self.max_mem_size)\n",
    "        batch = np.random.choice(max_mem, self.batch_size, replace=False)\n",
    "\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "\n",
    "        # memory = [state, action, reward, next_state, game_over, score, next_reward]\n",
    "        state_batch = torch.tensor([self.memory[i][0] for i in batch]).to(self.network.device, dtype=torch.float32)\n",
    "        action_batch = torch.tensor([self.memory[i][1] for i in batch])\n",
    "        reward_batch = torch.tensor([self.memory[i][2] for i in batch]).to(self.network.device, dtype=torch.float32)\n",
    "        new_state_batch = torch.tensor([self.memory[i][3] for i in batch]).to(self.network.device, dtype=torch.float32)\n",
    "        game_over_batch = torch.tensor([self.memory[i][4] for i in batch]).to(self.network.device, dtype=torch.bool)\n",
    "\n",
    "        #estimate q(s,a) and q(s',a').\n",
    "        q_current = self.network.forward(state_batch)[batch_index, action_batch]\n",
    "        q_next = self.network.forward(new_state_batch)\n",
    "        q_next[game_over_batch] = 0.0\n",
    "        # q(st,at) = r + gamma * max(q(s',a')\n",
    "        q_target = reward_batch + self.gamma * torch.max(q_next, dim=1)[0]\n",
    "\n",
    "        # smoothl1 loss and back-propagation\n",
    "        loss = self.network.loss(q_target, q_current).to(self.network.device)\n",
    "        loss.backward()\n",
    "        #prevent exploding gradient\n",
    "        torch.nn.utils.clip_grad_value_(self.network.parameters(), 100)\n",
    "        self.network.optimizer.step()\n",
    "\n",
    "    def learn_successful(self):\n",
    "        \"\"\"\n",
    "        learn from the set of experience that the agent was successful in. \n",
    "        Incentivises the agent to \n",
    "        \"\"\"\n",
    "        if self.mem_cntr_successful < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # print(\"learning successful\")\n",
    "        self.network.optimizer.zero_grad()\n",
    "        max_mem = min(self.mem_cntr_successful, self.max_mem_size)\n",
    "        batch = np.random.choice(max_mem, self.batch_size, replace=False)\n",
    "\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "\n",
    "        # memory = [state, action, reward, next_state, game_over, score]\n",
    "        state_batch = torch.tensor([self.memory[i][0] for i in batch]).to(self.network.device, dtype=torch.float32)\n",
    "        action_batch = torch.tensor([self.memory[i][1] for i in batch])\n",
    "        reward_batch = torch.tensor([self.memory[i][2] for i in batch]).to(self.network.device, dtype=torch.float32)\n",
    "        new_state_batch = torch.tensor([self.memory[i][3] for i in batch]).to(self.network.device, dtype=torch.float32)\n",
    "        game_over_batch = torch.tensor([self.memory[i][4] for i in batch]).to(self.network.device, dtype=torch.bool)\n",
    "        #estimate q(s,a) and q(s',a').\n",
    "        q_current = self.network.forward(state_batch)[batch_index, action_batch]\n",
    "        q_next = self.network.forward(new_state_batch)\n",
    "        q_next[game_over_batch] = 0.0\n",
    "        # q(st,at) = r + gamma * max(q(s',a')\n",
    "        q_target = reward_batch + self.gamma * torch.max(q_next, dim=1)[0]\n",
    "        \n",
    "        # smoothl1 loss and back-propagation\n",
    "        loss = self.network.loss(q_target, q_current).to(self.network.device)\n",
    "        loss.backward()\n",
    "        self.network.optimizer.step()\n",
    "\n",
    "    def update_episodic_memory(self, state, action, reward, next_state, done, score, current_step):\n",
    "        \"\"\"appends to a temporary memory. The temporary memory is uploaded to the main memory\n",
    "        once a game is complete.\n",
    "        \"\"\"\n",
    "        self.episodic_memory.append([state, action, reward, next_state, done, score, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the trainer allows a human to play the game then upload the relevant data to the agent.\n",
    "# increases the rate at which the agent initially learns.\n",
    "import keyboard\n",
    "import pickle\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self, agent):\n",
    "        self.runs = 10\n",
    "        self.agent = agent\n",
    "        self.game = GameState()\n",
    "        self.state_manager = StateManager(4)\n",
    "\n",
    "    def play(self, runs=10):\n",
    "        self.runs = runs\n",
    "        self.agent.episodic_memory = []\n",
    "        current_step = 0\n",
    "        # for runs amount of games\n",
    "        for i in range(self.runs):\n",
    "            #initialize game\n",
    "            self.game = GameState()\n",
    "            self.state_manager = StateManager(4)\n",
    "            state = self.state_manager.get()\n",
    "            done = False\n",
    "            score = 0\n",
    "            # manually play the game\n",
    "            while not done:\n",
    "                if keyboard.is_pressed(\" \"):\n",
    "                    action = 1\n",
    "                    _, reward, _ = self.game.frame_step(True)\n",
    "                else:\n",
    "                    action = 0\n",
    "                    _, reward, _ = self.game.frame_step(False)\n",
    "                if (reward == -5):\n",
    "                    done = True\n",
    "                    final_score = score\n",
    "                    reward = -5\n",
    "                score += reward\n",
    "\n",
    "                self.state_manager.push(self.game)\n",
    "                \n",
    "                #upload experience and train the agent on the human gameplay\n",
    "                next_state = self.state_manager.get()\n",
    "                self.agent.update_episodic_memory(state, action, reward, next_state, done, score, current_step)\n",
    "                self.agent.learn()\n",
    "                self.agent.learn_successful()\n",
    "                current_step += 1\n",
    "                state = next_state\n",
    "            for frame in self.agent.episodic_memory:\n",
    "                self.agent.remember(frame[0], frame[1], frame[2], frame[3], frame[4], frame[5], frame[6])\n",
    "                self.agent.remember_successful(frame[0], frame[1], frame[2], frame[3], frame[4], frame[5], frame[6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main training loop\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def train():\n",
    "\n",
    "    agent = Agent()\n",
    "    scores, median_scores, eps_history, time_history, time_median = [], [], [], [], []\n",
    "    n_games = 100000\n",
    "    success_threshold = 15\n",
    "    trainer = Trainer(agent)\n",
    "\n",
    "    # trainer.play(10)\n",
    "    # agent.save_experience()\n",
    "    agent.load_experience()\n",
    "    #learn from human experiences for a headstart\n",
    "    #without this, the agent typically just default to a policy of only flapping or only doing nothing\n",
    "    for i in range(100):\n",
    "        agent.learn()\n",
    "\n",
    "    # for n_games amount of games\n",
    "    for i in range(n_games):\n",
    "        #initialize game\n",
    "        game = GameNoGraphics()\n",
    "        if (keyboard.is_pressed(\"p\")):\n",
    "            game = GameState()\n",
    "        score = 0\n",
    "        game_over = False\n",
    "        state_manager = StateManager(4)\n",
    "        state = state_manager.get()\n",
    "        done = False\n",
    "        # state, action, reward, next_state, done, score\n",
    "        agent.episodic_memory = []\n",
    "        current_step = 0\n",
    "        #while the game is not complete\n",
    "        while not done:\n",
    "            #select an action\n",
    "            action = agent.select_action(state)\n",
    "            _, reward, _ = game.frame_step(action)\n",
    "            #calculate the state\n",
    "            state_manager.push(game)\n",
    "            next_state = state_manager.get()\n",
    "            if (reward == -5):\n",
    "                done = True\n",
    "                final_score = score\n",
    "                reward = -5\n",
    "            score += reward\n",
    "            #remember the action taken\n",
    "            # agent.remember(state, action, reward, next_state, done, score)\n",
    "            agent.update_episodic_memory(state, action, reward, next_state, done, score, current_step)\n",
    "            \n",
    "\n",
    "            state = next_state\n",
    "            current_step += 1\n",
    "        #after each game, learn a random batch of experiences from memory\n",
    "        #and also lean a batch of experiences that the bird was successful in.\n",
    "        agent.learn()\n",
    "        agent.learn_successful()\n",
    "\n",
    "        agent.updateEpsilon()\n",
    "        #upload memory to main memory\n",
    "        eps_history.append(agent.epsilon)\n",
    "        for frame in agent.episodic_memory:\n",
    "            agent.remember(frame[0], frame[1], frame[2], frame[3], frame[4], frame[5], frame[6])\n",
    "        # success_threshold is typically 10 greater than the median score.\n",
    "        if (score > success_threshold):\n",
    "            agent.remember_successful(frame[0], frame[1], frame[2], frame[3], frame[4], frame[5], frame[6])\n",
    "        # agent.remember(state, action, reward, next_state, done, score)\n",
    "\n",
    "        #calculate some statistics for evaluation\n",
    "        median_score = np.median(scores[-100:])\n",
    "        success_threshold = max(success_threshold, median_score + 10)\n",
    "        scores.append(score)\n",
    "        median_scores.append(median_score)\n",
    "        time_history.append(current_step/30)\n",
    "        median_t = np.median(time_history[-100:])\n",
    "        time_median.append(median_t)\n",
    "        if ((i % 100) == 0):\n",
    "            print('episode: ', i,'score: %.2f' % score,\n",
    "                    ' median score %.2f' % median_score, 'time %.2f' % (current_step/30),'median time %.2f' % (median_t) ,'epsilon %.2f' % agent.epsilon)\n",
    "        if (keyboard.is_pressed(\"`\")):\n",
    "            break\n",
    "    #save the final model\n",
    "    torch.save(agent.network.state_dict(), 'Models/DQL/dqlmodel.pth')\n",
    "    #plot.\n",
    "    plt.plot(time_median)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  0 score: 2.20  median score nan time 0.63 median time 0.63 epsilon 0.01\n",
      "episode:  100 score: 91.40  median score 14.60 time 5.67 median time 1.67 epsilon 0.01\n",
      "episode:  200 score: 31.00  median score 14.60 time 2.23 median time 1.67 epsilon 0.01\n",
      "episode:  300 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  400 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  500 score: 29.40  median score 14.60 time 2.10 median time 1.67 epsilon 0.01\n",
      "episode:  600 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  700 score: -5.20  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  800 score: 38.60  median score 14.60 time 2.87 median time 1.67 epsilon 0.01\n",
      "episode:  900 score: 11.40  median score 14.60 time 1.80 median time 1.82 epsilon 0.01\n",
      "episode:  1000 score: 8.60  median score 14.20 time 1.87 median time 1.73 epsilon 0.01\n",
      "episode:  1100 score: 2.20  median score 8.40 time 0.63 median time 1.70 epsilon 0.01\n",
      "episode:  1200 score: 6.60  median score 9.80 time 1.70 median time 1.70 epsilon 0.01\n",
      "episode:  1300 score: 4.60  median score 11.20 time 1.90 median time 1.68 epsilon 0.01\n",
      "episode:  1400 score: 14.60  median score 3.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  1500 score: 14.60  median score 12.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  1600 score: 29.80  median score 14.60 time 2.13 median time 1.67 epsilon 0.01\n",
      "episode:  1700 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  1800 score: 17.00  median score 14.60 time 1.87 median time 1.67 epsilon 0.01\n"
     ]
    }
   ],
   "source": [
    "#run the DQL model training.\n",
    "\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
