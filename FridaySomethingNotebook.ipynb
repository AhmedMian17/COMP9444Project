{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Friday Something Flappy Bird AI\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Flappy bird is a mobile game which became hugely popular in early 2014. In the game, the player controls a bird tasked with flying between gaps of green pipes. The bird continuously falls downwards, unless the player taps the screen, giving the bird a short burst of upwards speed. The goal of this project is to develop an artificial intellegence that can play Flappy Bird at a level comparable to or beyond a regular human player.\n",
    "\n",
    "## Reinforcement Learning Task\n",
    "\n",
    "Given that nature of the problem, reinforcement learning is the most appropriate learning paradigm. The task will be for the AI to, given some input, determine whether or not to \"flap\" at each frame. Every frame the bird does not crash into a pipe or the floor, the AI will be granted a reward of 1. This will encourage the AI to survive as long as possible.\n",
    "\n",
    "## Exploratory Analysis of Reinforcement Learning Task\n",
    "\n",
    "The task is resonably simple, the main decision to make is what input should be given for the AI. There are two options:\n",
    "1. Take a screenshot of the game, and use a convolutional neural network to extract important features (player, obstacles)\n",
    "1. Directly use player position and obstacle position as inputs\n",
    "\n",
    "It was chosen to use the second option as it would make the model simpler, could train faster as there is no need to draw graphics, and likely converge in fewer iterations as the model does not need to first learn what features are important. Seven inputs where given to the model, these were:\n",
    "- First pipe x coordinate\n",
    "- First pipe y coordinate\n",
    "- Second pipe x coordinate\n",
    "- Second pipe y coordinate\n",
    "- Player y coordinate\n",
    "- Player y velocity\n",
    "- Whether or not the player flapped on the previous frame\n",
    "In the NEAT Model these seven inputs were adequate, but in the Q-Learning model these inputs of the previous three frames where also given, totalling 28 inputs.\n",
    "\n",
    "## Models\n",
    "\n",
    "Given that the task was a non-episodic reinforcement learning problem it was necessary to use models that could train under these conditions. Some of the options available included:\n",
    "- Value Function Learning\n",
    "    - TD-Learning\n",
    "    - Q-Learning\n",
    "        - Deep Q-Networks (DQN)\n",
    "        - Double DQN\n",
    "- Policy Learning\n",
    "    - Evolutionary Strategies\n",
    "        - Genetic Algorithm\n",
    "        - NEAT\n",
    "    - Policy Gradients\n",
    "- Actor-Critic\n",
    "\n",
    "NEAT and Q-Learning were chosen because of the precedent of these methods being used for similar tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.7.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# FLAPPY BIRD GRAPHICS FILES FROM https://github.com/yenchenlin/DeepLearningFlappyBird\n",
    "\n",
    "import pygame\n",
    "import sys\n",
    "def load():\n",
    "    # path of player with different states\n",
    "    PLAYER_PATH = (\n",
    "            'assets/sprites/redbird-upflap.png',\n",
    "            'assets/sprites/redbird-midflap.png',\n",
    "            'assets/sprites/redbird-downflap.png'\n",
    "    )\n",
    "\n",
    "    # path of background\n",
    "    BACKGROUND_PATH = 'assets/sprites/background-black.png'\n",
    "\n",
    "    # path of pipe\n",
    "    PIPE_PATH = 'assets/sprites/pipe-green.png'\n",
    "\n",
    "    IMAGES, SOUNDS, HITMASKS = {}, {}, {}\n",
    "\n",
    "    # numbers sprites for score display\n",
    "    IMAGES['numbers'] = (\n",
    "        pygame.image.load('assets/sprites/0.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/1.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/2.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/3.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/4.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/5.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/6.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/7.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/8.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/9.png').convert_alpha()\n",
    "    )\n",
    "\n",
    "    # base (ground) sprite\n",
    "    IMAGES['base'] = pygame.image.load('assets/sprites/base.png').convert_alpha()\n",
    "\n",
    "    # sounds\n",
    "    if 'win' in sys.platform:\n",
    "        soundExt = '.wav'\n",
    "    else:\n",
    "        soundExt = '.ogg'\n",
    "\n",
    "    SOUNDS['die']    = pygame.mixer.Sound('assets/audio/die' + soundExt)\n",
    "    SOUNDS['hit']    = pygame.mixer.Sound('assets/audio/hit' + soundExt)\n",
    "    SOUNDS['point']  = pygame.mixer.Sound('assets/audio/point' + soundExt)\n",
    "    SOUNDS['swoosh'] = pygame.mixer.Sound('assets/audio/swoosh' + soundExt)\n",
    "    SOUNDS['wing']   = pygame.mixer.Sound('assets/audio/wing' + soundExt)\n",
    "\n",
    "    # select random background sprites\n",
    "    IMAGES['background'] = pygame.image.load(BACKGROUND_PATH).convert()\n",
    "\n",
    "    # select random player sprites\n",
    "    IMAGES['player'] = (\n",
    "        pygame.image.load(PLAYER_PATH[0]).convert_alpha(),\n",
    "        pygame.image.load(PLAYER_PATH[1]).convert_alpha(),\n",
    "        pygame.image.load(PLAYER_PATH[2]).convert_alpha(),\n",
    "    )\n",
    "\n",
    "    # select random pipe sprites\n",
    "    IMAGES['pipe'] = (\n",
    "        pygame.transform.rotate(\n",
    "            pygame.image.load(PIPE_PATH).convert_alpha(), 180),\n",
    "        pygame.image.load(PIPE_PATH).convert_alpha(),\n",
    "    )\n",
    "\n",
    "    # hismask for pipes\n",
    "    HITMASKS['pipe'] = (\n",
    "        getHitmask(IMAGES['pipe'][0]),\n",
    "        getHitmask(IMAGES['pipe'][1]),\n",
    "    )\n",
    "\n",
    "    # hitmask for player\n",
    "    HITMASKS['player'] = (\n",
    "        getHitmask(IMAGES['player'][0]),\n",
    "        getHitmask(IMAGES['player'][1]),\n",
    "        getHitmask(IMAGES['player'][2]),\n",
    "    )\n",
    "\n",
    "    return IMAGES, SOUNDS, HITMASKS\n",
    "\n",
    "def getHitmask(image):\n",
    "    \"\"\"returns a hitmask using an image's alpha.\"\"\"\n",
    "    mask = []\n",
    "    for x in range(image.get_width()):\n",
    "        mask.append([])\n",
    "        for y in range(image.get_height()):\n",
    "            mask[x].append(bool(image.get_at((x,y))[3]))\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# FLAPPY BIRD CODE FROM https://github.com/yenchenlin/DeepLearningFlappyBird\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import random\n",
    "import pygame\n",
    "import pygame.surfarray as surfarray\n",
    "from pygame.locals import *\n",
    "from itertools import cycle\n",
    "\n",
    "FPS = 30\n",
    "SCREENWIDTH  = 288\n",
    "SCREENHEIGHT = 512\n",
    "\n",
    "pygame.init()\n",
    "FPSCLOCK = pygame.time.Clock()\n",
    "SCREEN = pygame.display.set_mode((SCREENWIDTH, SCREENHEIGHT))\n",
    "pygame.display.set_caption('Flappy Bird')\n",
    "\n",
    "IMAGES, SOUNDS, HITMASKS = load()\n",
    "PIPEGAPSIZE = 100 # gap between upper and lower part of pipe\n",
    "BASEY = SCREENHEIGHT * 0.79\n",
    "\n",
    "PLAYER_WIDTH = IMAGES['player'][0].get_width()\n",
    "PLAYER_HEIGHT = IMAGES['player'][0].get_height()\n",
    "PIPE_WIDTH = IMAGES['pipe'][0].get_width()\n",
    "PIPE_HEIGHT = IMAGES['pipe'][0].get_height()\n",
    "BACKGROUND_WIDTH = IMAGES['background'].get_width()\n",
    "\n",
    "PLAYER_INDEX_GEN = cycle([0, 1, 2, 1])\n",
    "\n",
    "\n",
    "class GameState:\n",
    "    def __init__(self):\n",
    "        self.score = self.playerIndex = self.loopIter = 0\n",
    "        self.playerx = int(SCREENWIDTH * 0.2)\n",
    "        self.playery = int((SCREENHEIGHT - PLAYER_HEIGHT) / 2)\n",
    "        self.basex = 0\n",
    "        self.baseShift = IMAGES['base'].get_width() - BACKGROUND_WIDTH\n",
    "\n",
    "        newPipe1 = getRandomPipe()\n",
    "        newPipe2 = getRandomPipe()\n",
    "        self.upperPipes = [\n",
    "            {'x': SCREENWIDTH, 'y': newPipe1[0]['y']},\n",
    "            {'x': SCREENWIDTH + (SCREENWIDTH / 2), 'y': newPipe2[0]['y']},\n",
    "        ]\n",
    "        self.lowerPipes = [\n",
    "            {'x': SCREENWIDTH, 'y': newPipe1[1]['y']},\n",
    "            {'x': SCREENWIDTH + (SCREENWIDTH / 2), 'y': newPipe2[1]['y']},\n",
    "        ]\n",
    "\n",
    "        # player velocity, max velocity, downward accleration, accleration on flap\n",
    "        self.pipeVelX = -4\n",
    "        self.playerVelY    =  0    # player's velocity along Y, default same as playerFlapped\n",
    "        self.playerMaxVelY =  10   # max vel along Y, max descend speed\n",
    "        self.playerMinVelY =  -8   # min vel along Y, max ascend speed\n",
    "        self.playerAccY    =   1   # players downward accleration\n",
    "        self.playerFlapAcc =  -9   # players speed on flapping\n",
    "        self.playerFlapped = False # True when player flaps\n",
    "\n",
    "    def frame_step(self, flap):\n",
    "        pygame.event.pump()\n",
    "\n",
    "        reward = 0.1\n",
    "        terminal = False\n",
    "\n",
    "        if flap:\n",
    "            if self.playery > -2 * PLAYER_HEIGHT:\n",
    "                self.playerVelY = self.playerFlapAcc\n",
    "                self.playerFlapped = True\n",
    "                #SOUNDS['wing'].play()\n",
    "\n",
    "        # check for score\n",
    "        playerMidPos = self.playerx + PLAYER_WIDTH / 2\n",
    "        for pipe in self.upperPipes:\n",
    "            pipeMidPos = pipe['x'] + PIPE_WIDTH / 2\n",
    "            if pipeMidPos <= playerMidPos < pipeMidPos + 4:\n",
    "                self.score += 1\n",
    "                #SOUNDS['point'].play()\n",
    "                reward = 1\n",
    "\n",
    "        # playerIndex basex change\n",
    "        if (self.loopIter + 1) % 3 == 0:\n",
    "            self.playerIndex = next(PLAYER_INDEX_GEN)\n",
    "        self.loopIter = (self.loopIter + 1) % 30\n",
    "        self.basex = -((-self.basex + 100) % self.baseShift)\n",
    "\n",
    "        # player's movement\n",
    "        if self.playerVelY < self.playerMaxVelY and not self.playerFlapped:\n",
    "            self.playerVelY += self.playerAccY\n",
    "        if self.playerFlapped:\n",
    "            self.playerFlapped = False\n",
    "        self.playery += min(self.playerVelY, BASEY - self.playery - PLAYER_HEIGHT)\n",
    "        if self.playery < 0:\n",
    "            self.playery = 0\n",
    "\n",
    "        # move pipes to left\n",
    "        for uPipe, lPipe in zip(self.upperPipes, self.lowerPipes):\n",
    "            uPipe['x'] += self.pipeVelX\n",
    "            lPipe['x'] += self.pipeVelX\n",
    "\n",
    "        # add new pipe when first pipe is about to touch left of screen\n",
    "        if 0 < self.upperPipes[0]['x'] < 5:\n",
    "            newPipe = getRandomPipe()\n",
    "            self.upperPipes.append(newPipe[0])\n",
    "            self.lowerPipes.append(newPipe[1])\n",
    "\n",
    "        # remove first pipe if its out of the screen\n",
    "        if self.upperPipes[0]['x'] < -PIPE_WIDTH:\n",
    "            self.upperPipes.pop(0)\n",
    "            self.lowerPipes.pop(0)\n",
    "\n",
    "        # check if crash here\n",
    "        isCrash= checkCrash({'x': self.playerx, 'y': self.playery,\n",
    "                             'index': self.playerIndex},\n",
    "                            self.upperPipes, self.lowerPipes)\n",
    "        if isCrash:\n",
    "            #SOUNDS['hit'].play()\n",
    "            #SOUNDS['die'].play()\n",
    "            terminal = True\n",
    "            self.__init__()\n",
    "            reward = -1\n",
    "\n",
    "        # draw sprites\n",
    "        SCREEN.blit(IMAGES['background'], (0,0))\n",
    "\n",
    "        for uPipe, lPipe in zip(self.upperPipes, self.lowerPipes):\n",
    "            SCREEN.blit(IMAGES['pipe'][0], (uPipe['x'], uPipe['y']))\n",
    "            SCREEN.blit(IMAGES['pipe'][1], (lPipe['x'], lPipe['y']))\n",
    "\n",
    "        SCREEN.blit(IMAGES['base'], (self.basex, BASEY))\n",
    "        # print score so player overlaps the score\n",
    "        # showScore(self.score)\n",
    "        SCREEN.blit(IMAGES['player'][self.playerIndex],\n",
    "                    (self.playerx, self.playery))\n",
    "\n",
    "        image_data = pygame.surfarray.array3d(pygame.display.get_surface())\n",
    "        pygame.display.update()\n",
    "        FPSCLOCK.tick(FPS)\n",
    "        #print self.upperPipes[0]['y'] + PIPE_HEIGHT - int(BASEY * 0.2)\n",
    "        return image_data, reward, terminal\n",
    "\n",
    "def getRandomPipe():\n",
    "    \"\"\"returns a randomly generated pipe\"\"\"\n",
    "    # y of gap between upper and lower pipe\n",
    "    gapYs = [20, 30, 40, 50, 60, 70, 80, 90]\n",
    "    index = random.randint(0, len(gapYs)-1)\n",
    "    gapY = gapYs[index]\n",
    "\n",
    "    gapY += int(BASEY * 0.2)\n",
    "    pipeX = SCREENWIDTH + 10\n",
    "\n",
    "    return [\n",
    "        {'x': pipeX, 'y': gapY - PIPE_HEIGHT},  # upper pipe\n",
    "        {'x': pipeX, 'y': gapY + PIPEGAPSIZE},  # lower pipe\n",
    "    ]\n",
    "\n",
    "\n",
    "def showScore(score):\n",
    "    \"\"\"displays score in center of screen\"\"\"\n",
    "    scoreDigits = [int(x) for x in list(str(score))]\n",
    "    totalWidth = 0 # total width of all numbers to be printed\n",
    "\n",
    "    for digit in scoreDigits:\n",
    "        totalWidth += IMAGES['numbers'][digit].get_width()\n",
    "\n",
    "    Xoffset = (SCREENWIDTH - totalWidth) / 2\n",
    "\n",
    "    for digit in scoreDigits:\n",
    "        SCREEN.blit(IMAGES['numbers'][digit], (Xoffset, SCREENHEIGHT * 0.1))\n",
    "        Xoffset += IMAGES['numbers'][digit].get_width()\n",
    "\n",
    "\n",
    "def checkCrash(player, upperPipes, lowerPipes):\n",
    "    \"\"\"returns True if player collders with base or pipes.\"\"\"\n",
    "    pi = player['index']\n",
    "    player['w'] = IMAGES['player'][0].get_width()\n",
    "    player['h'] = IMAGES['player'][0].get_height()\n",
    "\n",
    "    # if player crashes into ground\n",
    "    if player['y'] + player['h'] >= BASEY - 1:\n",
    "        return True\n",
    "    else:\n",
    "\n",
    "        playerRect = pygame.Rect(player['x'], player['y'],\n",
    "                      player['w'], player['h'])\n",
    "\n",
    "        for uPipe, lPipe in zip(upperPipes, lowerPipes):\n",
    "            # upper and lower pipe rects\n",
    "            uPipeRect = pygame.Rect(uPipe['x'], uPipe['y'], PIPE_WIDTH, PIPE_HEIGHT)\n",
    "            lPipeRect = pygame.Rect(lPipe['x'], lPipe['y'], PIPE_WIDTH, PIPE_HEIGHT)\n",
    "\n",
    "            # player and upper/lower pipe hitmasks\n",
    "            pHitMask = HITMASKS['player'][pi]\n",
    "            uHitmask = HITMASKS['pipe'][0]\n",
    "            lHitmask = HITMASKS['pipe'][1]\n",
    "\n",
    "            # if bird collided with upipe or lpipe\n",
    "            uCollide = pixelCollision(playerRect, uPipeRect, pHitMask, uHitmask)\n",
    "            lCollide = pixelCollision(playerRect, lPipeRect, pHitMask, lHitmask)\n",
    "\n",
    "            if uCollide or lCollide:\n",
    "                return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def pixelCollision(rect1, rect2, hitmask1, hitmask2):\n",
    "    \"\"\"Checks if two objects collide and not just their rects\"\"\"\n",
    "    rect = rect1.clip(rect2)\n",
    "\n",
    "    if rect.width == 0 or rect.height == 0:\n",
    "        return False\n",
    "\n",
    "    x1, y1 = rect.x - rect1.x, rect.y - rect1.y\n",
    "    x2, y2 = rect.x - rect2.x, rect.y - rect2.y\n",
    "\n",
    "    for x in range(rect.width):\n",
    "        for y in range(rect.height):\n",
    "            if hitmask1[x1+x][y1+y] and hitmask2[x2+x][y2+y]:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flappy no graphics for DQL\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import random\n",
    "import pygame\n",
    "import game.flappy_bird_utils as flappy_bird_utils\n",
    "import pygame.surfarray as surfarray\n",
    "from pygame.locals import *\n",
    "from itertools import cycle\n",
    "\n",
    "FPS = 30\n",
    "SCREENWIDTH  = 288\n",
    "SCREENHEIGHT = 512\n",
    "\n",
    "pygame.init()\n",
    "FPSCLOCK = pygame.time.Clock()\n",
    "SCREEN = pygame.display.set_mode((SCREENWIDTH, SCREENHEIGHT))\n",
    "pygame.display.set_caption('Flappy Bird')\n",
    "\n",
    "IMAGES, SOUNDS, HITMASKS = flappy_bird_utils.load()\n",
    "PIPEGAPSIZE = 100 # gap between upper and lower part of pipe\n",
    "BASEY = SCREENHEIGHT * 0.79\n",
    "\n",
    "PLAYER_WIDTH = IMAGES['player'][0].get_width()\n",
    "PLAYER_HEIGHT = IMAGES['player'][0].get_height()\n",
    "PIPE_WIDTH = IMAGES['pipe'][0].get_width()\n",
    "PIPE_HEIGHT = IMAGES['pipe'][0].get_height()\n",
    "BACKGROUND_WIDTH = IMAGES['background'].get_width()\n",
    "\n",
    "PLAYER_INDEX_GEN = cycle([0, 1, 2, 1])\n",
    "\n",
    "\n",
    "class GameNoGraphics:\n",
    "    def __init__(self):\n",
    "        self.score = self.playerIndex = self.loopIter = 0\n",
    "        self.playerx = int(SCREENWIDTH * 0.2)\n",
    "        self.playery = int((SCREENHEIGHT - PLAYER_HEIGHT) / 2)\n",
    "        self.basex = 0\n",
    "        self.baseShift = IMAGES['base'].get_width() - BACKGROUND_WIDTH\n",
    "\n",
    "        newPipe1 = getRandomPipe()\n",
    "        newPipe2 = getRandomPipe()\n",
    "        self.upperPipes = [\n",
    "            {'x': SCREENWIDTH, 'y': newPipe1[0]['y']},\n",
    "            {'x': SCREENWIDTH + (SCREENWIDTH / 2), 'y': newPipe2[0]['y']},\n",
    "        ]\n",
    "        self.lowerPipes = [\n",
    "            {'x': SCREENWIDTH, 'y': newPipe1[1]['y']},\n",
    "            {'x': SCREENWIDTH + (SCREENWIDTH / 2), 'y': newPipe2[1]['y']},\n",
    "        ]\n",
    "\n",
    "        # player velocity, max velocity, downward accleration, accleration on flap\n",
    "        self.pipeVelX = -4\n",
    "        self.playerVelY    =  0    # player's velocity along Y, default same as playerFlapped\n",
    "        self.playerMaxVelY =  10   # max vel along Y, max descend speed\n",
    "        self.playerMinVelY =  -8   # min vel along Y, max ascend speed\n",
    "        self.playerAccY    =   1   # players downward accleration\n",
    "        self.playerFlapAcc =  -9   # players speed on flapping\n",
    "        self.playerFlapped = False # True when player flaps\n",
    "\n",
    "    def get_next_pipe_index(self):\n",
    "        playerMidPos = self.playerx + PLAYER_WIDTH / 2\n",
    "        distance = [0, 0]\n",
    "        idx = 0\n",
    "        for pipe in self.upperPipes:\n",
    "            if (idx == 2):\n",
    "                break\n",
    "            pipeMidPos = pipe['x'] + PIPE_WIDTH / 2\n",
    "            if pipeMidPos >= playerMidPos:\n",
    "                distance[idx] = pipeMidPos - playerMidPos\n",
    "            else:\n",
    "                distance[idx] = 999\n",
    "            idx += 1\n",
    "        return distance.index(min(distance))\n",
    "\n",
    "\n",
    "    def frame_step(self, flap):\n",
    "        pygame.event.pump()\n",
    "\n",
    "        reward = 0.4\n",
    "        terminal = False\n",
    "\n",
    "        if flap:\n",
    "            if self.playery > -2 * PLAYER_HEIGHT:\n",
    "                self.playerVelY = self.playerFlapAcc\n",
    "                self.playerFlapped = True\n",
    "                #SOUNDS['wing'].play()\n",
    "\n",
    "        next_pipe_idx = self.get_next_pipe_index()\n",
    "        playerMidPos = self.playery + PLAYER_HEIGHT/2\n",
    "        pipeUpperMidPos = self.upperPipes[next_pipe_idx]['y']\n",
    "        pipeLowerMidPos = self.lowerPipes[next_pipe_idx]['y']\n",
    "        # if playerMidPos < pipeUpperMidPos + 1 and playerMidPos > pipeLowerMidPos - 1:\n",
    "        #     reward = 0.2\n",
    "        # if playerMidPos > pipeLowerMidPos - 100 and playerMidPos < pipeLowerMidPos:\n",
    "        #     # print(\"within\")\n",
    "        #     reward = 0.2\n",
    "        if playerMidPos <= 0.2*SCREENHEIGHT:\n",
    "            # print(\"too high\")\n",
    "            reward = -0.2\n",
    "        if playerMidPos <= 0.1*SCREENHEIGHT:\n",
    "            # print(\"too high\")\n",
    "            reward = -0.4\n",
    "\n",
    "\n",
    "        # check for score\n",
    "        playerMidPos = self.playerx + PLAYER_WIDTH / 2\n",
    "        for pipe in self.upperPipes:\n",
    "            pipeMidPos = pipe['x'] + PIPE_WIDTH / 2\n",
    "            if pipeMidPos <= playerMidPos < pipeMidPos + 4:\n",
    "                self.score += 1\n",
    "                #SOUNDS['point'].play()\n",
    "                reward = 10\n",
    "\n",
    "        # playerIndex basex change\n",
    "        if (self.loopIter + 1) % 3 == 0:\n",
    "            self.playerIndex = next(PLAYER_INDEX_GEN)\n",
    "        self.loopIter = (self.loopIter + 1) % 30\n",
    "        self.basex = -((-self.basex + 100) % self.baseShift)\n",
    "\n",
    "        # player's movement\n",
    "        if self.playerVelY < self.playerMaxVelY and not self.playerFlapped:\n",
    "            self.playerVelY += self.playerAccY\n",
    "        if self.playerFlapped:\n",
    "            self.playerFlapped = False\n",
    "        self.playery += min(self.playerVelY, BASEY - self.playery - PLAYER_HEIGHT)\n",
    "        if self.playery < 0:\n",
    "            self.playery = 0\n",
    "\n",
    "        # move pipes to left\n",
    "        for uPipe, lPipe in zip(self.upperPipes, self.lowerPipes):\n",
    "            uPipe['x'] += self.pipeVelX\n",
    "            lPipe['x'] += self.pipeVelX\n",
    "\n",
    "        # add new pipe when first pipe is about to touch left of screen\n",
    "        if 0 < self.upperPipes[0]['x'] < 5:\n",
    "            newPipe = getRandomPipe()\n",
    "            self.upperPipes.append(newPipe[0])\n",
    "            self.lowerPipes.append(newPipe[1])\n",
    "\n",
    "        # remove first pipe if its out of the screen\n",
    "        if self.upperPipes[0]['x'] < -PIPE_WIDTH:\n",
    "            self.upperPipes.pop(0)\n",
    "            self.lowerPipes.pop(0)\n",
    "\n",
    "        # check if crash here\n",
    "        isCrash= checkCrash({'x': self.playerx, 'y': self.playery,\n",
    "                             'index': self.playerIndex},\n",
    "                            self.upperPipes, self.lowerPipes)\n",
    "        if isCrash:\n",
    "            #SOUNDS['hit'].play()\n",
    "            #SOUNDS['die'].play()\n",
    "            terminal = True\n",
    "            self.__init__()\n",
    "            reward = -5\n",
    "\n",
    "        # # draw sprites\n",
    "        # SCREEN.blit(IMAGES['background'], (0,0))\n",
    "\n",
    "        # for uPipe, lPipe in zip(self.upperPipes, self.lowerPipes):\n",
    "        #     SCREEN.blit(IMAGES['pipe'][0], (uPipe['x'], uPipe['y']))\n",
    "        #     SCREEN.blit(IMAGES['pipe'][1], (lPipe['x'], lPipe['y']))\n",
    "\n",
    "        # SCREEN.blit(IMAGES['base'], (self.basex, BASEY))\n",
    "        # # print score so player overlaps the score\n",
    "        # # showScore(self.score)\n",
    "        # SCREEN.blit(IMAGES['player'][self.playerIndex],\n",
    "        #             (self.playerx, self.playery))\n",
    "\n",
    "        # image_data = pygame.surfarray.array3d(pygame.display.get_surface())\n",
    "        # pygame.display.update()\n",
    "        # FPSCLOCK.tick(FPS)\n",
    "        #print self.upperPipes[0]['y'] + PIPE_HEIGHT - int(BASEY * 0.2)\n",
    "        return 0, reward, terminal\n",
    "\n",
    "def getRandomPipe():\n",
    "    \"\"\"returns a randomly generated pipe\"\"\"\n",
    "    # y of gap between upper and lower pipe\n",
    "    gapYs = [20, 30, 40, 50, 60, 70, 80, 90]\n",
    "    index = random.randint(0, len(gapYs)-1)\n",
    "    gapY = gapYs[index]\n",
    "\n",
    "    gapY += int(BASEY * 0.2)\n",
    "    pipeX = SCREENWIDTH + 10\n",
    "\n",
    "    return [\n",
    "        {'x': pipeX, 'y': gapY - PIPE_HEIGHT},  # upper pipe\n",
    "        {'x': pipeX, 'y': gapY + PIPEGAPSIZE},  # lower pipe\n",
    "    ]\n",
    "\n",
    "\n",
    "def showScore(score):\n",
    "    \"\"\"displays score in center of screen\"\"\"\n",
    "    scoreDigits = [int(x) for x in list(str(score))]\n",
    "    totalWidth = 0 # total width of all numbers to be printed\n",
    "\n",
    "    for digit in scoreDigits:\n",
    "        totalWidth += IMAGES['numbers'][digit].get_width()\n",
    "\n",
    "    Xoffset = (SCREENWIDTH - totalWidth) / 2\n",
    "\n",
    "    for digit in scoreDigits:\n",
    "        SCREEN.blit(IMAGES['numbers'][digit], (Xoffset, SCREENHEIGHT * 0.1))\n",
    "        Xoffset += IMAGES['numbers'][digit].get_width()\n",
    "\n",
    "\n",
    "def checkCrash(player, upperPipes, lowerPipes):\n",
    "    \"\"\"returns True if player collders with base or pipes.\"\"\"\n",
    "    pi = player['index']\n",
    "    player['w'] = IMAGES['player'][0].get_width()\n",
    "    player['h'] = IMAGES['player'][0].get_height()\n",
    "\n",
    "    # if player crashes into ground\n",
    "    if player['y'] + player['h'] >= BASEY - 1:\n",
    "        return True\n",
    "    else:\n",
    "\n",
    "        playerRect = pygame.Rect(player['x'], player['y'],\n",
    "                      player['w'], player['h'])\n",
    "\n",
    "        for uPipe, lPipe in zip(upperPipes, lowerPipes):\n",
    "            # upper and lower pipe rects\n",
    "            uPipeRect = pygame.Rect(uPipe['x'], uPipe['y'], PIPE_WIDTH, PIPE_HEIGHT)\n",
    "            lPipeRect = pygame.Rect(lPipe['x'], lPipe['y'], PIPE_WIDTH, PIPE_HEIGHT)\n",
    "\n",
    "            # player and upper/lower pipe hitmasks\n",
    "            pHitMask = HITMASKS['player'][pi]\n",
    "            uHitmask = HITMASKS['pipe'][0]\n",
    "            lHitmask = HITMASKS['pipe'][1]\n",
    "\n",
    "            # if bird collided with upipe or lpipe\n",
    "            uCollide = pixelCollision(playerRect, uPipeRect, pHitMask, uHitmask)\n",
    "            lCollide = pixelCollision(playerRect, lPipeRect, pHitMask, lHitmask)\n",
    "\n",
    "            if uCollide or lCollide:\n",
    "                return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def pixelCollision(rect1, rect2, hitmask1, hitmask2):\n",
    "    \"\"\"Checks if two objects collide and not just their rects\"\"\"\n",
    "    rect = rect1.clip(rect2)\n",
    "\n",
    "    if rect.width == 0 or rect.height == 0:\n",
    "        return False\n",
    "\n",
    "    x1, y1 = rect.x - rect1.x, rect.y - rect1.y\n",
    "    x2, y2 = rect.x - rect2.x, rect.y - rect2.y\n",
    "\n",
    "    for x in range(rect.width):\n",
    "        for y in range(rect.height):\n",
    "            if hitmask1[x1+x][y1+y] and hitmask2[x2+x][y2+y]:\n",
    "                return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# FLAPPY BIRD CODE WITH GRAPHICS REMOVED\n",
    "\n",
    "import random\n",
    "import pygame\n",
    "from itertools import cycle\n",
    "\n",
    "SCREENWIDTH  = 288\n",
    "SCREENHEIGHT = 512\n",
    "\n",
    "IMAGES, SOUNDS, HITMASKS = load()\n",
    "PIPEGAPSIZE = 100 # gap between upper and lower part of pipe\n",
    "BASEY = SCREENHEIGHT * 0.79\n",
    "\n",
    "PLAYER_WIDTH = IMAGES['player'][0].get_width()\n",
    "PLAYER_HEIGHT = IMAGES['player'][0].get_height()\n",
    "PIPE_WIDTH = IMAGES['pipe'][0].get_width()\n",
    "PIPE_HEIGHT = IMAGES['pipe'][0].get_height()\n",
    "BACKGROUND_WIDTH = IMAGES['background'].get_width()\n",
    "\n",
    "PLAYER_INDEX_GEN = cycle([0, 1, 2, 1])\n",
    "\n",
    "\n",
    "class GameStateNoGraphics:\n",
    "    def __init__(self):\n",
    "        self.score = self.playerIndex = self.loopIter = 0\n",
    "        self.playerx = int(SCREENWIDTH * 0.2)\n",
    "        self.playery = int((SCREENHEIGHT - PLAYER_HEIGHT) / 2)\n",
    "        self.basex = 0\n",
    "        self.baseShift = IMAGES['base'].get_width() - BACKGROUND_WIDTH\n",
    "\n",
    "        newPipe1 = getRandomPipe()\n",
    "        newPipe2 = getRandomPipe()\n",
    "        self.upperPipes = [\n",
    "            {'x': SCREENWIDTH, 'y': newPipe1[0]['y']},\n",
    "            {'x': SCREENWIDTH + (SCREENWIDTH / 2), 'y': newPipe2[0]['y']},\n",
    "        ]\n",
    "        self.lowerPipes = [\n",
    "            {'x': SCREENWIDTH, 'y': newPipe1[1]['y']},\n",
    "            {'x': SCREENWIDTH + (SCREENWIDTH / 2), 'y': newPipe2[1]['y']},\n",
    "        ]\n",
    "\n",
    "        # player velocity, max velocity, downward accleration, accleration on flap\n",
    "        self.pipeVelX = -4\n",
    "        self.playerVelY    =  0    # player's velocity along Y, default same as playerFlapped\n",
    "        self.playerMaxVelY =  10   # max vel along Y, max descend speed\n",
    "        self.playerMinVelY =  -8   # min vel along Y, max ascend speed\n",
    "        self.playerAccY    =   1   # players downward accleration\n",
    "        self.playerFlapAcc =  -9   # players speed on flapping\n",
    "        self.playerFlapped = False # True when player flaps\n",
    "\n",
    "    def frame_step(self, flap):\n",
    "        if flap:\n",
    "            if self.playery > -2 * PLAYER_HEIGHT:\n",
    "                self.playerVelY = self.playerFlapAcc\n",
    "                self.playerFlapped = True\n",
    "\n",
    "        # playerIndex basex change\n",
    "        if (self.loopIter + 1) % 3 == 0:\n",
    "            self.playerIndex = next(PLAYER_INDEX_GEN)\n",
    "        self.loopIter = (self.loopIter + 1) % 30\n",
    "        self.basex = -((-self.basex + 100) % self.baseShift)\n",
    "\n",
    "        # player's movement\n",
    "        if self.playerVelY < self.playerMaxVelY and not self.playerFlapped:\n",
    "            self.playerVelY += self.playerAccY\n",
    "        if self.playerFlapped:\n",
    "            self.playerFlapped = False\n",
    "        self.playery += min(self.playerVelY, BASEY - self.playery - PLAYER_HEIGHT)\n",
    "        if self.playery < 0:\n",
    "            self.playery = 0\n",
    "\n",
    "        # move pipes to left\n",
    "        for uPipe, lPipe in zip(self.upperPipes, self.lowerPipes):\n",
    "            uPipe['x'] += self.pipeVelX\n",
    "            lPipe['x'] += self.pipeVelX\n",
    "\n",
    "        # add new pipe when first pipe is about to touch left of screen\n",
    "        if 0 < self.upperPipes[0]['x'] < 5:\n",
    "            newPipe = getRandomPipe()\n",
    "            self.upperPipes.append(newPipe[0])\n",
    "            self.lowerPipes.append(newPipe[1])\n",
    "\n",
    "        # remove first pipe if it's out of the screen\n",
    "        if self.upperPipes[0]['x'] < -PIPE_WIDTH:\n",
    "            self.upperPipes.pop(0)\n",
    "            self.lowerPipes.pop(0)\n",
    "\n",
    "        # check if crash here\n",
    "        isCrash = checkCrash({'x': self.playerx, 'y': self.playery, 'index': self.playerIndex}, self.upperPipes, self.lowerPipes)\n",
    "        if isCrash:\n",
    "            self.__init__()\n",
    "\n",
    "        return isCrash\n",
    "\n",
    "\n",
    "def getRandomPipe():\n",
    "    \"\"\"returns a randomly generated pipe\"\"\"\n",
    "    # y of gap between upper and lower pipe\n",
    "    gapYs = [20, 30, 40, 50, 60, 70, 80, 90]\n",
    "    index = random.randint(0, len(gapYs)-1)\n",
    "    gapY = gapYs[index]\n",
    "\n",
    "    gapY += int(BASEY * 0.2)\n",
    "    pipeX = SCREENWIDTH + 10\n",
    "\n",
    "    return [\n",
    "        {'x': pipeX, 'y': gapY - PIPE_HEIGHT},  # upper pipe\n",
    "        {'x': pipeX, 'y': gapY + PIPEGAPSIZE},  # lower pipe\n",
    "    ]\n",
    "\n",
    "\n",
    "def checkCrash(player, upperPipes, lowerPipes):\n",
    "    \"\"\"returns True if player collders with base or pipes.\"\"\"\n",
    "    pi = player['index']\n",
    "    player['w'] = IMAGES['player'][0].get_width()\n",
    "    player['h'] = IMAGES['player'][0].get_height()\n",
    "\n",
    "    # if player crashes into ground\n",
    "    if player['y'] + player['h'] >= BASEY - 1:\n",
    "        return True\n",
    "    else:\n",
    "\n",
    "        playerRect = pygame.Rect(player['x'], player['y'],\n",
    "                      player['w'], player['h'])\n",
    "\n",
    "        for uPipe, lPipe in zip(upperPipes, lowerPipes):\n",
    "            # upper and lower pipe rects\n",
    "            uPipeRect = pygame.Rect(uPipe['x'], uPipe['y'], PIPE_WIDTH, PIPE_HEIGHT)\n",
    "            lPipeRect = pygame.Rect(lPipe['x'], lPipe['y'], PIPE_WIDTH, PIPE_HEIGHT)\n",
    "\n",
    "            # player and upper/lower pipe hitmasks\n",
    "            pHitMask = HITMASKS['player'][pi]\n",
    "            uHitmask = HITMASKS['pipe'][0]\n",
    "            lHitmask = HITMASKS['pipe'][1]\n",
    "\n",
    "            # if bird collided with upipe or lpipe\n",
    "            uCollide = pixelCollision(playerRect, uPipeRect, pHitMask, uHitmask)\n",
    "            lCollide = pixelCollision(playerRect, lPipeRect, pHitMask, lHitmask)\n",
    "\n",
    "            if uCollide or lCollide:\n",
    "                return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def pixelCollision(rect1, rect2, hitmask1, hitmask2):\n",
    "    \"\"\"Checks if two objects collide and not just their rects\"\"\"\n",
    "    rect = rect1.clip(rect2)\n",
    "\n",
    "    if rect.width == 0 or rect.height == 0:\n",
    "        return False\n",
    "\n",
    "    x1, y1 = rect.x - rect1.x, rect.y - rect1.y\n",
    "    x2, y2 = rect.x - rect2.x, rect.y - rect2.y\n",
    "\n",
    "    for x in range(rect.width):\n",
    "        for y in range(rect.height):\n",
    "            if hitmask1[x1+x][y1+y] and hitmask2[x2+x][y2+y]:\n",
    "                return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# HELPER FUNCTION TO GET INPUT FROM FLAPPY BIRD GAME\n",
    "\n",
    "import torch\n",
    "\n",
    "def get_gamestate_info(game_state):\n",
    "    \"\"\"\n",
    "    gets coordinates of the two pipes\n",
    "    usage:          pipe_info = get_pipes_info(game_state)\n",
    "                    pipe_info[\"pipe0\"][\"upper\"][\"x\"] = x coordinate of the upper pipe of the first pipe\n",
    "    @args:          game_state\n",
    "    @returns:         \n",
    "        \"pipe0\": {\n",
    "            \"upper\": {\n",
    "                \"x\":\n",
    "                \"y\": \n",
    "            },\n",
    "            \"lower\": {\n",
    "                \"x\": \n",
    "                \"y\": \n",
    "            }\n",
    "        },\n",
    "        \"pipe1\": {\n",
    "            \"upper\": {\n",
    "                \"x\": ,\n",
    "                \"y\": \n",
    "            },\n",
    "            \"lower\": {\n",
    "                \"x\": \n",
    "                \"y\": \n",
    "            }\n",
    "        }, \n",
    "        \"player\": {\n",
    "            \"x\": \n",
    "            \"y\": \n",
    "            \"VelY\":\n",
    "            \"AccY\": \n",
    "            \"Flapped\": \n",
    "        }\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"pipe0\": {\n",
    "            \"upper\": {\n",
    "                \"x\": game_state.upperPipes[0]['x'],\n",
    "                \"y\": game_state.upperPipes[0]['y']\n",
    "            },\n",
    "            \"lower\": {\n",
    "                \"x\": game_state.lowerPipes[0]['x'],\n",
    "                \"y\": game_state.lowerPipes[0]['y']\n",
    "            }\n",
    "        },\n",
    "        \"pipe1\": {\n",
    "            \"upper\": {\n",
    "                \"x\": game_state.upperPipes[1]['x'],\n",
    "                \"y\": game_state.upperPipes[1]['y']\n",
    "            },\n",
    "            \"lower\": {\n",
    "                \"x\": game_state.lowerPipes[1]['x'],\n",
    "                \"y\": game_state.lowerPipes[1]['y']\n",
    "            }\n",
    "        },\n",
    "        \"player\": {\n",
    "            \"x\": game_state.playerx,\n",
    "            \"y\": game_state.playery,\n",
    "            \"VelY\": game_state.playerVelY,\n",
    "            \"AccY\": game_state.playerAccY,\n",
    "            \"Flapped\": game_state.playerFlapped,\n",
    "        }\n",
    "    }\n",
    "\n",
    "def get_input_layer(game_state):\n",
    "    \"\"\"\n",
    "    gets gamestate but returns it as a tensor. Use when feeding into ML algorithm\n",
    "    Arguments: game_state\n",
    "    Returns: tensor of shape (6, 1) containing same information as get_gamestate_info, but without the dictionary.\n",
    "    \"\"\"\n",
    "    return torch.tensor([game_state.lowerPipes[0]['x'], game_state.lowerPipes[0]['y'], \n",
    "                         game_state.lowerPipes[1]['x'], game_state.lowerPipes[1]['y'], \n",
    "                        game_state.playery, game_state.playerVelY, game_state.playerFlapped])\n",
    "\n",
    "\n",
    "def get_input_layer_2(game_state):\n",
    "    \"\"\"\n",
    "    gets gamestate but returns it as a np array. Use when feeding into ML algorithm\n",
    "    Arguments: game_state\n",
    "    Returns: tensor of shape (7, 1) containing same information as get_gamestate_info, but without the dictionary.\n",
    "    \"\"\"\n",
    "    # print(\"pipe height: \", game_state.lowerPipes[0]['y'])\n",
    "    return np.array([game_state.lowerPipes[0]['x'], game_state.lowerPipes[0]['y'], game_state.lowerPipes[0]['y'] - 100,\n",
    "                         game_state.lowerPipes[1]['x'], game_state.lowerPipes[1]['y'], game_state.lowerPipes[0]['y'] - 100,\n",
    "                        game_state.playery])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEAT Algorithm\n",
    "\n",
    "The NeuroEvolution of Augmenting Topologies (NEAT) Algorithm is a variation of the Genetic Algorithm approach, developed by Kenneth O. Stanley and Risto Miikkulainen in 2002 [1]. Like the Genetic Algorithm, the general approach is to generate a population of random models. Each model is then given a fitness value, depending on how well it performs at a particular task. A new population is then made through a combination operator and a mutation operator. The combination operator takes two (or more) models and combines then, and the mutation operator takes a single model and changes it slightly. The models with higher fitness are more likely to be combined with other models for the next generation, which should increase the average fitness. \n",
    "\n",
    "The traditional Genetic Algorithm approach relies on a fixed model topology, such that only the weights of the model are modified. In contrast, the NEAT algorithm begins with a simplified model, such as input nodes connecting directly to output nodes, and evolves the topology of the models too. New neurons and connections are added to existing models through the mutation operator. To keep track of the evolving model topologies, each model is assigned a set of node genes and a set of connection genes. Each gene is given a unique innovation number. When combining models, only genes with the same innovation number are combined.\n",
    " \n",
    "The NEAT algorithm also employs a technique called Speciation, where the population is grouped into species of similar topologies. Models are then more likely to be combined with other models of the same species. This helps maintain diversity in the population and prevents early convergence to a suboptimal solution. \n",
    "\n",
    "A 2006 study by Taylor, Whiteson and Stone [2] compared the NEAT algorithm to a temporal difference method SARSA. The study found that NEAT could find a better policy than SARSA, although it required more iterations to do so. The study also found that SARSA performed better when the domain was fully observable and NEAT converged quicker with a deterministic fitness function. Overall NEAT is shown to be a highly competitive algorithm for reinforcement learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'neat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31136\\1226115634.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mneat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mNEATModel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'neat'"
     ]
    }
   ],
   "source": [
    "# NEAT MODEL\n",
    "\n",
    "import os.path\n",
    "import pickle\n",
    "import neat\n",
    "\n",
    "class NEATModel:\n",
    "    def __init__(self):\n",
    "        configFile = os.path.join(os.path.abspath(''), 'NEATConfig')\n",
    "\n",
    "        self.config = neat.Config(neat.DefaultGenome, neat.DefaultReproduction,\n",
    "                                  neat.DefaultSpeciesSet, neat.DefaultStagnation,\n",
    "                                  configFile)\n",
    "\n",
    "        self.population = neat.Population(self.config)\n",
    "\n",
    "        self.population.add_reporter(neat.StdOutReporter(True))\n",
    "        stats = neat.StatisticsReporter()\n",
    "        self.population.add_reporter(stats)\n",
    "        self.population.add_reporter(neat.Checkpointer(20))\n",
    "        self.bestGenome = None\n",
    "        self.gameState = None\n",
    "\n",
    "    def run(self, generations, checkpointFileName=\"\"):\n",
    "        if checkpointFileName != \"\":\n",
    "            self.population = neat.Checkpointer.restore_checkpoint(checkpointFileName)\n",
    "        self.bestGenome = self.population.run(self.evaluateGenomes, generations)\n",
    "        with open(\"NEATBestGenome.pkl\", \"wb\") as f:\n",
    "            pickle.dump(self.bestGenome, f)\n",
    "            f.close()\n",
    "\n",
    "    def loadBest(self):\n",
    "        with open(\"NEATBestGenome.pkl\", \"rb\") as f:\n",
    "            self.bestGenome = pickle.load(f)\n",
    "\n",
    "    def playGame(self):\n",
    "        self.gameState = GameState()\n",
    "        network = neat.nn.FeedForwardNetwork.create(self.bestGenome, self.config)\n",
    "        go = True\n",
    "        while go:\n",
    "            networkInput = get_input_layer(self.gameState)\n",
    "            networkOutput = network.activate(networkInput)[0]\n",
    "            flap = networkOutput > 0.5  # sigmoid activation, output should be between 0 and 1\n",
    "            _, _, terminal = self.gameState.frame_step(flap)\n",
    "            if terminal:\n",
    "                go = False\n",
    "\n",
    "    def testBest(self, runs):\n",
    "        self.gameState = GameStateNoGraphics()\n",
    "        network = neat.nn.FeedForwardNetwork.create(self.bestGenome, self.config)\n",
    "        fitnesses = []\n",
    "        for i in range(runs):\n",
    "            thisRunFitness = 0\n",
    "            go = True\n",
    "            while go:\n",
    "                thisRunFitness += 1\n",
    "                networkInput = get_input_layer(self.gameState)\n",
    "                networkOutput = network.activate(networkInput)[0]\n",
    "                flap = networkOutput > 0.5  # sigmoid activation, output should be between 0 and 1\n",
    "                if self.gameState.frame_step(flap) or thisRunFitness > 10000:\n",
    "                    go = False\n",
    "                    fitnesses.append(thisRunFitness)\n",
    "                    if (i+1) % 100 == 0:\n",
    "                        print(\"Finished run: \" + str(i+1) + \"/\" + str(runs))\n",
    "        print(fitnesses)\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluateGenomes(genomes, config):\n",
    "        gameState = GameStateNoGraphics()\n",
    "        for genome_id, genome in genomes:\n",
    "            network = neat.nn.FeedForwardNetwork.create(genome, config)\n",
    "            runs = 10\n",
    "            averageFitness = 0\n",
    "            for i in range(runs):\n",
    "                thisRunFitness = 0\n",
    "                go = True\n",
    "                while go:\n",
    "                    thisRunFitness += 1\n",
    "                    networkInput = get_input_layer(gameState)\n",
    "                    networkOutput = network.activate(networkInput)[0]\n",
    "                    flap = networkOutput > 0.5  # sigmoid activation, output should be between 0 and 1\n",
    "                    if gameState.frame_step(flap) or thisRunFitness > 10000:\n",
    "                        go = False\n",
    "                        averageFitness += thisRunFitness / runs\n",
    "            genome.fitness = averageFitness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run: 100/1000\n",
      "Finished run: 200/1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m neatModel\u001b[38;5;241m.\u001b[39mloadBest()  \u001b[38;5;66;03m# Load the best model from training\u001b[39;00m\n\u001b[0;32m      6\u001b[0m neatModel\u001b[38;5;241m.\u001b[39mplayGame()  \u001b[38;5;66;03m# Watch the best model play the game (look at your taskbar it won't popup automatically)\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mneatModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtestBest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Test performance of the best model\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 58\u001b[0m, in \u001b[0;36mNEATModel.testBest\u001b[1;34m(self, runs)\u001b[0m\n\u001b[0;32m     56\u001b[0m thisRunFitness \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     57\u001b[0m networkInput \u001b[38;5;241m=\u001b[39m get_input_layer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgameState)\n\u001b[1;32m---> 58\u001b[0m networkOutput \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetworkInput\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     59\u001b[0m flap \u001b[38;5;241m=\u001b[39m networkOutput \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m  \u001b[38;5;66;03m# sigmoid activation, output should be between 0 and 1\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgameState\u001b[38;5;241m.\u001b[39mframe_step(flap) \u001b[38;5;129;01mor\u001b[39;00m thisRunFitness \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m10000\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\neat\\nn\\feed_forward.py:24\u001b[0m, in \u001b[0;36mFeedForwardNetwork.activate\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     22\u001b[0m         node_inputs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues[i] \u001b[38;5;241m*\u001b[39m w)\n\u001b[0;32m     23\u001b[0m     s \u001b[38;5;241m=\u001b[39m agg_func(node_inputs)\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues[node] \u001b[38;5;241m=\u001b[39m \u001b[43mact_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_nodes]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\neat\\activations.py:12\u001b[0m, in \u001b[0;36msigmoid_activation\u001b[1;34m(z)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msigmoid_activation\u001b[39m(z):\n\u001b[1;32m---> 12\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m60.0\u001b[39m, \u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m60.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m+\u001b[39m math\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mz))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\tensor.py:27\u001b[0m, in \u001b[0;36m_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAINING AND TESTING THE NEAT MODEL\n",
    "\n",
    "neatModel = NEATModel()\n",
    "# neatModel.run(300)  # Train the model, will take quite a long time!\n",
    "neatModel.loadBest()  # Load the best model from training\n",
    "neatModel.playGame()  # Watch the best model play the game (look at your taskbar it won't popup automatically)\n",
    "neatModel.testBest(1000)  # Test performance of the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### NEAT Algorithm\n",
    "\n",
    "The training terminated after about 120 generations. Given that each generation had 150 models, the total number of models generated was 18000. The best model was run 1000 times to get a more accurate assessment of its performance. The mean fitness was 479, the median fitness was 234, and the standard deviation was 546. The graph below shows a histogram of the fitness of the 1000 runs.\n",
    "![Histogram of testing best NEAT Model](NEATHistogram.png)\n",
    "Noticeably 259 (~26%) of runs had a fitness of 86, which occurs when the model fails to get past the first pipe. The best run had a fitness of 3823. At 30 frames per second, this is equivalent to surviving for about 127 seconds.\n",
    "\n",
    "## Discussion\n",
    "\n",
    "### NEAT Algorithm\n",
    "\n",
    "The NEAT Algorithm resulted in a model that performed reasonably well, although with quite a large variance. With a median fitness of 234, this is equivalent to surviving for 7.8 seconds, which is similar to the performance of an average human player. Further training would likely improve the model further, but more care should be taken to avoid outlier runs from overestimating the performance of the model. This could be done by taking the median of a number of runs rather than the mean. Increasing the number of runs each model is tested on would also help reduce inaccuracy in performance evaluation, but this comes with an increase in training time. One approach could be to start with only a single run, and increase the number of runs with subsequent generations. This would help prevent multiple runs being wasted on early models that consistently perform poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Kenneth O. Stanley and Risto Miikkulainen, Efficient Evolution of Neural Network Topologies, 2002, The University of Texas<br>\n",
    "[2] Matthew E. Taylor, Shimon Whiteson, and Peter Stone, Comparing Evolutionary and Temporal Difference Methods in a Reinforcement Learning Domain, 2006, Proceedings of the Genetic and Evolutionary Computation Conference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "# from utils import get_input_layer_2 as input\n",
    "# import game.flappyNoGraphics as Game\n",
    "# import game.wrapped_flappy_bird as GameVisual\n",
    "from collections import deque\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "# define the neural net of the agent\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, lr):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.n_actions = 2\n",
    "        self.hid_1 = 128\n",
    "        self.hid_2 = 128\n",
    "        self.hid_3 = 128\n",
    "        self.inputs = 7 * 4\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(self.inputs, self.hid_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hid_1, self.hid_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hid_2, self.hid_3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hid_3, self.n_actions),\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss = nn.SmoothL1Loss()\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model(x.type(torch.FloatTensor).to(self.device))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a class to manage the state. Each state is a stack of 4 \"frame\" of the game\n",
    "# which provides the agent on information of the bird's movement.\n",
    "\n",
    "from collections import deque\n",
    "import torch\n",
    "# from utils import get_input_layer_2 as input\n",
    "import numpy as np\n",
    "\n",
    "class StateManager(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "        starting_state = [0, 0, 0, 0, 0, 0, 0]\n",
    "        for _ in range(capacity):\n",
    "            self.memory.append(starting_state)\n",
    "\n",
    "    def push(self, game):\n",
    "        \"\"\"Save a frame, \n",
    "            returns tensor of flattened state frames\n",
    "        \"\"\"\n",
    "        state_frame = get_input_layer_2(game)\n",
    "        self.memory.popleft()\n",
    "        self.memory.append(state_frame)\n",
    "        tensor_list = []\n",
    "        for i in range(4):\n",
    "            tensor_list.append(self.memory[i])\n",
    "        return np.array(tensor_list).flatten()\n",
    "    \n",
    "    def get(self):\n",
    "        # return np array of state frames\n",
    "        tensor_list = []\n",
    "        for i in range(4):\n",
    "            tensor_list.append(self.memory[i])\n",
    "        return np.array(tensor_list).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the class for the agent.\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Porperties:\n",
    "            gamma (float): Future reward discount rate.\n",
    "            epsilon (float): Probability for choosing random policy.\n",
    "            epsilon_decay (float): Rate at which epsilon decays toward zero.\n",
    "            learning_rate (float): Learning rate for Adam optimizer.\n",
    "\n",
    "        Returns:\n",
    "            Agent\n",
    "        \"\"\"\n",
    "        # constant parameters\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.996\n",
    "        self.lr = 0.00005\n",
    "        self.batch_size = 64\n",
    "        self.max_mem_size = 10000\n",
    "        # self.input_dims = 7 * 4\n",
    "\n",
    "        #variable parameters\n",
    "        self.epsilon = 0.01\n",
    "        self.mem_cntr = 0\n",
    "        self.mem_cntr_successful = 0\n",
    "\n",
    "        # initializing memory\n",
    "        self.memory = deque(maxlen=self.max_mem_size)\n",
    "        self.memory_successful = deque(maxlen=1000)\n",
    "        self.episodic_memory = []\n",
    "\n",
    "        #initialize networks\n",
    "        self.network = Network(self.lr)\n",
    "\n",
    "    def save_experience(self):\n",
    "        with open('Models/DQL/experience.pickle', 'wb') as handle:\n",
    "            pickle.dump(self.memory, handle)\n",
    "        with open('Models/DQL/experience_successful.pickle', 'wb') as handle:\n",
    "            pickle.dump(self.memory_successful, handle)\n",
    "\n",
    "    def load_experience(self):\n",
    "        with open('Models/DQL/experience.pickle', 'rb') as handle:\n",
    "            self.memory = pickle.load(handle)\n",
    "        with open('Models/DQL/experience_successful.pickle', 'rb') as handle:\n",
    "            self.memory_successful = pickle.load(handle)\n",
    "\n",
    "    def getMemory(self):\n",
    "        return self.memory\n",
    "\n",
    "    def nextEpisode(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def getepsilon(self):\n",
    "        return self.epsilon\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, game_over, score, next_reward):\n",
    "        if (self.mem_cntr >= self.max_mem_size - 2):\n",
    "            for i in range(self.max_mem_size - 3000):\n",
    "                self.memory.popleft()\n",
    "            self.mem_cntr = len(self.memory) - 1\n",
    "\n",
    "        memory = [state, action, reward, next_state, game_over, score, next_reward]\n",
    "        self.memory.append(memory)\n",
    "\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def remember_successful(self, state, action, reward, next_state, game_over, score, next_reward):\n",
    "        if (self.mem_cntr_successful >= 1000 - 20):\n",
    "            for i in range(1000 - 500):\n",
    "                self.memory_successful.popleft()\n",
    "            self.mem_cntr_successful = len(self.memory_successful) - 1\n",
    "\n",
    "        memory = [state, action, reward, next_state, game_over, score, next_reward]\n",
    "        self.memory_successful.append(memory)\n",
    "\n",
    "        self.mem_cntr_successful += 1\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            # exploration\n",
    "\n",
    "            # 2 in 30 = averages about 1 press every 0.5 seconds which is in the ballpark of whats required to play the game. \n",
    "            # Gives bot best start possible (as it actually has a chance of making it through the first block!)\n",
    "            # in flappy bird a flap changes the gamestate a lot more than a no-flap.\n",
    "            determiner = np.random.randint(0, 30);\n",
    "            if (determiner <= 2):\n",
    "                return 1\n",
    "            return 0\n",
    "        else:\n",
    "            # exploitation, select epsilon-greedy action.\n",
    "                state_tensor = torch.tensor([state]).to(self.network.device, dtype=torch.int32)\n",
    "                action = torch.argmax(self.network.forward(state_tensor)).item()\n",
    "                \n",
    "        return action\n",
    "    \n",
    "    def updateEpsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        learn from a random batch of experiences\n",
    "        \"\"\"\n",
    "        if self.mem_cntr < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        self.network.optimizer.zero_grad()\n",
    "        max_mem = min(self.mem_cntr, self.max_mem_size)\n",
    "        batch = np.random.choice(max_mem, self.batch_size, replace=False)\n",
    "\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "\n",
    "        # memory = [state, action, reward, next_state, game_over, score, next_reward]\n",
    "        state_batch = torch.tensor([self.memory[i][0] for i in batch]).to(self.network.device, dtype=torch.float32)\n",
    "        action_batch = torch.tensor([self.memory[i][1] for i in batch])\n",
    "        reward_batch = torch.tensor([self.memory[i][2] for i in batch]).to(self.network.device, dtype=torch.float32)\n",
    "        new_state_batch = torch.tensor([self.memory[i][3] for i in batch]).to(self.network.device, dtype=torch.float32)\n",
    "        game_over_batch = torch.tensor([self.memory[i][4] for i in batch]).to(self.network.device, dtype=torch.bool)\n",
    "\n",
    "        #estimate q(s,a) and q(s',a').\n",
    "        q_current = self.network.forward(state_batch)[batch_index, action_batch]\n",
    "        q_next = self.network.forward(new_state_batch)\n",
    "        q_next[game_over_batch] = 0.0\n",
    "        # q(st,at) = r + gamma * max(q(s',a')\n",
    "        q_target = reward_batch + self.gamma * torch.max(q_next, dim=1)[0]\n",
    "\n",
    "        # smoothl1 loss and back-propagation\n",
    "        loss = self.network.loss(q_target, q_current).to(self.network.device)\n",
    "        loss.backward()\n",
    "        #prevent exploding gradient\n",
    "        torch.nn.utils.clip_grad_value_(self.network.parameters(), 100)\n",
    "        self.network.optimizer.step()\n",
    "\n",
    "    def learn_successful(self):\n",
    "        \"\"\"\n",
    "        learn from the set of experience that the agent was successful in. \n",
    "        Incentivises the agent to \n",
    "        \"\"\"\n",
    "        if self.mem_cntr_successful < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # print(\"learning successful\")\n",
    "        self.network.optimizer.zero_grad()\n",
    "        max_mem = min(self.mem_cntr_successful, self.max_mem_size)\n",
    "        batch = np.random.choice(max_mem, self.batch_size, replace=False)\n",
    "\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "\n",
    "        # memory = [state, action, reward, next_state, game_over, score]\n",
    "        state_batch = torch.tensor([self.memory[i][0] for i in batch]).to(self.network.device, dtype=torch.float32)\n",
    "        action_batch = torch.tensor([self.memory[i][1] for i in batch])\n",
    "        reward_batch = torch.tensor([self.memory[i][2] for i in batch]).to(self.network.device, dtype=torch.float32)\n",
    "        new_state_batch = torch.tensor([self.memory[i][3] for i in batch]).to(self.network.device, dtype=torch.float32)\n",
    "        game_over_batch = torch.tensor([self.memory[i][4] for i in batch]).to(self.network.device, dtype=torch.bool)\n",
    "        #estimate q(s,a) and q(s',a').\n",
    "        q_current = self.network.forward(state_batch)[batch_index, action_batch]\n",
    "        q_next = self.network.forward(new_state_batch)\n",
    "        q_next[game_over_batch] = 0.0\n",
    "        # q(st,at) = r + gamma * max(q(s',a')\n",
    "        q_target = reward_batch + self.gamma * torch.max(q_next, dim=1)[0]\n",
    "        \n",
    "        # smoothl1 loss and back-propagation\n",
    "        loss = self.network.loss(q_target, q_current).to(self.network.device)\n",
    "        loss.backward()\n",
    "        self.network.optimizer.step()\n",
    "\n",
    "    def update_episodic_memory(self, state, action, reward, next_state, done, score, current_step):\n",
    "        \"\"\"appends to a temporary memory. The temporary memory is uploaded to the main memory\n",
    "        once a game is complete.\n",
    "        \"\"\"\n",
    "        self.episodic_memory.append([state, action, reward, next_state, done, score, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the trainer allows a human to play the game then upload the relevant data to the agent.\n",
    "# increases the rate at which the agent initially learns.\n",
    "import keyboard\n",
    "import pickle\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self, agent):\n",
    "        self.runs = 10\n",
    "        self.agent = agent\n",
    "        self.game = GameState()\n",
    "        self.state_manager = StateManager(4)\n",
    "\n",
    "    def play(self, runs=10):\n",
    "        self.runs = runs\n",
    "        self.agent.episodic_memory = []\n",
    "        current_step = 0\n",
    "        # for runs amount of games\n",
    "        for i in range(self.runs):\n",
    "            #initialize game\n",
    "            self.game = GameState()\n",
    "            self.state_manager = StateManager(4)\n",
    "            state = self.state_manager.get()\n",
    "            done = False\n",
    "            score = 0\n",
    "            # manually play the game\n",
    "            while not done:\n",
    "                if keyboard.is_pressed(\" \"):\n",
    "                    action = 1\n",
    "                    _, reward, _ = self.game.frame_step(True)\n",
    "                else:\n",
    "                    action = 0\n",
    "                    _, reward, _ = self.game.frame_step(False)\n",
    "                if (reward == -5):\n",
    "                    done = True\n",
    "                    final_score = score\n",
    "                    reward = -5\n",
    "                score += reward\n",
    "\n",
    "                self.state_manager.push(self.game)\n",
    "                \n",
    "                #upload experience and train the agent on the human gameplay\n",
    "                next_state = self.state_manager.get()\n",
    "                self.agent.update_episodic_memory(state, action, reward, next_state, done, score, current_step)\n",
    "                self.agent.learn()\n",
    "                self.agent.learn_successful()\n",
    "                current_step += 1\n",
    "                state = next_state\n",
    "            for frame in self.agent.episodic_memory:\n",
    "                self.agent.remember(frame[0], frame[1], frame[2], frame[3], frame[4], frame[5], frame[6])\n",
    "                self.agent.remember_successful(frame[0], frame[1], frame[2], frame[3], frame[4], frame[5], frame[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main training loop\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def train():\n",
    "\n",
    "    agent = Agent()\n",
    "    scores, median_scores, eps_history, time_history, time_median = [], [], [], [], []\n",
    "    n_games = 100000\n",
    "    success_threshold = 15\n",
    "    trainer = Trainer(agent)\n",
    "\n",
    "    # trainer.play(10)\n",
    "    # agent.save_experience()\n",
    "    agent.load_experience()\n",
    "    #learn from human experiences for a headstart\n",
    "    #without this, the agent typically just default to a policy of only flapping or only doing nothing\n",
    "    for i in range(100):\n",
    "        agent.learn()\n",
    "\n",
    "    # for n_games amount of games\n",
    "    for i in range(n_games):\n",
    "        #initialize game\n",
    "        game = GameNoGraphics()\n",
    "        if (keyboard.is_pressed(\"p\")):\n",
    "            game = GameState()\n",
    "        score = 0\n",
    "        game_over = False\n",
    "        state_manager = StateManager(4)\n",
    "        state = state_manager.get()\n",
    "        done = False\n",
    "        # state, action, reward, next_state, done, score\n",
    "        agent.episodic_memory = []\n",
    "        current_step = 0\n",
    "        #while the game is not complete\n",
    "        while not done:\n",
    "            #select an action\n",
    "            action = agent.select_action(state)\n",
    "            _, reward, _ = game.frame_step(action)\n",
    "            #calculate the state\n",
    "            state_manager.push(game)\n",
    "            next_state = state_manager.get()\n",
    "            if (reward == -5):\n",
    "                done = True\n",
    "                final_score = score\n",
    "                reward = -5\n",
    "            score += reward\n",
    "            #remember the action taken\n",
    "            # agent.remember(state, action, reward, next_state, done, score)\n",
    "            agent.update_episodic_memory(state, action, reward, next_state, done, score, current_step)\n",
    "            \n",
    "\n",
    "            state = next_state\n",
    "            current_step += 1\n",
    "        #after each game, learn a random batch of experiences from memory\n",
    "        #and also lean a batch of experiences that the bird was successful in.\n",
    "        agent.learn()\n",
    "        agent.learn_successful()\n",
    "\n",
    "        agent.updateEpsilon()\n",
    "        #upload memory to main memory\n",
    "        eps_history.append(agent.epsilon)\n",
    "        for frame in agent.episodic_memory:\n",
    "            agent.remember(frame[0], frame[1], frame[2], frame[3], frame[4], frame[5], frame[6])\n",
    "        # success_threshold is typically 10 greater than the median score.\n",
    "        if (score > success_threshold):\n",
    "            agent.remember_successful(frame[0], frame[1], frame[2], frame[3], frame[4], frame[5], frame[6])\n",
    "        # agent.remember(state, action, reward, next_state, done, score)\n",
    "\n",
    "        #calculate some statistics for evaluation\n",
    "        median_score = np.median(scores[-100:])\n",
    "        success_threshold = max(success_threshold, median_score + 10)\n",
    "        scores.append(score)\n",
    "        median_scores.append(median_score)\n",
    "        time_history.append(current_step/30)\n",
    "        median_t = np.median(time_history[-100:])\n",
    "        time_median.append(median_t)\n",
    "        if ((i % 100) == 0):\n",
    "            print('episode: ', i,'score: %.2f' % score,\n",
    "                    ' median score %.2f' % median_score, 'time %.2f' % (current_step/30),'median time %.2f' % (median_t) ,'epsilon %.2f' % agent.epsilon)\n",
    "        if (keyboard.is_pressed(\"`\")):\n",
    "            break\n",
    "    #save the final model\n",
    "    torch.save(agent.network.state_dict(), 'Models/DQL/dqlmodel.pth')\n",
    "    #plot.\n",
    "    plt.plot(time_median)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  0 score: -8.40  median score nan time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  100 score: 39.80  median score 8.60 time 2.97 median time 1.67 epsilon 0.01\n",
      "episode:  200 score: 10.20  median score 14.60 time 1.30 median time 1.67 epsilon 0.01\n",
      "episode:  300 score: 8.60  median score 8.60 time 1.17 median time 1.17 epsilon 0.01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31136\\900881219.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#run the DQL model training.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31136\\4229028515.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;31m#and also lean a batch of experiences that the bird was successful in.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn_successful\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdateEpsilon\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31136\\327844496.py\u001b[0m in \u001b[0;36mlearn_successful\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_current\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate_episodic_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\vscode\\python\\COMP9444Project\\venv\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m                     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\vscode\\python\\COMP9444Project\\venv\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'differentiable'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\vscode\\python\\COMP9444Project\\venv\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[0;32m    250\u001b[0m                  \u001b[0mfused\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'fused'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m                  \u001b[0mgrad_scale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgrad_scale\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m                  found_inf=found_inf)\n\u001b[0m\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\vscode\\python\\COMP9444Project\\venv\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    314\u001b[0m          \u001b[0mdifferentiable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdifferentiable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m          \u001b[0mgrad_scale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgrad_scale\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m          found_inf=found_inf)\n\u001b[0m\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\vscode\\python\\COMP9444Project\\venv\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    408\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 410\u001b[1;33m                 \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m             \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#run the DQL model training.\n",
    "\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
