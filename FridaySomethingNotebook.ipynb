{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Friday Something Flappy Bird AI\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Flappy bird is a mobile game which became hugely popular in early 2014. In the game, the player controls a bird tasked with flying between gaps of green pipes. The bird continuously falls downwards, unless the player taps the screen, giving the bird a short burst of upwards speed. The goal of this project is to develop an artificial intellegence that can play Flappy Bird at a level comparable to or beyond a regular human player.\n",
    "\n",
    "## Reinforcement Learning Task\n",
    "\n",
    "Given that nature of the problem, reinforcement learning is the most appropriate learning paradigm. The task will be for the AI to, given some input, determine whether or not to \"flap\" at each frame. Every frame the bird does not crash into a pipe or the floor, the AI will be granted a reward of 1. This will encourage the AI to survive as long as possible.\n",
    "\n",
    "## Exploratory Analysis of Reinforcement Learning Task\n",
    "\n",
    "The task is resonably simple, the main decision to make is what input should be given for the AI. There are two options:\n",
    "1. Take a screenshot of the game, and use a convolutional neural network to extract important features (player, obstacles)\n",
    "1. Directly use player position and obstacle position as inputs\n",
    "\n",
    "It was chosen to use the second option as it would make the model simpler, could train faster as there is no need to draw graphics, and likely converge in fewer iterations as the model does not need to first learn what features are important. Seven inputs where given to the model, these were:\n",
    "- First pipe x coordinate\n",
    "- First pipe y coordinate\n",
    "- Second pipe x coordinate\n",
    "- Second pipe y coordinate\n",
    "- Player y coordinate\n",
    "- Player y velocity\n",
    "- Whether or not the player flapped on the previous frame\n",
    "In the NEAT Model these seven inputs were adequate, but in the Q-Learning model these inputs of the previous three frames where also given, totalling 28 inputs.\n",
    "\n",
    "## Models\n",
    "\n",
    "Given that the task was a non-episodic reinforcement learning problem it was necessary to use models that could train under these conditions. Some of the options available included:\n",
    "- Value Function Learning\n",
    "    - TD-Learning\n",
    "    - Q-Learning\n",
    "        - Deep Q-Networks (DQN)\n",
    "        - Double DQN\n",
    "- Policy Learning\n",
    "    - Evolutionary Strategies\n",
    "        - Genetic Algorithm\n",
    "        - NEAT\n",
    "    - Policy Gradients\n",
    "- Actor-Critic\n",
    "\n",
    "NEAT and Q-Learning were chosen because of the precedent of these methods being used for similar tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flappy bird game, and graphics is credit of https://github.com/yenchenlin/DeepLearningFlappyBird.\n",
    "\n",
    "Yen Chen Lin utlized a CNN to read the screen pixels to play flappy bird, we have utilised his game but adapted the outputs so our models could learn the game on a few basic parameters such as [player position, pipes position, player velocity]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.7.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# FLAPPY BIRD GRAPHICS FILES FROM https://github.com/yenchenlin/DeepLearningFlappyBird\n",
    "\n",
    "import pygame\n",
    "import sys\n",
    "def load():\n",
    "    # path of player with different states\n",
    "    PLAYER_PATH = (\n",
    "            'assets/sprites/redbird-upflap.png',\n",
    "            'assets/sprites/redbird-midflap.png',\n",
    "            'assets/sprites/redbird-downflap.png'\n",
    "    )\n",
    "\n",
    "    # path of background\n",
    "    BACKGROUND_PATH = 'assets/sprites/background-black.png'\n",
    "\n",
    "    # path of pipe\n",
    "    PIPE_PATH = 'assets/sprites/pipe-green.png'\n",
    "\n",
    "    IMAGES, SOUNDS, HITMASKS = {}, {}, {}\n",
    "\n",
    "    # numbers sprites for score display\n",
    "    IMAGES['numbers'] = (\n",
    "        pygame.image.load('assets/sprites/0.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/1.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/2.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/3.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/4.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/5.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/6.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/7.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/8.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/9.png').convert_alpha()\n",
    "    )\n",
    "\n",
    "    # base (ground) sprite\n",
    "    IMAGES['base'] = pygame.image.load('assets/sprites/base.png').convert_alpha()\n",
    "\n",
    "    # sounds\n",
    "    if 'win' in sys.platform:\n",
    "        soundExt = '.wav'\n",
    "    else:\n",
    "        soundExt = '.ogg'\n",
    "\n",
    "    SOUNDS['die']    = pygame.mixer.Sound('assets/audio/die' + soundExt)\n",
    "    SOUNDS['hit']    = pygame.mixer.Sound('assets/audio/hit' + soundExt)\n",
    "    SOUNDS['point']  = pygame.mixer.Sound('assets/audio/point' + soundExt)\n",
    "    SOUNDS['swoosh'] = pygame.mixer.Sound('assets/audio/swoosh' + soundExt)\n",
    "    SOUNDS['wing']   = pygame.mixer.Sound('assets/audio/wing' + soundExt)\n",
    "\n",
    "    # select random background sprites\n",
    "    IMAGES['background'] = pygame.image.load(BACKGROUND_PATH).convert()\n",
    "\n",
    "    # select random player sprites\n",
    "    IMAGES['player'] = (\n",
    "        pygame.image.load(PLAYER_PATH[0]).convert_alpha(),\n",
    "        pygame.image.load(PLAYER_PATH[1]).convert_alpha(),\n",
    "        pygame.image.load(PLAYER_PATH[2]).convert_alpha(),\n",
    "    )\n",
    "\n",
    "    # select random pipe sprites\n",
    "    IMAGES['pipe'] = (\n",
    "        pygame.transform.rotate(\n",
    "            pygame.image.load(PIPE_PATH).convert_alpha(), 180),\n",
    "        pygame.image.load(PIPE_PATH).convert_alpha(),\n",
    "    )\n",
    "\n",
    "    # hismask for pipes\n",
    "    HITMASKS['pipe'] = (\n",
    "        getHitmask(IMAGES['pipe'][0]),\n",
    "        getHitmask(IMAGES['pipe'][1]),\n",
    "    )\n",
    "\n",
    "    # hitmask for player\n",
    "    HITMASKS['player'] = (\n",
    "        getHitmask(IMAGES['player'][0]),\n",
    "        getHitmask(IMAGES['player'][1]),\n",
    "        getHitmask(IMAGES['player'][2]),\n",
    "    )\n",
    "\n",
    "    return IMAGES, SOUNDS, HITMASKS\n",
    "\n",
    "def getHitmask(image):\n",
    "    \"\"\"returns a hitmask using an image's alpha.\"\"\"\n",
    "    mask = []\n",
    "    for x in range(image.get_width()):\n",
    "        mask.append([])\n",
    "        for y in range(image.get_height()):\n",
    "            mask[x].append(bool(image.get_at((x,y))[3]))\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# FLAPPY BIRD CODE FROM https://github.com/yenchenlin/DeepLearningFlappyBird\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import random\n",
    "import pygame\n",
    "import pygame.surfarray as surfarray\n",
    "from pygame.locals import *\n",
    "from itertools import cycle\n",
    "\n",
    "FPS = 30\n",
    "SCREENWIDTH  = 288\n",
    "SCREENHEIGHT = 512\n",
    "\n",
    "pygame.init()\n",
    "FPSCLOCK = pygame.time.Clock()\n",
    "SCREEN = pygame.display.set_mode((SCREENWIDTH, SCREENHEIGHT))\n",
    "pygame.display.set_caption('Flappy Bird')\n",
    "\n",
    "IMAGES, SOUNDS, HITMASKS = load()\n",
    "PIPEGAPSIZE = 100 # gap between upper and lower part of pipe\n",
    "BASEY = SCREENHEIGHT * 0.79\n",
    "\n",
    "PLAYER_WIDTH = IMAGES['player'][0].get_width()\n",
    "PLAYER_HEIGHT = IMAGES['player'][0].get_height()\n",
    "PIPE_WIDTH = IMAGES['pipe'][0].get_width()\n",
    "PIPE_HEIGHT = IMAGES['pipe'][0].get_height()\n",
    "BACKGROUND_WIDTH = IMAGES['background'].get_width()\n",
    "\n",
    "PLAYER_INDEX_GEN = cycle([0, 1, 2, 1])\n",
    "\n",
    "\n",
    "class GameState:\n",
    "    def __init__(self):\n",
    "        self.score = self.playerIndex = self.loopIter = 0\n",
    "        self.playerx = int(SCREENWIDTH * 0.2)\n",
    "        self.playery = int((SCREENHEIGHT - PLAYER_HEIGHT) / 2)\n",
    "        self.basex = 0\n",
    "        self.baseShift = IMAGES['base'].get_width() - BACKGROUND_WIDTH\n",
    "\n",
    "        newPipe1 = getRandomPipe()\n",
    "        newPipe2 = getRandomPipe()\n",
    "        self.upperPipes = [\n",
    "            {'x': SCREENWIDTH, 'y': newPipe1[0]['y']},\n",
    "            {'x': SCREENWIDTH + (SCREENWIDTH / 2), 'y': newPipe2[0]['y']},\n",
    "        ]\n",
    "        self.lowerPipes = [\n",
    "            {'x': SCREENWIDTH, 'y': newPipe1[1]['y']},\n",
    "            {'x': SCREENWIDTH + (SCREENWIDTH / 2), 'y': newPipe2[1]['y']},\n",
    "        ]\n",
    "\n",
    "        # player velocity, max velocity, downward accleration, accleration on flap\n",
    "        self.pipeVelX = -4\n",
    "        self.playerVelY    =  0    # player's velocity along Y, default same as playerFlapped\n",
    "        self.playerMaxVelY =  10   # max vel along Y, max descend speed\n",
    "        self.playerMinVelY =  -8   # min vel along Y, max ascend speed\n",
    "        self.playerAccY    =   1   # players downward accleration\n",
    "        self.playerFlapAcc =  -9   # players speed on flapping\n",
    "        self.playerFlapped = False # True when player flaps\n",
    "\n",
    "    def frame_step(self, flap):\n",
    "        pygame.event.pump()\n",
    "\n",
    "        reward = 0.1\n",
    "        terminal = False\n",
    "\n",
    "        if flap:\n",
    "            if self.playery > -2 * PLAYER_HEIGHT:\n",
    "                self.playerVelY = self.playerFlapAcc\n",
    "                self.playerFlapped = True\n",
    "                #SOUNDS['wing'].play()\n",
    "\n",
    "        # check for score\n",
    "        playerMidPos = self.playerx + PLAYER_WIDTH / 2\n",
    "        for pipe in self.upperPipes:\n",
    "            pipeMidPos = pipe['x'] + PIPE_WIDTH / 2\n",
    "            if pipeMidPos <= playerMidPos < pipeMidPos + 4:\n",
    "                self.score += 1\n",
    "                #SOUNDS['point'].play()\n",
    "                reward = 1\n",
    "\n",
    "        # playerIndex basex change\n",
    "        if (self.loopIter + 1) % 3 == 0:\n",
    "            self.playerIndex = next(PLAYER_INDEX_GEN)\n",
    "        self.loopIter = (self.loopIter + 1) % 30\n",
    "        self.basex = -((-self.basex + 100) % self.baseShift)\n",
    "\n",
    "        # player's movement\n",
    "        if self.playerVelY < self.playerMaxVelY and not self.playerFlapped:\n",
    "            self.playerVelY += self.playerAccY\n",
    "        if self.playerFlapped:\n",
    "            self.playerFlapped = False\n",
    "        self.playery += min(self.playerVelY, BASEY - self.playery - PLAYER_HEIGHT)\n",
    "        if self.playery < 0:\n",
    "            self.playery = 0\n",
    "\n",
    "        # move pipes to left\n",
    "        for uPipe, lPipe in zip(self.upperPipes, self.lowerPipes):\n",
    "            uPipe['x'] += self.pipeVelX\n",
    "            lPipe['x'] += self.pipeVelX\n",
    "\n",
    "        # add new pipe when first pipe is about to touch left of screen\n",
    "        if 0 < self.upperPipes[0]['x'] < 5:\n",
    "            newPipe = getRandomPipe()\n",
    "            self.upperPipes.append(newPipe[0])\n",
    "            self.lowerPipes.append(newPipe[1])\n",
    "\n",
    "        # remove first pipe if its out of the screen\n",
    "        if self.upperPipes[0]['x'] < -PIPE_WIDTH:\n",
    "            self.upperPipes.pop(0)\n",
    "            self.lowerPipes.pop(0)\n",
    "\n",
    "        # check if crash here\n",
    "        isCrash= checkCrash({'x': self.playerx, 'y': self.playery,\n",
    "                             'index': self.playerIndex},\n",
    "                            self.upperPipes, self.lowerPipes)\n",
    "        if isCrash:\n",
    "            #SOUNDS['hit'].play()\n",
    "            #SOUNDS['die'].play()\n",
    "            terminal = True\n",
    "            self.__init__()\n",
    "            reward = -1\n",
    "\n",
    "        # draw sprites\n",
    "        SCREEN.blit(IMAGES['background'], (0,0))\n",
    "\n",
    "        for uPipe, lPipe in zip(self.upperPipes, self.lowerPipes):\n",
    "            SCREEN.blit(IMAGES['pipe'][0], (uPipe['x'], uPipe['y']))\n",
    "            SCREEN.blit(IMAGES['pipe'][1], (lPipe['x'], lPipe['y']))\n",
    "\n",
    "        SCREEN.blit(IMAGES['base'], (self.basex, BASEY))\n",
    "        # print score so player overlaps the score\n",
    "        # showScore(self.score)\n",
    "        SCREEN.blit(IMAGES['player'][self.playerIndex],\n",
    "                    (self.playerx, self.playery))\n",
    "\n",
    "        image_data = pygame.surfarray.array3d(pygame.display.get_surface())\n",
    "        pygame.display.update()\n",
    "        FPSCLOCK.tick(FPS)\n",
    "        #print self.upperPipes[0]['y'] + PIPE_HEIGHT - int(BASEY * 0.2)\n",
    "        return image_data, reward, terminal\n",
    "\n",
    "def getRandomPipe():\n",
    "    \"\"\"returns a randomly generated pipe\"\"\"\n",
    "    # y of gap between upper and lower pipe\n",
    "    gapYs = [20, 30, 40, 50, 60, 70, 80, 90]\n",
    "    index = random.randint(0, len(gapYs)-1)\n",
    "    gapY = gapYs[index]\n",
    "\n",
    "    gapY += int(BASEY * 0.2)\n",
    "    pipeX = SCREENWIDTH + 10\n",
    "\n",
    "    return [\n",
    "        {'x': pipeX, 'y': gapY - PIPE_HEIGHT},  # upper pipe\n",
    "        {'x': pipeX, 'y': gapY + PIPEGAPSIZE},  # lower pipe\n",
    "    ]\n",
    "\n",
    "\n",
    "def showScore(score):\n",
    "    \"\"\"displays score in center of screen\"\"\"\n",
    "    scoreDigits = [int(x) for x in list(str(score))]\n",
    "    totalWidth = 0 # total width of all numbers to be printed\n",
    "\n",
    "    for digit in scoreDigits:\n",
    "        totalWidth += IMAGES['numbers'][digit].get_width()\n",
    "\n",
    "    Xoffset = (SCREENWIDTH - totalWidth) / 2\n",
    "\n",
    "    for digit in scoreDigits:\n",
    "        SCREEN.blit(IMAGES['numbers'][digit], (Xoffset, SCREENHEIGHT * 0.1))\n",
    "        Xoffset += IMAGES['numbers'][digit].get_width()\n",
    "\n",
    "\n",
    "def checkCrash(player, upperPipes, lowerPipes):\n",
    "    \"\"\"returns True if player collders with base or pipes.\"\"\"\n",
    "    pi = player['index']\n",
    "    player['w'] = IMAGES['player'][0].get_width()\n",
    "    player['h'] = IMAGES['player'][0].get_height()\n",
    "\n",
    "    # if player crashes into ground\n",
    "    if player['y'] + player['h'] >= BASEY - 1:\n",
    "        return True\n",
    "    else:\n",
    "\n",
    "        playerRect = pygame.Rect(player['x'], player['y'],\n",
    "                      player['w'], player['h'])\n",
    "\n",
    "        for uPipe, lPipe in zip(upperPipes, lowerPipes):\n",
    "            # upper and lower pipe rects\n",
    "            uPipeRect = pygame.Rect(uPipe['x'], uPipe['y'], PIPE_WIDTH, PIPE_HEIGHT)\n",
    "            lPipeRect = pygame.Rect(lPipe['x'], lPipe['y'], PIPE_WIDTH, PIPE_HEIGHT)\n",
    "\n",
    "            # player and upper/lower pipe hitmasks\n",
    "            pHitMask = HITMASKS['player'][pi]\n",
    "            uHitmask = HITMASKS['pipe'][0]\n",
    "            lHitmask = HITMASKS['pipe'][1]\n",
    "\n",
    "            # if bird collided with upipe or lpipe\n",
    "            uCollide = pixelCollision(playerRect, uPipeRect, pHitMask, uHitmask)\n",
    "            lCollide = pixelCollision(playerRect, lPipeRect, pHitMask, lHitmask)\n",
    "\n",
    "            if uCollide or lCollide:\n",
    "                return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def pixelCollision(rect1, rect2, hitmask1, hitmask2):\n",
    "    \"\"\"Checks if two objects collide and not just their rects\"\"\"\n",
    "    rect = rect1.clip(rect2)\n",
    "\n",
    "    if rect.width == 0 or rect.height == 0:\n",
    "        return False\n",
    "\n",
    "    x1, y1 = rect.x - rect1.x, rect.y - rect1.y\n",
    "    x2, y2 = rect.x - rect2.x, rect.y - rect2.y\n",
    "\n",
    "    for x in range(rect.width):\n",
    "        for y in range(rect.height):\n",
    "            if hitmask1[x1+x][y1+y] and hitmask2[x2+x][y2+y]:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flappy no graphics for DQL\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import random\n",
    "import pygame\n",
    "import game.flappy_bird_utils as flappy_bird_utils\n",
    "import pygame.surfarray as surfarray\n",
    "from pygame.locals import *\n",
    "from itertools import cycle\n",
    "\n",
    "FPS = 30\n",
    "SCREENWIDTH  = 288\n",
    "SCREENHEIGHT = 512\n",
    "\n",
    "pygame.init()\n",
    "FPSCLOCK = pygame.time.Clock()\n",
    "SCREEN = pygame.display.set_mode((SCREENWIDTH, SCREENHEIGHT))\n",
    "pygame.display.set_caption('Flappy Bird')\n",
    "\n",
    "IMAGES, SOUNDS, HITMASKS = flappy_bird_utils.load()\n",
    "PIPEGAPSIZE = 100 # gap between upper and lower part of pipe\n",
    "BASEY = SCREENHEIGHT * 0.79\n",
    "\n",
    "PLAYER_WIDTH = IMAGES['player'][0].get_width()\n",
    "PLAYER_HEIGHT = IMAGES['player'][0].get_height()\n",
    "PIPE_WIDTH = IMAGES['pipe'][0].get_width()\n",
    "PIPE_HEIGHT = IMAGES['pipe'][0].get_height()\n",
    "BACKGROUND_WIDTH = IMAGES['background'].get_width()\n",
    "\n",
    "PLAYER_INDEX_GEN = cycle([0, 1, 2, 1])\n",
    "\n",
    "\n",
    "class GameNoGraphics:\n",
    "    def __init__(self):\n",
    "        self.score = self.playerIndex = self.loopIter = 0\n",
    "        self.playerx = int(SCREENWIDTH * 0.2)\n",
    "        self.playery = int((SCREENHEIGHT - PLAYER_HEIGHT) / 2)\n",
    "        self.basex = 0\n",
    "        self.baseShift = IMAGES['base'].get_width() - BACKGROUND_WIDTH\n",
    "\n",
    "        newPipe1 = getRandomPipe()\n",
    "        newPipe2 = getRandomPipe()\n",
    "        self.upperPipes = [\n",
    "            {'x': SCREENWIDTH, 'y': newPipe1[0]['y']},\n",
    "            {'x': SCREENWIDTH + (SCREENWIDTH / 2), 'y': newPipe2[0]['y']},\n",
    "        ]\n",
    "        self.lowerPipes = [\n",
    "            {'x': SCREENWIDTH, 'y': newPipe1[1]['y']},\n",
    "            {'x': SCREENWIDTH + (SCREENWIDTH / 2), 'y': newPipe2[1]['y']},\n",
    "        ]\n",
    "\n",
    "        # player velocity, max velocity, downward accleration, accleration on flap\n",
    "        self.pipeVelX = -4\n",
    "        self.playerVelY    =  0    # player's velocity along Y, default same as playerFlapped\n",
    "        self.playerMaxVelY =  10   # max vel along Y, max descend speed\n",
    "        self.playerMinVelY =  -8   # min vel along Y, max ascend speed\n",
    "        self.playerAccY    =   1   # players downward accleration\n",
    "        self.playerFlapAcc =  -9   # players speed on flapping\n",
    "        self.playerFlapped = False # True when player flaps\n",
    "\n",
    "    def get_next_pipe_index(self):\n",
    "        playerMidPos = self.playerx + PLAYER_WIDTH / 2\n",
    "        distance = [0, 0]\n",
    "        idx = 0\n",
    "        for pipe in self.upperPipes:\n",
    "            if (idx == 2):\n",
    "                break\n",
    "            pipeMidPos = pipe['x'] + PIPE_WIDTH / 2\n",
    "            if pipeMidPos >= playerMidPos:\n",
    "                distance[idx] = pipeMidPos - playerMidPos\n",
    "            else:\n",
    "                distance[idx] = 999\n",
    "            idx += 1\n",
    "        return distance.index(min(distance))\n",
    "\n",
    "\n",
    "    def frame_step(self, flap):\n",
    "        pygame.event.pump()\n",
    "\n",
    "        reward = 0.4\n",
    "        terminal = False\n",
    "\n",
    "        if flap:\n",
    "            if self.playery > -2 * PLAYER_HEIGHT:\n",
    "                self.playerVelY = self.playerFlapAcc\n",
    "                self.playerFlapped = True\n",
    "                #SOUNDS['wing'].play()\n",
    "\n",
    "        next_pipe_idx = self.get_next_pipe_index()\n",
    "        playerMidPos = self.playery + PLAYER_HEIGHT/2\n",
    "        pipeUpperMidPos = self.upperPipes[next_pipe_idx]['y']\n",
    "        pipeLowerMidPos = self.lowerPipes[next_pipe_idx]['y']\n",
    "        # if playerMidPos < pipeUpperMidPos + 1 and playerMidPos > pipeLowerMidPos - 1:\n",
    "        #     reward = 0.2\n",
    "        # if playerMidPos > pipeLowerMidPos - 100 and playerMidPos < pipeLowerMidPos:\n",
    "        #     # print(\"within\")\n",
    "        #     reward = 0.2\n",
    "        if playerMidPos <= 0.2*SCREENHEIGHT:\n",
    "            # print(\"too high\")\n",
    "            reward = -0.2\n",
    "        if playerMidPos <= 0.1*SCREENHEIGHT:\n",
    "            # print(\"too high\")\n",
    "            reward = -0.4\n",
    "\n",
    "\n",
    "        # check for score\n",
    "        playerMidPos = self.playerx + PLAYER_WIDTH / 2\n",
    "        for pipe in self.upperPipes:\n",
    "            pipeMidPos = pipe['x'] + PIPE_WIDTH / 2\n",
    "            if pipeMidPos <= playerMidPos < pipeMidPos + 4:\n",
    "                self.score += 1\n",
    "                #SOUNDS['point'].play()\n",
    "                reward = 10\n",
    "\n",
    "        # playerIndex basex change\n",
    "        if (self.loopIter + 1) % 3 == 0:\n",
    "            self.playerIndex = next(PLAYER_INDEX_GEN)\n",
    "        self.loopIter = (self.loopIter + 1) % 30\n",
    "        self.basex = -((-self.basex + 100) % self.baseShift)\n",
    "\n",
    "        # player's movement\n",
    "        if self.playerVelY < self.playerMaxVelY and not self.playerFlapped:\n",
    "            self.playerVelY += self.playerAccY\n",
    "        if self.playerFlapped:\n",
    "            self.playerFlapped = False\n",
    "        self.playery += min(self.playerVelY, BASEY - self.playery - PLAYER_HEIGHT)\n",
    "        if self.playery < 0:\n",
    "            self.playery = 0\n",
    "\n",
    "        # move pipes to left\n",
    "        for uPipe, lPipe in zip(self.upperPipes, self.lowerPipes):\n",
    "            uPipe['x'] += self.pipeVelX\n",
    "            lPipe['x'] += self.pipeVelX\n",
    "\n",
    "        # add new pipe when first pipe is about to touch left of screen\n",
    "        if 0 < self.upperPipes[0]['x'] < 5:\n",
    "            newPipe = getRandomPipe()\n",
    "            self.upperPipes.append(newPipe[0])\n",
    "            self.lowerPipes.append(newPipe[1])\n",
    "\n",
    "        # remove first pipe if its out of the screen\n",
    "        if self.upperPipes[0]['x'] < -PIPE_WIDTH:\n",
    "            self.upperPipes.pop(0)\n",
    "            self.lowerPipes.pop(0)\n",
    "\n",
    "        # check if crash here\n",
    "        isCrash= checkCrash({'x': self.playerx, 'y': self.playery,\n",
    "                             'index': self.playerIndex},\n",
    "                            self.upperPipes, self.lowerPipes)\n",
    "        if isCrash:\n",
    "            #SOUNDS['hit'].play()\n",
    "            #SOUNDS['die'].play()\n",
    "            terminal = True\n",
    "            self.__init__()\n",
    "            reward = -5\n",
    "\n",
    "        # # draw sprites\n",
    "        # SCREEN.blit(IMAGES['background'], (0,0))\n",
    "\n",
    "        # for uPipe, lPipe in zip(self.upperPipes, self.lowerPipes):\n",
    "        #     SCREEN.blit(IMAGES['pipe'][0], (uPipe['x'], uPipe['y']))\n",
    "        #     SCREEN.blit(IMAGES['pipe'][1], (lPipe['x'], lPipe['y']))\n",
    "\n",
    "        # SCREEN.blit(IMAGES['base'], (self.basex, BASEY))\n",
    "        # # print score so player overlaps the score\n",
    "        # # showScore(self.score)\n",
    "        # SCREEN.blit(IMAGES['player'][self.playerIndex],\n",
    "        #             (self.playerx, self.playery))\n",
    "\n",
    "        # image_data = pygame.surfarray.array3d(pygame.display.get_surface())\n",
    "        # pygame.display.update()\n",
    "        # FPSCLOCK.tick(FPS)\n",
    "        #print self.upperPipes[0]['y'] + PIPE_HEIGHT - int(BASEY * 0.2)\n",
    "        return 0, reward, terminal\n",
    "\n",
    "def getRandomPipe():\n",
    "    \"\"\"returns a randomly generated pipe\"\"\"\n",
    "    # y of gap between upper and lower pipe\n",
    "    gapYs = [20, 30, 40, 50, 60, 70, 80, 90]\n",
    "    index = random.randint(0, len(gapYs)-1)\n",
    "    gapY = gapYs[index]\n",
    "\n",
    "    gapY += int(BASEY * 0.2)\n",
    "    pipeX = SCREENWIDTH + 10\n",
    "\n",
    "    return [\n",
    "        {'x': pipeX, 'y': gapY - PIPE_HEIGHT},  # upper pipe\n",
    "        {'x': pipeX, 'y': gapY + PIPEGAPSIZE},  # lower pipe\n",
    "    ]\n",
    "\n",
    "\n",
    "def showScore(score):\n",
    "    \"\"\"displays score in center of screen\"\"\"\n",
    "    scoreDigits = [int(x) for x in list(str(score))]\n",
    "    totalWidth = 0 # total width of all numbers to be printed\n",
    "\n",
    "    for digit in scoreDigits:\n",
    "        totalWidth += IMAGES['numbers'][digit].get_width()\n",
    "\n",
    "    Xoffset = (SCREENWIDTH - totalWidth) / 2\n",
    "\n",
    "    for digit in scoreDigits:\n",
    "        SCREEN.blit(IMAGES['numbers'][digit], (Xoffset, SCREENHEIGHT * 0.1))\n",
    "        Xoffset += IMAGES['numbers'][digit].get_width()\n",
    "\n",
    "\n",
    "def checkCrash(player, upperPipes, lowerPipes):\n",
    "    \"\"\"returns True if player collders with base or pipes.\"\"\"\n",
    "    pi = player['index']\n",
    "    player['w'] = IMAGES['player'][0].get_width()\n",
    "    player['h'] = IMAGES['player'][0].get_height()\n",
    "\n",
    "    # if player crashes into ground\n",
    "    if player['y'] + player['h'] >= BASEY - 1:\n",
    "        return True\n",
    "    else:\n",
    "\n",
    "        playerRect = pygame.Rect(player['x'], player['y'],\n",
    "                      player['w'], player['h'])\n",
    "\n",
    "        for uPipe, lPipe in zip(upperPipes, lowerPipes):\n",
    "            # upper and lower pipe rects\n",
    "            uPipeRect = pygame.Rect(uPipe['x'], uPipe['y'], PIPE_WIDTH, PIPE_HEIGHT)\n",
    "            lPipeRect = pygame.Rect(lPipe['x'], lPipe['y'], PIPE_WIDTH, PIPE_HEIGHT)\n",
    "\n",
    "            # player and upper/lower pipe hitmasks\n",
    "            pHitMask = HITMASKS['player'][pi]\n",
    "            uHitmask = HITMASKS['pipe'][0]\n",
    "            lHitmask = HITMASKS['pipe'][1]\n",
    "\n",
    "            # if bird collided with upipe or lpipe\n",
    "            uCollide = pixelCollision(playerRect, uPipeRect, pHitMask, uHitmask)\n",
    "            lCollide = pixelCollision(playerRect, lPipeRect, pHitMask, lHitmask)\n",
    "\n",
    "            if uCollide or lCollide:\n",
    "                return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def pixelCollision(rect1, rect2, hitmask1, hitmask2):\n",
    "    \"\"\"Checks if two objects collide and not just their rects\"\"\"\n",
    "    rect = rect1.clip(rect2)\n",
    "\n",
    "    if rect.width == 0 or rect.height == 0:\n",
    "        return False\n",
    "\n",
    "    x1, y1 = rect.x - rect1.x, rect.y - rect1.y\n",
    "    x2, y2 = rect.x - rect2.x, rect.y - rect2.y\n",
    "\n",
    "    for x in range(rect.width):\n",
    "        for y in range(rect.height):\n",
    "            if hitmask1[x1+x][y1+y] and hitmask2[x2+x][y2+y]:\n",
    "                return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# FLAPPY BIRD CODE WITH GRAPHICS REMOVED\n",
    "\n",
    "import random\n",
    "import pygame\n",
    "from itertools import cycle\n",
    "\n",
    "SCREENWIDTH  = 288\n",
    "SCREENHEIGHT = 512\n",
    "\n",
    "IMAGES, SOUNDS, HITMASKS = load()\n",
    "PIPEGAPSIZE = 100 # gap between upper and lower part of pipe\n",
    "BASEY = SCREENHEIGHT * 0.79\n",
    "\n",
    "PLAYER_WIDTH = IMAGES['player'][0].get_width()\n",
    "PLAYER_HEIGHT = IMAGES['player'][0].get_height()\n",
    "PIPE_WIDTH = IMAGES['pipe'][0].get_width()\n",
    "PIPE_HEIGHT = IMAGES['pipe'][0].get_height()\n",
    "BACKGROUND_WIDTH = IMAGES['background'].get_width()\n",
    "\n",
    "PLAYER_INDEX_GEN = cycle([0, 1, 2, 1])\n",
    "\n",
    "\n",
    "class GameStateNoGraphics:\n",
    "    def __init__(self):\n",
    "        self.score = self.playerIndex = self.loopIter = 0\n",
    "        self.playerx = int(SCREENWIDTH * 0.2)\n",
    "        self.playery = int((SCREENHEIGHT - PLAYER_HEIGHT) / 2)\n",
    "        self.basex = 0\n",
    "        self.baseShift = IMAGES['base'].get_width() - BACKGROUND_WIDTH\n",
    "\n",
    "        newPipe1 = getRandomPipe()\n",
    "        newPipe2 = getRandomPipe()\n",
    "        self.upperPipes = [\n",
    "            {'x': SCREENWIDTH, 'y': newPipe1[0]['y']},\n",
    "            {'x': SCREENWIDTH + (SCREENWIDTH / 2), 'y': newPipe2[0]['y']},\n",
    "        ]\n",
    "        self.lowerPipes = [\n",
    "            {'x': SCREENWIDTH, 'y': newPipe1[1]['y']},\n",
    "            {'x': SCREENWIDTH + (SCREENWIDTH / 2), 'y': newPipe2[1]['y']},\n",
    "        ]\n",
    "\n",
    "        # player velocity, max velocity, downward accleration, accleration on flap\n",
    "        self.pipeVelX = -4\n",
    "        self.playerVelY    =  0    # player's velocity along Y, default same as playerFlapped\n",
    "        self.playerMaxVelY =  10   # max vel along Y, max descend speed\n",
    "        self.playerMinVelY =  -8   # min vel along Y, max ascend speed\n",
    "        self.playerAccY    =   1   # players downward accleration\n",
    "        self.playerFlapAcc =  -9   # players speed on flapping\n",
    "        self.playerFlapped = False # True when player flaps\n",
    "\n",
    "    def frame_step(self, flap):\n",
    "        if flap:\n",
    "            if self.playery > -2 * PLAYER_HEIGHT:\n",
    "                self.playerVelY = self.playerFlapAcc\n",
    "                self.playerFlapped = True\n",
    "\n",
    "        # playerIndex basex change\n",
    "        if (self.loopIter + 1) % 3 == 0:\n",
    "            self.playerIndex = next(PLAYER_INDEX_GEN)\n",
    "        self.loopIter = (self.loopIter + 1) % 30\n",
    "        self.basex = -((-self.basex + 100) % self.baseShift)\n",
    "\n",
    "        # player's movement\n",
    "        if self.playerVelY < self.playerMaxVelY and not self.playerFlapped:\n",
    "            self.playerVelY += self.playerAccY\n",
    "        if self.playerFlapped:\n",
    "            self.playerFlapped = False\n",
    "        self.playery += min(self.playerVelY, BASEY - self.playery - PLAYER_HEIGHT)\n",
    "        if self.playery < 0:\n",
    "            self.playery = 0\n",
    "\n",
    "        # move pipes to left\n",
    "        for uPipe, lPipe in zip(self.upperPipes, self.lowerPipes):\n",
    "            uPipe['x'] += self.pipeVelX\n",
    "            lPipe['x'] += self.pipeVelX\n",
    "\n",
    "        # add new pipe when first pipe is about to touch left of screen\n",
    "        if 0 < self.upperPipes[0]['x'] < 5:\n",
    "            newPipe = getRandomPipe()\n",
    "            self.upperPipes.append(newPipe[0])\n",
    "            self.lowerPipes.append(newPipe[1])\n",
    "\n",
    "        # remove first pipe if it's out of the screen\n",
    "        if self.upperPipes[0]['x'] < -PIPE_WIDTH:\n",
    "            self.upperPipes.pop(0)\n",
    "            self.lowerPipes.pop(0)\n",
    "\n",
    "        # check if crash here\n",
    "        isCrash = checkCrash({'x': self.playerx, 'y': self.playery, 'index': self.playerIndex}, self.upperPipes, self.lowerPipes)\n",
    "        if isCrash:\n",
    "            self.__init__()\n",
    "\n",
    "        return isCrash\n",
    "\n",
    "\n",
    "def getRandomPipe():\n",
    "    \"\"\"returns a randomly generated pipe\"\"\"\n",
    "    # y of gap between upper and lower pipe\n",
    "    gapYs = [20, 30, 40, 50, 60, 70, 80, 90]\n",
    "    index = random.randint(0, len(gapYs)-1)\n",
    "    gapY = gapYs[index]\n",
    "\n",
    "    gapY += int(BASEY * 0.2)\n",
    "    pipeX = SCREENWIDTH + 10\n",
    "\n",
    "    return [\n",
    "        {'x': pipeX, 'y': gapY - PIPE_HEIGHT},  # upper pipe\n",
    "        {'x': pipeX, 'y': gapY + PIPEGAPSIZE},  # lower pipe\n",
    "    ]\n",
    "\n",
    "\n",
    "def checkCrash(player, upperPipes, lowerPipes):\n",
    "    \"\"\"returns True if player collders with base or pipes.\"\"\"\n",
    "    pi = player['index']\n",
    "    player['w'] = IMAGES['player'][0].get_width()\n",
    "    player['h'] = IMAGES['player'][0].get_height()\n",
    "\n",
    "    # if player crashes into ground\n",
    "    if player['y'] + player['h'] >= BASEY - 1:\n",
    "        return True\n",
    "    else:\n",
    "\n",
    "        playerRect = pygame.Rect(player['x'], player['y'],\n",
    "                      player['w'], player['h'])\n",
    "\n",
    "        for uPipe, lPipe in zip(upperPipes, lowerPipes):\n",
    "            # upper and lower pipe rects\n",
    "            uPipeRect = pygame.Rect(uPipe['x'], uPipe['y'], PIPE_WIDTH, PIPE_HEIGHT)\n",
    "            lPipeRect = pygame.Rect(lPipe['x'], lPipe['y'], PIPE_WIDTH, PIPE_HEIGHT)\n",
    "\n",
    "            # player and upper/lower pipe hitmasks\n",
    "            pHitMask = HITMASKS['player'][pi]\n",
    "            uHitmask = HITMASKS['pipe'][0]\n",
    "            lHitmask = HITMASKS['pipe'][1]\n",
    "\n",
    "            # if bird collided with upipe or lpipe\n",
    "            uCollide = pixelCollision(playerRect, uPipeRect, pHitMask, uHitmask)\n",
    "            lCollide = pixelCollision(playerRect, lPipeRect, pHitMask, lHitmask)\n",
    "\n",
    "            if uCollide or lCollide:\n",
    "                return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def pixelCollision(rect1, rect2, hitmask1, hitmask2):\n",
    "    \"\"\"Checks if two objects collide and not just their rects\"\"\"\n",
    "    rect = rect1.clip(rect2)\n",
    "\n",
    "    if rect.width == 0 or rect.height == 0:\n",
    "        return False\n",
    "\n",
    "    x1, y1 = rect.x - rect1.x, rect.y - rect1.y\n",
    "    x2, y2 = rect.x - rect2.x, rect.y - rect2.y\n",
    "\n",
    "    for x in range(rect.width):\n",
    "        for y in range(rect.height):\n",
    "            if hitmask1[x1+x][y1+y] and hitmask2[x2+x][y2+y]:\n",
    "                return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# HELPER FUNCTION TO GET INPUT FROM FLAPPY BIRD GAME\n",
    "\n",
    "import torch\n",
    "\n",
    "def get_gamestate_info(game_state):\n",
    "    \"\"\"\n",
    "    gets coordinates of the two pipes\n",
    "    usage:          pipe_info = get_pipes_info(game_state)\n",
    "                    pipe_info[\"pipe0\"][\"upper\"][\"x\"] = x coordinate of the upper pipe of the first pipe\n",
    "    @args:          game_state\n",
    "    @returns:         \n",
    "        \"pipe0\": {\n",
    "            \"upper\": {\n",
    "                \"x\":\n",
    "                \"y\": \n",
    "            },\n",
    "            \"lower\": {\n",
    "                \"x\": \n",
    "                \"y\": \n",
    "            }\n",
    "        },\n",
    "        \"pipe1\": {\n",
    "            \"upper\": {\n",
    "                \"x\": ,\n",
    "                \"y\": \n",
    "            },\n",
    "            \"lower\": {\n",
    "                \"x\": \n",
    "                \"y\": \n",
    "            }\n",
    "        }, \n",
    "        \"player\": {\n",
    "            \"x\": \n",
    "            \"y\": \n",
    "            \"VelY\":\n",
    "            \"AccY\": \n",
    "            \"Flapped\": \n",
    "        }\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"pipe0\": {\n",
    "            \"upper\": {\n",
    "                \"x\": game_state.upperPipes[0]['x'],\n",
    "                \"y\": game_state.upperPipes[0]['y']\n",
    "            },\n",
    "            \"lower\": {\n",
    "                \"x\": game_state.lowerPipes[0]['x'],\n",
    "                \"y\": game_state.lowerPipes[0]['y']\n",
    "            }\n",
    "        },\n",
    "        \"pipe1\": {\n",
    "            \"upper\": {\n",
    "                \"x\": game_state.upperPipes[1]['x'],\n",
    "                \"y\": game_state.upperPipes[1]['y']\n",
    "            },\n",
    "            \"lower\": {\n",
    "                \"x\": game_state.lowerPipes[1]['x'],\n",
    "                \"y\": game_state.lowerPipes[1]['y']\n",
    "            }\n",
    "        },\n",
    "        \"player\": {\n",
    "            \"x\": game_state.playerx,\n",
    "            \"y\": game_state.playery,\n",
    "            \"VelY\": game_state.playerVelY,\n",
    "            \"AccY\": game_state.playerAccY,\n",
    "            \"Flapped\": game_state.playerFlapped,\n",
    "        }\n",
    "    }\n",
    "\n",
    "def get_input_layer(game_state):\n",
    "    \"\"\"\n",
    "    gets gamestate but returns it as a tensor. Use when feeding into ML algorithm\n",
    "    Arguments: game_state\n",
    "    Returns: tensor of shape (6, 1) containing same information as get_gamestate_info, but without the dictionary.\n",
    "    \"\"\"\n",
    "    return torch.tensor([game_state.lowerPipes[0]['x'], game_state.lowerPipes[0]['y'], \n",
    "                         game_state.lowerPipes[1]['x'], game_state.lowerPipes[1]['y'], \n",
    "                        game_state.playery, game_state.playerVelY, game_state.playerFlapped])\n",
    "\n",
    "\n",
    "def get_input_layer_2(game_state):\n",
    "    \"\"\"\n",
    "    gets gamestate but returns it as a np array. Use when feeding into ML algorithm\n",
    "    Arguments: game_state\n",
    "    Returns: tensor of shape (7, 1) containing same information as get_gamestate_info, but without the dictionary.\n",
    "    \"\"\"\n",
    "    # print(\"pipe height: \", game_state.lowerPipes[0]['y'])\n",
    "    return np.array([game_state.lowerPipes[0]['x'], game_state.lowerPipes[0]['y'], game_state.lowerPipes[0]['y'] - 100,\n",
    "                         game_state.lowerPipes[1]['x'], game_state.lowerPipes[1]['y'], game_state.lowerPipes[0]['y'] - 100,\n",
    "                        game_state.playery])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEAT Algorithm\n",
    "\n",
    "The NeuroEvolution of Augmenting Topologies (NEAT) Algorithm is a variation of the Genetic Algorithm approach, developed by Kenneth O. Stanley and Risto Miikkulainen in 2002 [1]. Like the Genetic Algorithm, the general approach is to generate a population of random models. Each model is then given a fitness value, depending on how well it performs at a particular task. A new population is then made through a combination operator and a mutation operator. The combination operator takes two (or more) models and combines then, and the mutation operator takes a single model and changes it slightly. The models with higher fitness are more likely to be combined with other models for the next generation, which should increase the average fitness. \n",
    "\n",
    "The traditional Genetic Algorithm approach relies on a fixed model topology, such that only the weights of the model are modified. In contrast, the NEAT algorithm begins with a simplified model, such as input nodes connecting directly to output nodes, and evolves the topology of the models too. New neurons and connections are added to existing models through the mutation operator. To keep track of the evolving model topologies, each model is assigned a set of node genes and a set of connection genes. Each gene is given a unique innovation number. When combining models, only genes with the same innovation number are combined.\n",
    " \n",
    "The NEAT algorithm also employs a technique called Speciation, where the population is grouped into species of similar topologies. Models are then more likely to be combined with other models of the same species. This helps maintain diversity in the population and prevents early convergence to a suboptimal solution. \n",
    "\n",
    "A 2006 study by Taylor, Whiteson and Stone [2] compared the NEAT algorithm to a temporal difference method SARSA. The study found that NEAT could find a better policy than SARSA, although it required more iterations to do so. The study also found that SARSA performed better when the domain was fully observable and NEAT converged quicker with a deterministic fitness function. Overall NEAT is shown to be a highly competitive algorithm for reinforcement learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'neat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31136\\1226115634.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mneat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mNEATModel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'neat'"
     ]
    }
   ],
   "source": [
    "# NEAT MODEL\n",
    "\n",
    "import os.path\n",
    "import pickle\n",
    "import neat\n",
    "\n",
    "class NEATModel:\n",
    "    def __init__(self):\n",
    "        configFile = os.path.join(os.path.abspath(''), 'NEATConfig')\n",
    "\n",
    "        self.config = neat.Config(neat.DefaultGenome, neat.DefaultReproduction,\n",
    "                                  neat.DefaultSpeciesSet, neat.DefaultStagnation,\n",
    "                                  configFile)\n",
    "\n",
    "        self.population = neat.Population(self.config)\n",
    "\n",
    "        self.population.add_reporter(neat.StdOutReporter(True))\n",
    "        stats = neat.StatisticsReporter()\n",
    "        self.population.add_reporter(stats)\n",
    "        self.population.add_reporter(neat.Checkpointer(20))\n",
    "        self.bestGenome = None\n",
    "        self.gameState = None\n",
    "\n",
    "    def run(self, generations, checkpointFileName=\"\"):\n",
    "        if checkpointFileName != \"\":\n",
    "            self.population = neat.Checkpointer.restore_checkpoint(checkpointFileName)\n",
    "        self.bestGenome = self.population.run(self.evaluateGenomes, generations)\n",
    "        with open(\"NEATBestGenome.pkl\", \"wb\") as f:\n",
    "            pickle.dump(self.bestGenome, f)\n",
    "            f.close()\n",
    "\n",
    "    def loadBest(self):\n",
    "        with open(\"NEATBestGenome.pkl\", \"rb\") as f:\n",
    "            self.bestGenome = pickle.load(f)\n",
    "\n",
    "    def playGame(self):\n",
    "        self.gameState = GameState()\n",
    "        network = neat.nn.FeedForwardNetwork.create(self.bestGenome, self.config)\n",
    "        go = True\n",
    "        while go:\n",
    "            networkInput = get_input_layer(self.gameState)\n",
    "            networkOutput = network.activate(networkInput)[0]\n",
    "            flap = networkOutput > 0.5  # sigmoid activation, output should be between 0 and 1\n",
    "            _, _, terminal = self.gameState.frame_step(flap)\n",
    "            if terminal:\n",
    "                go = False\n",
    "\n",
    "    def testBest(self, runs):\n",
    "        self.gameState = GameStateNoGraphics()\n",
    "        network = neat.nn.FeedForwardNetwork.create(self.bestGenome, self.config)\n",
    "        fitnesses = []\n",
    "        for i in range(runs):\n",
    "            thisRunFitness = 0\n",
    "            go = True\n",
    "            while go:\n",
    "                thisRunFitness += 1\n",
    "                networkInput = get_input_layer(self.gameState)\n",
    "                networkOutput = network.activate(networkInput)[0]\n",
    "                flap = networkOutput > 0.5  # sigmoid activation, output should be between 0 and 1\n",
    "                if self.gameState.frame_step(flap) or thisRunFitness > 10000:\n",
    "                    go = False\n",
    "                    fitnesses.append(thisRunFitness)\n",
    "                    if (i+1) % 100 == 0:\n",
    "                        print(\"Finished run: \" + str(i+1) + \"/\" + str(runs))\n",
    "        print(fitnesses)\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluateGenomes(genomes, config):\n",
    "        gameState = GameStateNoGraphics()\n",
    "        for genome_id, genome in genomes:\n",
    "            network = neat.nn.FeedForwardNetwork.create(genome, config)\n",
    "            runs = 10\n",
    "            averageFitness = 0\n",
    "            for i in range(runs):\n",
    "                thisRunFitness = 0\n",
    "                go = True\n",
    "                while go:\n",
    "                    thisRunFitness += 1\n",
    "                    networkInput = get_input_layer(gameState)\n",
    "                    networkOutput = network.activate(networkInput)[0]\n",
    "                    flap = networkOutput > 0.5  # sigmoid activation, output should be between 0 and 1\n",
    "                    if gameState.frame_step(flap) or thisRunFitness > 10000:\n",
    "                        go = False\n",
    "                        averageFitness += thisRunFitness / runs\n",
    "            genome.fitness = averageFitness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run: 100/1000\n",
      "Finished run: 200/1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m neatModel\u001b[38;5;241m.\u001b[39mloadBest()  \u001b[38;5;66;03m# Load the best model from training\u001b[39;00m\n\u001b[0;32m      6\u001b[0m neatModel\u001b[38;5;241m.\u001b[39mplayGame()  \u001b[38;5;66;03m# Watch the best model play the game (look at your taskbar it won't popup automatically)\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mneatModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtestBest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Test performance of the best model\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 58\u001b[0m, in \u001b[0;36mNEATModel.testBest\u001b[1;34m(self, runs)\u001b[0m\n\u001b[0;32m     56\u001b[0m thisRunFitness \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     57\u001b[0m networkInput \u001b[38;5;241m=\u001b[39m get_input_layer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgameState)\n\u001b[1;32m---> 58\u001b[0m networkOutput \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetworkInput\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     59\u001b[0m flap \u001b[38;5;241m=\u001b[39m networkOutput \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m  \u001b[38;5;66;03m# sigmoid activation, output should be between 0 and 1\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgameState\u001b[38;5;241m.\u001b[39mframe_step(flap) \u001b[38;5;129;01mor\u001b[39;00m thisRunFitness \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m10000\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\neat\\nn\\feed_forward.py:24\u001b[0m, in \u001b[0;36mFeedForwardNetwork.activate\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     22\u001b[0m         node_inputs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues[i] \u001b[38;5;241m*\u001b[39m w)\n\u001b[0;32m     23\u001b[0m     s \u001b[38;5;241m=\u001b[39m agg_func(node_inputs)\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues[node] \u001b[38;5;241m=\u001b[39m \u001b[43mact_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_nodes]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\neat\\activations.py:12\u001b[0m, in \u001b[0;36msigmoid_activation\u001b[1;34m(z)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msigmoid_activation\u001b[39m(z):\n\u001b[1;32m---> 12\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m60.0\u001b[39m, \u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m60.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m+\u001b[39m math\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mz))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\tensor.py:27\u001b[0m, in \u001b[0;36m_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAINING AND TESTING THE NEAT MODEL\n",
    "\n",
    "neatModel = NEATModel()\n",
    "# neatModel.run(300)  # Train the model, will take quite a long time!\n",
    "neatModel.loadBest()  # Load the best model from training\n",
    "neatModel.playGame()  # Watch the best model play the game (look at your taskbar it won't popup automatically)\n",
    "neatModel.testBest(1000)  # Test performance of the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### NEAT Algorithm\n",
    "\n",
    "The training terminated after about 120 generations. Given that each generation had 150 models, the total number of models generated was 18000. The best model was run 1000 times to get a more accurate assessment of its performance. The mean fitness was 479, the median fitness was 234, and the standard deviation was 546. The graph below shows a histogram of the fitness of the 1000 runs.\n",
    "![Histogram of testing best NEAT Model](NEATHistogram.png)\n",
    "Noticeably 259 (~26%) of runs had a fitness of 86, which occurs when the model fails to get past the first pipe. The best run had a fitness of 3823. At 30 frames per second, this is equivalent to surviving for about 127 seconds.\n",
    "\n",
    "## Discussion\n",
    "\n",
    "### NEAT Algorithm\n",
    "\n",
    "The NEAT Algorithm resulted in a model that performed reasonably well, although with quite a large variance. With a median fitness of 234, this is equivalent to surviving for 7.8 seconds, which is similar to the performance of an average human player. Further training would likely improve the model further, but more care should be taken to avoid outlier runs from overestimating the performance of the model. This could be done by taking the median of a number of runs rather than the mean. Increasing the number of runs each model is tested on would also help reduce inaccuracy in performance evaluation, but this comes with an increase in training time. One approach could be to start with only a single run, and increase the number of runs with subsequent generations. This would help prevent multiple runs being wasted on early models that consistently perform poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Kenneth O. Stanley and Risto Miikkulainen, Efficient Evolution of Neural Network Topologies, 2002, The University of Texas<br>\n",
    "[2] Matthew E. Taylor, Shimon Whiteson, and Peter Stone, Comparing Evolutionary and Temporal Difference Methods in a Reinforcement Learning Domain, 2006, Proceedings of the Genetic and Evolutionary Computation Conference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Q-Learning\n",
    "\n",
    "Q-learning is one of the most popular methods of doing reinforcement learning. It operates by estimating the value of a state action pair Q(s,a) and updates this value in accordance to the reward it receives for its actions. Traditional Q-learning uses a \"lookup table\" which stores the estimated value of a state action pair. However, this method has its flaws. Firstly, all possible values must be populated on the lookup table, and if the task is complex this means that the lookup table is impractically large and immensely difficult to populate fully. Additionally, traditional tables also have difficulty handling high-dimentional state spaces, which hinders its ability to interpret complex tasks such as flapping a bird.\n",
    "\n",
    "Using a Neural Network as an estimator for the Q value solves both of these problems. Accordingly many famous papers such as \"Playing Atari with Deep Reinforcement Learning\" by Volodymyr Mnih, et. al used deep Q learning to solve tasks similar to flappy bird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the neural net of the agent\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "# from utils import get_input_layer_2 as input\n",
    "# import game.flappyNoGraphics as Game\n",
    "# import game.wrapped_flappy_bird as GameVisual\n",
    "from collections import deque\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, lr):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.n_actions = 2\n",
    "        self.hid_1 = 128\n",
    "        self.hid_2 = 128\n",
    "        self.hid_3 = 128\n",
    "        self.inputs = 7 * 4\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(self.inputs, self.hid_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hid_1, self.hid_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hid_2, self.hid_3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hid_3, self.n_actions),\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss = nn.SmoothL1Loss()\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model(x.type(torch.FloatTensor).to(self.device))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a class to manage the state. Each state is a stack of 4 \"frame\" of the game\n",
    "# which provides the agent on information of the bird's movement.\n",
    "\n",
    "from collections import deque\n",
    "import torch\n",
    "# from utils import get_input_layer_2 as input\n",
    "import numpy as np\n",
    "\n",
    "class StateManager(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "        starting_state = [0, 0, 0, 0, 0, 0, 0]\n",
    "        for _ in range(capacity):\n",
    "            self.memory.append(starting_state)\n",
    "\n",
    "    def push(self, game):\n",
    "        \"\"\"Save a frame, \n",
    "            returns tensor of flattened state frames\n",
    "        \"\"\"\n",
    "        state_frame = get_input_layer_2(game)\n",
    "        self.memory.popleft()\n",
    "        self.memory.append(state_frame)\n",
    "        tensor_list = []\n",
    "        for i in range(4):\n",
    "            tensor_list.append(self.memory[i])\n",
    "        return np.array(tensor_list).flatten()\n",
    "    \n",
    "    def get(self):\n",
    "        # return np array of state frames\n",
    "        tensor_list = []\n",
    "        for i in range(4):\n",
    "            tensor_list.append(self.memory[i])\n",
    "        return np.array(tensor_list).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the class for the agent.\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Porperties:\n",
    "            gamma (float): Future reward discount rate.\n",
    "            epsilon (float): Probability for choosing random policy.\n",
    "            epsilon_decay (float): Rate at which epsilon decays toward zero.\n",
    "            learning_rate (float): Learning rate for Adam optimizer.\n",
    "\n",
    "        Returns:\n",
    "            Agent\n",
    "        \"\"\"\n",
    "        # constant parameters\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.996\n",
    "        self.lr = 0.00005\n",
    "        self.batch_size = 64\n",
    "        self.max_mem_size = 10000\n",
    "        # self.input_dims = 7 * 4\n",
    "\n",
    "        #variable parameters\n",
    "        self.epsilon = 0.01\n",
    "        self.mem_cntr = 0\n",
    "        self.mem_cntr_successful = 0\n",
    "\n",
    "        # initializing memory\n",
    "        self.memory = deque(maxlen=self.max_mem_size)\n",
    "        self.memory_successful = deque(maxlen=1000)\n",
    "        self.episodic_memory = []\n",
    "\n",
    "        #initialize networks\n",
    "        self.network = Network(self.lr)\n",
    "\n",
    "    def save_self(self):\n",
    "        with open('model.pickle', 'wb') as handle:\n",
    "            pickle.dump(self, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def save_experience(self):\n",
    "        with open('experience.pickle', 'wb') as handle:\n",
    "            pickle.dump(self.memory, handle)\n",
    "        with open('experience_successful.pickle', 'wb') as handle:\n",
    "            pickle.dump(self.memory_successful, handle)\n",
    "\n",
    "    def load_experience(self):\n",
    "        with open('experience.pickle', 'rb') as handle:\n",
    "            self.memory = pickle.load(handle)\n",
    "        with open('experience_successful.pickle', 'rb') as handle:\n",
    "            self.memory_successful = pickle.load(handle)\n",
    "\n",
    "    def getMemory(self):\n",
    "        return self.memory\n",
    "\n",
    "    def nextEpisode(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def getepsilon(self):\n",
    "        return self.epsilon\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, game_over, score, next_reward):\n",
    "        if (self.mem_cntr >= self.max_mem_size - 2):\n",
    "            for i in range(self.max_mem_size - 3000):\n",
    "                self.memory.popleft()\n",
    "            self.mem_cntr = len(self.memory) - 1\n",
    "\n",
    "        memory = [state, action, reward, next_state, game_over, score, next_reward]\n",
    "        self.memory.append(memory)\n",
    "\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def remember_successful(self, state, action, reward, next_state, game_over, score, next_reward):\n",
    "        if (self.mem_cntr_successful >= 1000 - 20):\n",
    "            for i in range(1000 - 500):\n",
    "                self.memory_successful.popleft()\n",
    "            self.mem_cntr_successful = len(self.memory_successful) - 1\n",
    "\n",
    "        memory = [state, action, reward, next_state, game_over, score, next_reward]\n",
    "        self.memory_successful.append(memory)\n",
    "\n",
    "        self.mem_cntr_successful += 1\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            # exploration\n",
    "\n",
    "            # 2 in 30 = averages about 1 press every 0.5 seconds which is in the ballpark of whats required to play the game. \n",
    "            # Gives bot best start possible (as it actually has a chance of making it through the first block!)\n",
    "            # in flappy bird a flap changes the gamestate a lot more than a no-flap.\n",
    "            determiner = np.random.randint(0, 30);\n",
    "            if (determiner <= 2):\n",
    "                return 1\n",
    "            return 0\n",
    "        else:\n",
    "            # exploitation, select epsilon-greedy action.\n",
    "                state_tensor = torch.tensor([state]).to(self.network.device, dtype=torch.int32)\n",
    "                action = torch.argmax(self.network.forward(state_tensor)).item()\n",
    "                \n",
    "        return action\n",
    "    \n",
    "    def updateEpsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        learn from a random batch of experiences\n",
    "        \"\"\"\n",
    "        if self.mem_cntr < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        self.network.optimizer.zero_grad()\n",
    "        max_mem = min(self.mem_cntr, self.max_mem_size)\n",
    "        batch = np.random.choice(max_mem, self.batch_size, replace=False)\n",
    "\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "\n",
    "        # memory = [state, action, reward, next_state, game_over, score, next_reward]\n",
    "        state_batch = torch.tensor([self.memory[i][0] for i in batch]).to(self.network.device, dtype=torch.float32)\n",
    "        action_batch = torch.tensor([self.memory[i][1] for i in batch])\n",
    "        reward_batch = torch.tensor([self.memory[i][2] for i in batch]).to(self.network.device, dtype=torch.float32)\n",
    "        new_state_batch = torch.tensor([self.memory[i][3] for i in batch]).to(self.network.device, dtype=torch.float32)\n",
    "        game_over_batch = torch.tensor([self.memory[i][4] for i in batch]).to(self.network.device, dtype=torch.bool)\n",
    "\n",
    "        #estimate q(s,a) and q(s',a').\n",
    "        q_current = self.network.forward(state_batch)[batch_index, action_batch]\n",
    "        q_next = self.network.forward(new_state_batch)\n",
    "        q_next[game_over_batch] = 0.0\n",
    "        # q(st,at) = r + gamma * max(q(s',a')\n",
    "        q_target = reward_batch + self.gamma * torch.max(q_next, dim=1)[0]\n",
    "\n",
    "        # smoothl1 loss and back-propagation\n",
    "        loss = self.network.loss(q_target, q_current).to(self.network.device)\n",
    "        loss.backward()\n",
    "        #prevent exploding gradient\n",
    "        torch.nn.utils.clip_grad_value_(self.network.parameters(), 100)\n",
    "        self.network.optimizer.step()\n",
    "\n",
    "    def learn_successful(self):\n",
    "        \"\"\"\n",
    "        learn from the set of experience that the agent was successful in. \n",
    "        Incentivises the agent to \n",
    "        \"\"\"\n",
    "        if self.mem_cntr_successful < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # print(\"learning successful\")\n",
    "        self.network.optimizer.zero_grad()\n",
    "        max_mem = min(self.mem_cntr_successful, self.max_mem_size)\n",
    "        batch = np.random.choice(max_mem, self.batch_size, replace=False)\n",
    "\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "\n",
    "        # memory = [state, action, reward, next_state, game_over, score]\n",
    "        state_batch = torch.tensor([self.memory[i][0] for i in batch]).to(self.network.device, dtype=torch.float32)\n",
    "        action_batch = torch.tensor([self.memory[i][1] for i in batch])\n",
    "        reward_batch = torch.tensor([self.memory[i][2] for i in batch]).to(self.network.device, dtype=torch.float32)\n",
    "        new_state_batch = torch.tensor([self.memory[i][3] for i in batch]).to(self.network.device, dtype=torch.float32)\n",
    "        game_over_batch = torch.tensor([self.memory[i][4] for i in batch]).to(self.network.device, dtype=torch.bool)\n",
    "        #estimate q(s,a) and q(s',a').\n",
    "        q_current = self.network.forward(state_batch)[batch_index, action_batch]\n",
    "        q_next = self.network.forward(new_state_batch)\n",
    "        q_next[game_over_batch] = 0.0\n",
    "        # q(st,at) = r + gamma * max(q(s',a')\n",
    "        q_target = reward_batch + self.gamma * torch.max(q_next, dim=1)[0]\n",
    "        \n",
    "        # smoothl1 loss and back-propagation\n",
    "        loss = self.network.loss(q_target, q_current).to(self.network.device)\n",
    "        loss.backward()\n",
    "        self.network.optimizer.step()\n",
    "\n",
    "    def update_episodic_memory(self, state, action, reward, next_state, done, score, current_step):\n",
    "        \"\"\"appends to a temporary memory. The temporary memory is uploaded to the main memory\n",
    "        once a game is complete.\n",
    "        \"\"\"\n",
    "        self.episodic_memory.append([state, action, reward, next_state, done, score, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_self():\n",
    "    with open('model.pickle', 'rb') as handle:\n",
    "        agent = pickle.load(handle)\n",
    "        return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the trainer allows a human to play the game then upload the relevant data to the agent.\n",
    "# increases the rate at which the agent initially learns.\n",
    "import keyboard\n",
    "import pickle\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self, agent):\n",
    "        self.runs = 10\n",
    "        self.agent = agent\n",
    "        self.game = GameState()\n",
    "        self.state_manager = StateManager(4)\n",
    "\n",
    "    def play(self, runs=10):\n",
    "        self.runs = runs\n",
    "        self.agent.episodic_memory = []\n",
    "        current_step = 0\n",
    "        # for runs amount of games\n",
    "        for i in range(self.runs):\n",
    "            #initialize game\n",
    "            self.game = GameState()\n",
    "            self.state_manager = StateManager(4)\n",
    "            state = self.state_manager.get()\n",
    "            done = False\n",
    "            score = 0\n",
    "            # manually play the game\n",
    "            while not done:\n",
    "                if keyboard.is_pressed(\" \"):\n",
    "                    action = 1\n",
    "                    _, reward, _ = self.game.frame_step(True)\n",
    "                else:\n",
    "                    action = 0\n",
    "                    _, reward, _ = self.game.frame_step(False)\n",
    "                if (reward == -5):\n",
    "                    done = True\n",
    "                    final_score = score\n",
    "                    reward = -5\n",
    "                score += reward\n",
    "\n",
    "                self.state_manager.push(self.game)\n",
    "                \n",
    "                #upload experience and train the agent on the human gameplay\n",
    "                next_state = self.state_manager.get()\n",
    "                self.agent.update_episodic_memory(state, action, reward, next_state, done, score, current_step)\n",
    "                self.agent.learn()\n",
    "                self.agent.learn_successful()\n",
    "                current_step += 1\n",
    "                state = next_state\n",
    "            for frame in self.agent.episodic_memory:\n",
    "                self.agent.remember(frame[0], frame[1], frame[2], frame[3], frame[4], frame[5], frame[6])\n",
    "                self.agent.remember_successful(frame[0], frame[1], frame[2], frame[3], frame[4], frame[5], frame[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main training loop\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def train(agent):\n",
    "\n",
    "    \n",
    "    scores, median_scores, eps_history, time_history, time_median = [], [], [], [], []\n",
    "    n_games = 100000\n",
    "    success_threshold = 15\n",
    "    trainer = Trainer(agent)\n",
    "\n",
    "    # trainer.play(10)\n",
    "    # agent.save_experience()\n",
    "    agent.load_experience()\n",
    "    #learn from human experiences for a headstart\n",
    "    #without this, the agent typically just default to a policy of only flapping or only doing nothing\n",
    "    for i in range(100):\n",
    "        agent.learn()\n",
    "    high_score = 40\n",
    "    # for n_games amount of games\n",
    "    for i in range(n_games):\n",
    "        #initialize game\n",
    "        game = GameNoGraphics()\n",
    "        if (keyboard.is_pressed(\"p\")):\n",
    "            game = GameState()\n",
    "        score = 0\n",
    "        game_over = False\n",
    "        state_manager = StateManager(4)\n",
    "        state = state_manager.get()\n",
    "        done = False\n",
    "        # state, action, reward, next_state, done, score\n",
    "        agent.episodic_memory = []\n",
    "        current_step = 0\n",
    "        #while the game is not complete\n",
    "        while not done:\n",
    "            #select an action\n",
    "            action = agent.select_action(state)\n",
    "            _, reward, _ = game.frame_step(action)\n",
    "            #calculate the state\n",
    "            state_manager.push(game)\n",
    "            next_state = state_manager.get()\n",
    "            if (reward == -5):\n",
    "                done = True\n",
    "                final_score = score\n",
    "                reward = -5\n",
    "            score += reward\n",
    "            #remember the action taken\n",
    "            # agent.remember(state, action, reward, next_state, done, score)\n",
    "            agent.update_episodic_memory(state, action, reward, next_state, done, score, current_step)\n",
    "            \n",
    "\n",
    "            state = next_state\n",
    "            current_step += 1\n",
    "        #after each game, learn a random batch of experiences from memory\n",
    "        #and also lean a batch of experiences that the bird was successful in.\n",
    "        agent.learn()\n",
    "        agent.learn_successful()\n",
    "        if (score > high_score):\n",
    "            high_score = score\n",
    "            agent.save_self()\n",
    "        agent.updateEpsilon()\n",
    "        #upload memory to main memory\n",
    "        eps_history.append(agent.epsilon)\n",
    "        for frame in agent.episodic_memory:\n",
    "            agent.remember(frame[0], frame[1], frame[2], frame[3], frame[4], frame[5], frame[6])\n",
    "        # success_threshold is typically 10 greater than the median score.\n",
    "        if (score > success_threshold):\n",
    "            agent.remember_successful(frame[0], frame[1], frame[2], frame[3], frame[4], frame[5], frame[6])\n",
    "        # agent.remember(state, action, reward, next_state, done, score)\n",
    "\n",
    "        #calculate some statistics for evaluation\n",
    "        median_score = np.median(scores[-100:])\n",
    "        success_threshold = max(success_threshold, median_score + 10)\n",
    "        scores.append(score)\n",
    "        median_scores.append(median_score)\n",
    "        time_history.append(current_step/30)\n",
    "        median_t = np.median(time_history[-100:])\n",
    "        time_median.append(median_t)\n",
    "        if ((i % 100) == 0):\n",
    "            print('episode: ', i,'score: %.2f' % score,\n",
    "                    ' median score %.2f' % median_score, 'time %.2f' % (current_step/30),'median time %.2f' % (median_t) ,'epsilon %.2f' % agent.epsilon)\n",
    "        if (keyboard.is_pressed(\"`\")):\n",
    "            break\n",
    "    #save the final model\n",
    "    torch.save(agent.network.state_dict(), 'Models/DQL/dqlmodel.pth')\n",
    "    #plot.\n",
    "    plt.plot(time_median)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test(agent):\n",
    "    #main training loop\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def test():\n",
    "\n",
    "    \n",
    "    scores, median_scores, eps_history, time_history, time_median = [], [], [], [], []\n",
    "    n_games = 1000\n",
    "    success_threshold = 15\n",
    "    \n",
    "\n",
    "\n",
    "    with open('model.pickle', 'rb') as handle:\n",
    "        agent = pickle.load(handle)\n",
    "\n",
    "    high_score = 40\n",
    "    agent.epsilon = 0\n",
    "    # for n_games amount of games\n",
    "    for i in range(n_games):\n",
    "        #initialize game\n",
    "        game = GameState()\n",
    "        score = 0\n",
    "        game_over = False\n",
    "        state_manager = StateManager(4)\n",
    "        state = state_manager.get()\n",
    "        done = False\n",
    "        # state, action, reward, next_state, done, score\n",
    "        current_step = 0\n",
    "        #while the game is not complete\n",
    "        while not done:\n",
    "            #select an action\n",
    "            action = agent.select_action(state)\n",
    "            _, reward, _ = game.frame_step(action)\n",
    "            #calculate the state\n",
    "            state_manager.push(game)\n",
    "            next_state = state_manager.get()\n",
    "            if (reward == -5):\n",
    "                done = True\n",
    "                final_score = score\n",
    "                reward = -5\n",
    "            score += reward\n",
    "            #remember the action taken\n",
    "            # agent.remember(state, action, reward, next_state, done, score)\n",
    "            # agent.update_episodic_memory(state, action, reward, next_state, done, score, current_step)\n",
    "            \n",
    "\n",
    "            state = next_state\n",
    "            current_step += 1\n",
    "        #after each game, learn a random batch of experiences from memory\n",
    "        #and also lean a batch of experiences that the bird was successful in.\n",
    "        # agent.learn()\n",
    "        # agent.learn_successful()\n",
    "        # if (score > high_score):\n",
    "        #     high_score = score\n",
    "        #     agent.save_self()\n",
    "        # agent.updateEpsilon()\n",
    "        #upload memory to main memory\n",
    "        eps_history.append(agent.epsilon)\n",
    "        # for frame in agent.episodic_memory:\n",
    "        #     agent.remember(frame[0], frame[1], frame[2], frame[3], frame[4], frame[5], frame[6])\n",
    "        # # success_threshold is typically 10 greater than the median score.\n",
    "        # if (score > success_threshold):\n",
    "        #     agent.remember_successful(frame[0], frame[1], frame[2], frame[3], frame[4], frame[5], frame[6])\n",
    "        # agent.remember(state, action, reward, next_state, done, score)\n",
    "\n",
    "        #calculate some statistics for evaluation\n",
    "        median_score = np.median(scores[-100:])\n",
    "        success_threshold = max(success_threshold, median_score + 10)\n",
    "        scores.append(score)\n",
    "        median_scores.append(median_score)\n",
    "        time_history.append(current_step/30)\n",
    "        median_t = np.median(time_history[-100:])\n",
    "        time_median.append(median_t)\n",
    "        if ((i % 100) == 0):\n",
    "            print('episode: ', i,'score: %.2f' % score,\n",
    "                    ' median score %.2f' % median_score, 'time %.2f' % (current_step/30),'median time %.2f' % (median_t) ,'epsilon %.2f' % agent.epsilon)\n",
    "        if (keyboard.is_pressed(\"`\")):\n",
    "            break\n",
    "    #save the final model\n",
    "    torch.save(agent.network.state_dict(), 'Models/DQL/dqlmodel.pth')\n",
    "    #plot.\n",
    "    plt.plot(time_median)\n",
    "    # plt.show()\n",
    "    return time_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31136\\2149358599.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtime_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31136\\4137306498.py\u001b[0m in \u001b[0;36mtest\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[1;31m#select an action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframe_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m             \u001b[1;31m#calculate the state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mstate_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31136\\3055913781.py\u001b[0m in \u001b[0;36mframe_step\u001b[1;34m(self, flap)\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[0mimage_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpygame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msurfarray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpygame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_surface\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[0mpygame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m         \u001b[0mFPSCLOCK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFPS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;31m#print self.upperPipes[0]['y'] + PIPE_HEIGHT - int(BASEY * 0.2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimage_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "time_history = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.766666666666667, 14.133333333333333, 5.6, 4.266666666666667, 3.533333333333333, 32.7, 29.366666666666667, 8.033333333333333, 9.6, 3.033333333333333, 5.4, 3.466666666666667, 6.666666666666667, 8.233333333333333, 5.3, 5.533333333333333, 6.733333333333333, 5.566666666666666, 16.433333333333334, 22.566666666666666, 37.7, 4.366666666666666, 8.1, 25.033333333333335, 14.066666666666666, 2.9, 8.133333333333333, 5.9, 31.433333333333334, 15.333333333333334, 27.6, 2.8666666666666667, 8.066666666666666, 37.36666666666667, 8.266666666666667, 9.133333333333333, 4.2, 13.0, 12.733333333333333, 2.9, 7.966666666666667, 17.866666666666667, 3.3333333333333335, 20.6, 8.0, 8.333333333333334, 8.033333333333333, 5.8, 5.466666666666667, 10.466666666666667, 4.666666666666667, 2.933333333333333, 13.266666666666667, 2.8666666666666667, 17.966666666666665, 9.166666666666666, 25.2, 9.666666666666666, 11.6, 4.333333333333333, 5.433333333333334, 14.333333333333334, 28.1, 11.566666666666666, 8.033333333333333, 20.1, 4.4, 8.066666666666666, 8.066666666666666, 10.5, 2.8666666666666667, 12.966666666666667, 15.3, 8.1, 19.133333333333333, 4.2, 22.733333333333334, 4.1, 13.3, 3.3333333333333335, 12.966666666666667, 18.233333333333334, 10.466666666666667, 17.633333333333333, 5.3, 15.333333333333334, 8.033333333333333, 12.966666666666667, 5.433333333333334, 9.1, 44.8, 3.033333333333333, 22.833333333333332, 4.466666666666667, 3.4, 7.933333333333334, 3.0, 9.133333333333333, 4.666666666666667, 6.633333333333334, 21.4, 8.066666666666666, 8.066666666666666, 5.6, 9.133333333333333, 5.9, 4.266666666666667, 15.166666666666666, 4.7, 4.2, 5.366666666666666, 3.433333333333333, 3.033333333333333, 6.6, 2.8666666666666667, 2.8666666666666667, 3.033333333333333, 5.466666666666667, 27.766666666666666, 3.5, 14.0, 5.633333333333334, 4.7, 21.633333333333333, 2.8666666666666667, 10.233333333333333, 11.733333333333333, 32.733333333333334, 21.533333333333335, 19.033333333333335, 12.8, 5.5, 9.2, 10.566666666666666, 7.966666666666667, 8.333333333333334, 15.166666666666666, 11.6, 5.633333333333334, 17.033333333333335, 5.933333333333334, 11.6, 2.933333333333333, 8.366666666666667, 5.933333333333334, 18.9, 20.566666666666666, 8.066666666666666, 12.966666666666667, 9.166666666666666, 2.8666666666666667, 2.8666666666666667, 23.2, 9.666666666666666, 8.066666666666666, 2.933333333333333, 3.433333333333333, 5.933333333333334, 3.5, 15.466666666666667, 5.6, 13.966666666666667, 5.5, 4.366666666666666, 19.033333333333335, 8.366666666666667, 17.633333333333333, 8.033333333333333, 7.066666666666666, 9.2, 27.0, 9.133333333333333, 15.666666666666666, 2.933333333333333, 19.133333333333333, 7.2, 7.833333333333333, 20.3, 3.033333333333333, 5.566666666666666, 9.666666666666666, 10.466666666666667, 3.3333333333333335, 3.033333333333333, 45.333333333333336, 9.166666666666666, 5.9, 15.366666666666667, 16.666666666666668, 2.8666666666666667, 5.466666666666667, 10.566666666666666, 13.333333333333334, 12.966666666666667, 22.9, 9.666666666666666, 3.5, 7.966666666666667, 7.9, 7.8, 2.8666666666666667, 7.166666666666667, 7.9, 6.733333333333333, 5.3, 3.5, 2.8666666666666667, 4.733333333333333, 25.2, 9.266666666666667, 5.433333333333334, 3.533333333333333, 7.933333333333334, 2.8666666666666667, 30.133333333333333, 15.2, 20.233333333333334, 9.233333333333333, 22.0, 9.233333333333333, 14.566666666666666, 10.533333333333333, 5.6, 5.933333333333334, 10.866666666666667, 20.733333333333334, 4.3, 5.466666666666667, 25.666666666666668, 7.966666666666667, 6.733333333333333, 8.033333333333333, 10.433333333333334, 16.533333333333335, 17.066666666666666, 16.633333333333333, 7.933333333333334, 25.266666666666666, 10.233333333333333, 32.43333333333333, 15.3, 6.766666666666667, 3.1333333333333333, 14.066666666666666, 7.966666666666667, 2.8666666666666667, 17.9, 13.966666666666667, 5.5, 14.166666666666666, 8.1, 13.1, 5.733333333333333, 4.733333333333333, 7.766666666666667, 15.766666666666667, 5.3, 5.933333333333334, 12.833333333333334, 2.933333333333333, 13.3, 7.166666666666667, 7.866666666666666, 5.4, 6.666666666666667, 10.433333333333334, 11.5, 9.233333333333333, 30.566666666666666, 2.8666666666666667, 4.8, 3.1333333333333333, 10.866666666666667, 21.366666666666667, 5.333333333333333, 10.466666666666667, 7.9, 25.233333333333334, 4.1, 20.733333333333334, 2.9, 8.1, 23.166666666666668, 6.7, 9.266666666666667, 17.633333333333333, 9.033333333333333, 15.2, 10.5, 2.933333333333333, 27.633333333333333, 11.6, 7.966666666666667, 19.033333333333335, 5.6, 7.933333333333334, 5.8, 16.5, 4.366666666666666, 16.533333333333335, 6.733333333333333, 9.633333333333333, 4.366666666666666, 3.533333333333333, 10.833333333333334, 15.466666666666667, 7.933333333333334, 16.6, 8.066666666666666, 38.0, 5.866666666666666, 6.633333333333334, 23.966666666666665, 3.1333333333333333, 10.5, 5.3, 41.3, 2.933333333333333, 3.033333333333333, 13.233333333333333, 15.466666666666667, 8.1, 7.2, 41.7, 17.633333333333333, 4.366666666666666, 15.166666666666666, 4.733333333333333, 16.566666666666666, 2.9, 7.0, 20.4, 5.9, 12.966666666666667, 3.066666666666667, 6.633333333333334, 4.7, 25.033333333333335, 17.633333333333333, 16.566666666666666, 7.966666666666667, 8.033333333333333, 6.666666666666667, 9.733333333333333, 17.633333333333333, 5.4, 38.93333333333333, 18.266666666666666, 17.033333333333335, 4.366666666666666, 11.6, 11.566666666666666, 4.333333333333333, 6.833333333333333, 12.9, 9.666666666666666, 7.9, 5.433333333333334, 3.033333333333333, 15.433333333333334, 9.3, 4.333333333333333, 15.333333333333334, 5.433333333333334, 6.6, 9.1, 4.733333333333333, 11.633333333333333, 12.933333333333334, 15.466666666666667, 3.066666666666667, 4.266666666666667, 8.033333333333333, 3.4, 35.266666666666666, 17.9, 3.1333333333333333, 24.3, 28.966666666666665, 12.866666666666667, 4.233333333333333, 15.166666666666666, 4.3, 3.033333333333333, 5.466666666666667, 17.866666666666667, 12.7, 20.466666666666665, 25.633333333333333, 16.566666666666666, 8.1, 4.333333333333333, 4.266666666666667, 4.266666666666667, 14.566666666666666, 2.8666666666666667, 5.5, 3.466666666666667, 4.466666666666667, 9.133333333333333, 2.8666666666666667, 24.1, 7.966666666666667, 21.966666666666665, 7.766666666666667, 27.8, 13.333333333333334, 3.466666666666667, 5.466666666666667, 25.066666666666666, 8.4, 5.666666666666667, 27.633333333333333, 10.533333333333333, 21.6, 10.8, 18.266666666666666, 15.333333333333334, 4.733333333333333, 18.1, 11.733333333333333, 12.866666666666667, 17.866666666666667, 23.966666666666665, 6.766666666666667, 2.9, 11.966666666666667, 42.9, 3.5, 18.233333333333334, 9.4, 2.8666666666666667, 13.3, 13.266666666666667, 14.666666666666666, 8.4, 10.333333333333334, 9.2, 8.4, 3.3333333333333335, 15.433333333333334, 19.0, 12.866666666666667, 4.733333333333333, 5.933333333333334, 15.166666666666666, 5.433333333333334, 8.4, 23.833333333333332, 15.333333333333334, 4.733333333333333, 2.9, 23.033333333333335, 3.4, 5.533333333333333, 5.6, 3.4, 12.833333333333334, 3.5, 8.4, 8.033333333333333, 3.5, 2.8666666666666667, 14.133333333333333, 10.366666666666667, 5.733333333333333, 12.833333333333334, 30.133333333333333, 9.033333333333333, 5.533333333333333, 3.033333333333333, 8.066666666666666, 5.566666666666666, 12.7, 9.3, 5.6, 4.733333333333333, 25.333333333333332, 2.9, 11.766666666666667, 25.033333333333335, 3.066666666666667, 9.1, 16.933333333333334, 7.933333333333334, 8.4, 4.2, 10.466666666666667, 22.7, 2.8666666666666667, 13.166666666666666, 3.5, 18.333333333333332, 17.633333333333333, 5.4, 2.8666666666666667, 7.766666666666667, 19.033333333333335, 8.0, 10.233333333333333, 17.633333333333333, 5.533333333333333, 27.733333333333334, 27.733333333333334, 17.866666666666667, 4.266666666666667, 9.033333333333333, 12.866666666666667, 4.733333333333333, 9.133333333333333, 5.333333333333333, 6.7, 26.4, 2.933333333333333, 4.2, 15.366666666666667, 5.3, 28.933333333333334, 15.333333333333334, 19.5, 41.7, 6.666666666666667, 14.6, 5.666666666666667, 5.933333333333334, 6.566666666666666, 3.033333333333333, 5.633333333333334, 3.533333333333333, 16.633333333333333, 5.866666666666666, 39.96666666666667, 5.733333333333333, 12.1, 35.5, 8.066666666666666, 5.633333333333334, 10.6, 7.966666666666667, 5.6, 17.633333333333333, 9.1, 17.9, 13.333333333333334, 6.8, 3.5, 19.033333333333335, 14.166666666666666, 14.133333333333333, 6.9, 10.666666666666666, 28.033333333333335, 18.266666666666666, 5.566666666666666, 19.0, 9.1, 12.7, 10.233333333333333, 25.233333333333334, 8.066666666666666, 8.366666666666667, 16.6, 5.533333333333333, 27.533333333333335, 6.7, 3.1333333333333333, 5.633333333333334, 4.233333333333333, 11.6, 4.233333333333333, 3.033333333333333, 20.4, 6.766666666666667, 16.633333333333333, 13.033333333333333, 15.166666666666666, 12.9, 21.4, 14.033333333333333, 4.333333333333333, 7.766666666666667, 8.366666666666667, 5.3, 7.8, 4.266666666666667, 9.133333333333333, 10.833333333333334, 22.8, 5.866666666666666, 5.566666666666666, 3.1333333333333333, 6.833333333333333, 3.5, 32.733333333333334, 6.6, 8.033333333333333, 12.133333333333333, 3.0, 4.366666666666666, 10.866666666666667, 4.2, 9.266666666666667, 20.366666666666667, 14.1, 9.3, 15.5, 5.5, 15.766666666666667, 2.933333333333333, 3.0, 6.733333333333333, 5.3, 3.033333333333333, 15.166666666666666, 31.233333333333334, 5.9, 25.266666666666666, 7.9, 4.2, 9.133333333333333, 2.933333333333333, 4.3, 9.266666666666667, 15.466666666666667, 5.3, 17.033333333333335, 19.6, 13.0, 9.233333333333333, 7.933333333333334, 20.7, 10.4, 11.7, 5.3, 4.7, 13.0, 3.033333333333333, 12.9, 18.9, 6.7, 5.433333333333334, 4.2, 11.733333333333333, 27.866666666666667, 16.666666666666668, 8.4, 4.7, 8.066666666666666, 15.433333333333334, 2.8666666666666667, 5.8, 5.3, 15.366666666666667, 16.7, 5.433333333333334, 7.2, 6.633333333333334, 3.033333333333333, 5.4, 4.266666666666667, 35.06666666666667, 31.366666666666667, 19.533333333333335, 25.3, 25.666666666666668, 5.6, 7.766666666666667, 11.6, 20.333333333333332, 13.0, 4.733333333333333, 7.766666666666667, 35.03333333333333, 2.8666666666666667, 13.3, 13.066666666666666, 19.0, 13.333333333333334, 11.633333333333333, 10.366666666666667, 7.166666666666667, 6.766666666666667, 9.3, 5.933333333333334, 2.8666666666666667, 5.3, 12.1, 5.633333333333334, 2.8666666666666667, 12.166666666666666, 3.066666666666667, 16.666666666666668, 5.8, 6.7, 14.133333333333333, 9.233333333333333, 30.566666666666666, 4.366666666666666, 7.9, 36.3, 5.3, 7.766666666666667, 22.8, 2.933333333333333, 5.933333333333334, 10.266666666666667, 20.133333333333333, 4.3, 19.066666666666666, 23.166666666666668, 15.166666666666666, 8.066666666666666, 15.2, 10.866666666666667, 4.7, 11.7, 5.466666666666667, 4.7, 10.533333333333333, 3.5, 12.7, 10.566666666666666, 8.0, 17.8, 7.766666666666667, 14.1, 5.766666666666667, 5.466666666666667, 20.133333333333333, 12.733333333333333, 8.333333333333334, 10.266666666666667, 9.2, 23.966666666666665, 21.466666666666665, 25.2, 3.066666666666667, 7.966666666666667, 11.666666666666666, 5.633333333333334, 5.466666666666667, 9.066666666666666, 9.233333333333333, 8.1, 9.566666666666666, 17.066666666666666, 2.8666666666666667, 9.233333333333333, 18.233333333333334, 11.666666666666666, 6.733333333333333, 5.6, 10.4, 13.333333333333334, 16.433333333333334, 17.933333333333334, 3.433333333333333, 62.53333333333333, 2.8666666666666667, 6.666666666666667, 15.433333333333334, 10.866666666666667, 4.733333333333333, 10.533333333333333, 20.466666666666665, 5.3, 6.833333333333333, 5.9, 2.933333333333333, 2.8666666666666667, 20.533333333333335, 9.633333333333333, 5.466666666666667, 10.466666666666667, 7.933333333333334, 9.266666666666667, 19.466666666666665, 17.633333333333333, 6.733333333333333, 6.766666666666667, 5.6, 6.566666666666666, 10.566666666666666, 20.3, 5.3, 2.933333333333333, 21.466666666666665, 3.1333333333333333, 13.966666666666667, 11.5, 4.3, 15.466666666666667, 3.1333333333333333, 5.6, 9.266666666666667, 8.033333333333333, 5.566666666666666, 8.066666666666666, 6.866666666666666, 25.266666666666666, 14.6, 32.6, 21.533333333333335, 5.5, 3.533333333333333, 7.9, 3.0, 10.433333333333334, 5.933333333333334, 10.833333333333334, 20.366666666666667, 10.5, 4.333333333333333, 6.633333333333334, 6.766666666666667, 17.9, 6.7, 9.133333333333333, 4.3, 2.9, 10.233333333333333, 6.733333333333333, 8.1, 5.4, 9.633333333333333, 15.533333333333333, 2.8666666666666667, 30.233333333333334, 6.666666666666667, 8.2, 9.2, 12.7, 9.033333333333333, 8.4, 6.766666666666667, 9.133333333333333, 10.533333333333333, 23.166666666666668, 3.533333333333333, 7.2, 29.266666666666666, 7.2, 11.533333333333333, 9.166666666666666, 3.0, 10.266666666666667, 15.166666666666666, 5.933333333333334, 3.066666666666667, 8.066666666666666, 4.3, 10.833333333333334, 15.366666666666667, 12.7, 2.9, 22.833333333333332, 3.033333333333333, 8.266666666666667, 20.1, 7.133333333333334, 2.8666666666666667, 9.566666666666666, 11.833333333333334, 8.4, 9.233333333333333, 6.666666666666667, 12.7, 17.666666666666668, 2.9, 5.5, 5.3, 2.9, 9.133333333333333, 13.033333333333333, 8.033333333333333, 6.833333333333333, 5.833333333333333, 5.466666666666667, 6.766666666666667, 7.933333333333334, 5.4, 4.266666666666667, 10.833333333333334, 20.233333333333334, 3.1333333333333333, 9.033333333333333, 8.066666666666666, 8.366666666666667, 9.133333333333333, 6.666666666666667, 7.2, 12.833333333333334, 4.8, 2.8666666666666667, 5.5, 20.4, 14.1, 17.7, 2.8666666666666667, 15.766666666666667, 15.4, 44.2, 20.133333333333333, 3.066666666666667, 5.6, 4.366666666666666, 5.466666666666667, 7.766666666666667, 3.5, 9.133333333333333, 25.4, 9.466666666666667, 17.9, 15.433333333333334, 10.833333333333334, 8.4, 20.4, 40.1, 5.566666666666666, 8.066666666666666, 2.933333333333333, 26.5, 32.766666666666666, 5.3, 17.8, 3.033333333333333, 11.7, 11.6, 5.3, 19.366666666666667, 13.033333333333333, 10.233333333333333, 7.8, 41.36666666666667, 4.233333333333333, 17.033333333333335, 3.4, 10.866666666666667, 14.1, 32.7, 8.1, 6.666666666666667, 10.5, 10.4, 5.566666666666666, 5.333333333333333, 6.0, 3.1333333333333333, 12.933333333333334, 15.4, 12.966666666666667, 4.733333333333333, 34.36666666666667, 8.066666666666666, 3.0, 23.966666666666665, 2.9, 6.633333333333334, 5.3, 6.7, 14.066666666666666, 3.033333333333333, 37.56666666666667, 7.9, 20.7, 45.13333333333333, 37.666666666666664, 5.466666666666667, 2.8666666666666667, 10.233333333333333, 10.7, 4.2, 10.833333333333334, 17.966666666666665, 12.833333333333334, 21.7, 10.4, 4.366666666666666, 2.933333333333333, 6.766666666666667, 6.666666666666667, 2.933333333333333, 21.366666666666667, 9.133333333333333, 14.4, 9.233333333333333, 7.2, 26.933333333333334, 5.866666666666666, 5.533333333333333, 15.766666666666667, 14.1, 12.9, 11.6, 7.966666666666667, 16.433333333333334, 5.3, 6.766666666666667, 16.633333333333333, 13.0]\n"
     ]
    }
   ],
   "source": [
    "print(time_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  0 score: -9.00  median score nan time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  100 score: 15.00  median score 18.80 time 1.70 median time 2.02 epsilon 0.01\n",
      "episode:  200 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  300 score: 31.00  median score 15.40 time 2.23 median time 1.80 epsilon 0.01\n",
      "episode:  400 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  500 score: 14.60  median score 18.60 time 1.67 median time 2.00 epsilon 0.01\n",
      "episode:  600 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  700 score: 63.40  median score 17.60 time 4.13 median time 1.95 epsilon 0.01\n",
      "episode:  800 score: 63.80  median score 14.60 time 4.17 median time 1.67 epsilon 0.01\n",
      "episode:  900 score: 19.00  median score 14.60 time 2.03 median time 1.67 epsilon 0.01\n",
      "episode:  1000 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  1100 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  1200 score: 80.60  median score 14.60 time 4.77 median time 1.67 epsilon 0.01\n",
      "episode:  1300 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  1400 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  1500 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  1600 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  1700 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  1800 score: 29.80  median score 14.60 time 2.13 median time 1.67 epsilon 0.01\n",
      "episode:  1900 score: 21.80  median score 14.60 time 2.17 median time 1.67 epsilon 0.01\n",
      "episode:  2000 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  2100 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  2200 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  2300 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  2400 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  2500 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  2600 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  2700 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  2800 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  2900 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  3000 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  3100 score: 2.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  3200 score: 14.60  median score 9.40 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  3300 score: 7.80  median score 9.40 time 1.10 median time 1.30 epsilon 0.01\n",
      "episode:  3400 score: 7.80  median score 10.20 time 1.10 median time 1.32 epsilon 0.01\n",
      "episode:  3500 score: 1.40  median score 10.20 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  3600 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  3700 score: -2.40  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  3800 score: 9.40  median score 10.20 time 1.23 median time 1.67 epsilon 0.01\n",
      "episode:  3900 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  4000 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  4100 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  4200 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  4300 score: 38.60  median score 14.60 time 2.87 median time 1.67 epsilon 0.01\n",
      "episode:  4400 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  4500 score: 15.80  median score 14.60 time 1.77 median time 1.67 epsilon 0.01\n",
      "episode:  4600 score: 19.00  median score 14.60 time 2.03 median time 1.67 epsilon 0.01\n",
      "episode:  4700 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  4800 score: 15.00  median score 14.60 time 1.70 median time 1.67 epsilon 0.01\n",
      "episode:  4900 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  5000 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  5100 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  5200 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  5300 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  5400 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  5500 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  5600 score: 30.20  median score 14.60 time 2.17 median time 1.67 epsilon 0.01\n",
      "episode:  5700 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  5800 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  5900 score: 7.80  median score 14.60 time 1.10 median time 1.67 epsilon 0.01\n",
      "episode:  6000 score: 12.80  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  6100 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  6200 score: 18.20  median score 14.60 time 1.97 median time 1.67 epsilon 0.01\n",
      "episode:  6300 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  6400 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  6500 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  6600 score: 14.60  median score 15.80 time 1.67 median time 1.80 epsilon 0.01\n",
      "episode:  6700 score: 15.00  median score 16.20 time 1.70 median time 1.80 epsilon 0.01\n",
      "episode:  6800 score: 15.80  median score 15.40 time 1.77 median time 1.75 epsilon 0.01\n",
      "episode:  6900 score: 7.80  median score 14.60 time 1.10 median time 1.67 epsilon 0.01\n",
      "episode:  7000 score: 18.60  median score 14.60 time 2.00 median time 1.67 epsilon 0.01\n",
      "episode:  7100 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  7200 score: 14.60  median score 14.80 time 1.67 median time 1.68 epsilon 0.01\n",
      "episode:  7300 score: 29.80  median score 15.80 time 2.13 median time 1.77 epsilon 0.01\n",
      "episode:  7400 score: 30.60  median score 17.60 time 2.20 median time 1.92 epsilon 0.01\n",
      "episode:  7500 score: 15.80  median score 15.80 time 1.77 median time 1.77 epsilon 0.01\n",
      "episode:  7600 score: 18.20  median score 16.40 time 1.97 median time 1.83 epsilon 0.01\n",
      "episode:  7700 score: 9.80  median score 15.80 time 1.27 median time 1.75 epsilon 0.01\n",
      "episode:  7800 score: 18.60  median score 14.60 time 2.00 median time 1.67 epsilon 0.01\n",
      "episode:  7900 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  8000 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  8100 score: 15.00  median score 14.60 time 1.70 median time 1.67 epsilon 0.01\n",
      "episode:  8200 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  8300 score: 17.40  median score 16.60 time 1.90 median time 1.83 epsilon 0.01\n",
      "episode:  8400 score: 17.00  median score 14.60 time 1.87 median time 1.70 epsilon 0.01\n",
      "episode:  8500 score: 14.60  median score 14.60 time 1.67 median time 1.68 epsilon 0.01\n",
      "episode:  8600 score: 14.60  median score 15.40 time 1.67 median time 1.77 epsilon 0.01\n",
      "episode:  8700 score: 31.00  median score 17.20 time 2.23 median time 1.90 epsilon 0.01\n",
      "episode:  8800 score: 17.80  median score 17.80 time 1.93 median time 1.93 epsilon 0.01\n",
      "episode:  8900 score: 14.60  median score 16.80 time 1.67 median time 1.83 epsilon 0.01\n",
      "episode:  9000 score: 17.40  median score 17.40 time 1.90 median time 1.90 epsilon 0.01\n",
      "episode:  9100 score: 19.00  median score 17.80 time 2.03 median time 1.93 epsilon 0.01\n",
      "episode:  9200 score: 30.20  median score 16.20 time 2.17 median time 1.80 epsilon 0.01\n",
      "episode:  9300 score: 31.00  median score 18.80 time 2.23 median time 2.02 epsilon 0.01\n",
      "episode:  9400 score: 17.00  median score 16.60 time 1.87 median time 1.83 epsilon 0.01\n",
      "episode:  9500 score: 8.20  median score 17.40 time 1.73 median time 1.90 epsilon 0.01\n",
      "episode:  9600 score: 29.00  median score 17.00 time 2.07 median time 1.87 epsilon 0.01\n",
      "episode:  9700 score: 7.80  median score 14.60 time 1.10 median time 1.67 epsilon 0.01\n",
      "episode:  9800 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  9900 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  10000 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  10100 score: 29.00  median score 15.00 time 2.07 median time 1.70 epsilon 0.01\n",
      "episode:  10200 score: 29.40  median score 15.40 time 2.10 median time 1.73 epsilon 0.01\n",
      "episode:  10300 score: 5.40  median score 17.80 time 1.67 median time 1.93 epsilon 0.01\n",
      "episode:  10400 score: 18.60  median score 17.80 time 2.00 median time 1.95 epsilon 0.01\n",
      "episode:  10500 score: 29.40  median score 19.00 time 2.10 median time 2.03 epsilon 0.01\n",
      "episode:  10600 score: 29.80  median score 18.00 time 2.13 median time 1.95 epsilon 0.01\n",
      "episode:  10700 score: 32.20  median score 18.60 time 2.33 median time 2.00 epsilon 0.01\n",
      "episode:  10800 score: 14.60  median score 19.00 time 1.67 median time 2.02 epsilon 0.01\n",
      "episode:  10900 score: 14.60  median score 18.60 time 1.67 median time 2.00 epsilon 0.01\n",
      "episode:  11000 score: 30.20  median score 17.00 time 2.17 median time 1.88 epsilon 0.01\n",
      "episode:  11100 score: 14.60  median score 17.00 time 1.67 median time 1.87 epsilon 0.01\n",
      "episode:  11200 score: 18.60  median score 18.20 time 2.00 median time 2.00 epsilon 0.01\n",
      "episode:  11300 score: 31.80  median score 18.40 time 2.30 median time 2.00 epsilon 0.01\n",
      "episode:  11400 score: 19.00  median score 18.60 time 2.03 median time 2.00 epsilon 0.01\n",
      "episode:  11500 score: 14.60  median score 17.40 time 1.67 median time 1.90 epsilon 0.01\n",
      "episode:  11600 score: 29.80  median score 19.00 time 2.13 median time 2.03 epsilon 0.01\n",
      "episode:  11700 score: 31.40  median score 19.00 time 2.27 median time 2.03 epsilon 0.01\n",
      "episode:  11800 score: 31.40  median score 18.20 time 2.27 median time 1.97 epsilon 0.01\n",
      "episode:  11900 score: 17.80  median score 18.80 time 1.93 median time 2.00 epsilon 0.01\n",
      "episode:  12000 score: 18.20  median score 17.80 time 1.97 median time 1.95 epsilon 0.01\n",
      "episode:  12100 score: 17.40  median score 14.60 time 1.90 median time 1.73 epsilon 0.01\n",
      "episode:  12200 score: 17.80  median score 17.60 time 1.93 median time 1.93 epsilon 0.01\n",
      "episode:  12300 score: 16.60  median score 18.20 time 1.83 median time 1.97 epsilon 0.01\n",
      "episode:  12400 score: 14.60  median score 18.60 time 1.67 median time 2.00 epsilon 0.01\n",
      "episode:  12500 score: -6.80  median score 17.80 time 1.67 median time 1.93 epsilon 0.01\n",
      "episode:  12600 score: 11.00  median score 16.80 time 1.67 median time 1.85 epsilon 0.01\n",
      "episode:  12700 score: 30.20  median score 16.20 time 2.17 median time 1.82 epsilon 0.01\n",
      "episode:  12800 score: 18.20  median score 17.20 time 1.97 median time 1.88 epsilon 0.01\n",
      "episode:  12900 score: 18.20  median score 16.20 time 1.97 median time 1.80 epsilon 0.01\n",
      "episode:  13000 score: 16.60  median score 17.80 time 1.83 median time 1.92 epsilon 0.01\n",
      "episode:  13100 score: 18.20  median score 18.60 time 1.97 median time 2.00 epsilon 0.01\n",
      "episode:  13200 score: 18.20  median score 19.00 time 1.97 median time 2.03 epsilon 0.01\n",
      "episode:  13300 score: 30.20  median score 17.80 time 2.17 median time 1.93 epsilon 0.01\n",
      "episode:  13400 score: 14.60  median score 14.60 time 1.67 median time 1.67 epsilon 0.01\n",
      "episode:  13500 score: 17.80  median score 18.20 time 1.93 median time 1.97 epsilon 0.01\n",
      "episode:  13600 score: 38.60  median score 24.00 time 2.87 median time 2.08 epsilon 0.01\n",
      "episode:  13700 score: 29.00  median score 30.60 time 2.07 median time 2.20 epsilon 0.01\n",
      "episode:  13800 score: 17.80  median score 30.20 time 1.93 median time 2.17 epsilon 0.01\n",
      "episode:  13900 score: 31.80  median score 29.40 time 2.30 median time 2.10 epsilon 0.01\n",
      "episode:  14000 score: 14.60  median score 18.20 time 1.67 median time 1.97 epsilon 0.01\n",
      "episode:  14100 score: 19.00  median score 29.40 time 2.03 median time 2.10 epsilon 0.01\n",
      "episode:  14200 score: 17.40  median score 19.00 time 1.90 median time 2.03 epsilon 0.01\n",
      "episode:  14300 score: 31.00  median score 29.40 time 2.23 median time 2.10 epsilon 0.01\n",
      "episode:  14400 score: 31.40  median score 17.60 time 2.27 median time 1.92 epsilon 0.01\n",
      "episode:  14500 score: 38.60  median score 18.20 time 2.87 median time 1.97 epsilon 0.01\n",
      "episode:  14600 score: 16.60  median score 18.60 time 1.83 median time 2.00 epsilon 0.01\n",
      "episode:  14700 score: 38.60  median score 29.40 time 2.87 median time 2.10 epsilon 0.01\n",
      "episode:  14800 score: 38.60  median score 31.40 time 2.87 median time 2.27 epsilon 0.01\n",
      "episode:  14900 score: 29.00  median score 31.00 time 2.07 median time 2.23 epsilon 0.01\n",
      "episode:  15000 score: 38.60  median score 30.80 time 2.87 median time 2.23 epsilon 0.01\n",
      "episode:  15100 score: 38.60  median score 30.60 time 2.87 median time 2.20 epsilon 0.01\n",
      "episode:  15200 score: 14.60  median score 30.60 time 1.67 median time 2.20 epsilon 0.01\n",
      "episode:  15300 score: 38.60  median score 30.00 time 2.87 median time 2.17 epsilon 0.01\n",
      "episode:  15400 score: 30.60  median score 30.60 time 2.20 median time 2.20 epsilon 0.01\n",
      "episode:  15500 score: 31.80  median score 29.20 time 2.30 median time 2.08 epsilon 0.01\n",
      "episode:  15600 score: 32.20  median score 30.60 time 2.33 median time 2.20 epsilon 0.01\n",
      "episode:  15700 score: 30.60  median score 29.80 time 2.20 median time 2.13 epsilon 0.01\n",
      "episode:  15800 score: 14.60  median score 29.40 time 1.67 median time 2.10 epsilon 0.01\n",
      "episode:  15900 score: 14.60  median score 18.80 time 1.67 median time 2.02 epsilon 0.01\n",
      "episode:  16000 score: 19.00  median score 31.00 time 2.87 median time 2.23 epsilon 0.01\n",
      "episode:  16100 score: 38.60  median score 31.40 time 2.87 median time 2.83 epsilon 0.01\n",
      "episode:  16200 score: 38.60  median score 19.00 time 2.87 median time 2.03 epsilon 0.01\n",
      "episode:  16300 score: 15.80  median score 20.60 time 1.77 median time 2.02 epsilon 0.01\n",
      "episode:  16400 score: 38.60  median score 30.40 time 2.87 median time 2.22 epsilon 0.01\n",
      "episode:  16500 score: 38.60  median score 34.40 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  16600 score: 38.60  median score 17.40 time 2.87 median time 1.90 epsilon 0.01\n",
      "episode:  16700 score: 14.60  median score 18.60 time 1.67 median time 2.03 epsilon 0.01\n",
      "episode:  16800 score: 29.00  median score 29.40 time 2.07 median time 2.17 epsilon 0.01\n",
      "episode:  16900 score: 14.60  median score 19.00 time 1.67 median time 2.03 epsilon 0.01\n",
      "episode:  17000 score: 15.80  median score 16.80 time 1.77 median time 1.85 epsilon 0.01\n",
      "episode:  17100 score: 29.80  median score 30.60 time 2.13 median time 2.23 epsilon 0.01\n",
      "episode:  17200 score: 38.60  median score 30.20 time 2.87 median time 2.23 epsilon 0.01\n",
      "episode:  17300 score: 17.80  median score 29.40 time 1.93 median time 2.13 epsilon 0.01\n",
      "episode:  17400 score: 30.60  median score 32.20 time 2.20 median time 2.60 epsilon 0.01\n",
      "episode:  17500 score: 29.80  median score 31.60 time 2.13 median time 2.28 epsilon 0.01\n",
      "episode:  17600 score: 38.60  median score 18.80 time 2.87 median time 2.02 epsilon 0.01\n",
      "episode:  17700 score: 39.00  median score 30.60 time 2.90 median time 2.20 epsilon 0.01\n",
      "episode:  17800 score: 38.60  median score 31.60 time 2.87 median time 2.28 epsilon 0.01\n",
      "episode:  17900 score: 38.60  median score 30.60 time 2.87 median time 2.23 epsilon 0.01\n",
      "episode:  18000 score: 38.60  median score 31.00 time 2.87 median time 2.23 epsilon 0.01\n",
      "episode:  18100 score: 38.60  median score 31.20 time 2.87 median time 2.25 epsilon 0.01\n",
      "episode:  18200 score: 38.60  median score 30.00 time 2.87 median time 2.18 epsilon 0.01\n",
      "episode:  18300 score: 38.60  median score 29.80 time 2.87 median time 2.15 epsilon 0.01\n",
      "episode:  18400 score: 30.60  median score 30.80 time 2.20 median time 2.20 epsilon 0.01\n",
      "episode:  18500 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  18600 score: 36.20  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  18700 score: 31.40  median score 32.40 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  18800 score: 17.80  median score 36.00 time 1.93 median time 2.87 epsilon 0.01\n",
      "episode:  18900 score: 38.60  median score 31.80 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  19000 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  19100 score: 14.60  median score 38.60 time 1.67 median time 2.87 epsilon 0.01\n",
      "episode:  19200 score: 15.00  median score 32.40 time 1.70 median time 2.87 epsilon 0.01\n",
      "episode:  19300 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  19400 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  19500 score: 25.00  median score 38.20 time 2.13 median time 2.87 epsilon 0.01\n",
      "episode:  19600 score: 29.00  median score 38.60 time 2.07 median time 2.87 epsilon 0.01\n",
      "episode:  19700 score: 38.60  median score 31.40 time 2.87 median time 2.83 epsilon 0.01\n",
      "episode:  19800 score: 2.20  median score 33.20 time 0.63 median time 2.58 epsilon 0.01\n",
      "episode:  19900 score: 19.00  median score 38.60 time 2.03 median time 2.87 epsilon 0.01\n",
      "episode:  20000 score: 15.40  median score 38.60 time 1.73 median time 2.87 epsilon 0.01\n",
      "episode:  20100 score: 38.60  median score 16.80 time 2.87 median time 1.87 epsilon 0.01\n",
      "episode:  20200 score: 14.60  median score 32.80 time 1.67 median time 2.28 epsilon 0.01\n",
      "episode:  20300 score: 38.60  median score 30.80 time 2.87 median time 2.25 epsilon 0.01\n",
      "episode:  20400 score: 29.40  median score 31.00 time 2.10 median time 2.23 epsilon 0.01\n",
      "episode:  20500 score: 8.20  median score 31.40 time 1.73 median time 2.27 epsilon 0.01\n",
      "episode:  20600 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  20700 score: 15.00  median score 38.60 time 1.70 median time 2.87 epsilon 0.01\n",
      "episode:  20800 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  20900 score: 14.60  median score 31.80 time 1.67 median time 2.30 epsilon 0.01\n",
      "episode:  21000 score: 22.20  median score 38.60 time 2.20 median time 2.87 epsilon 0.01\n",
      "episode:  21100 score: 18.60  median score 31.80 time 2.00 median time 2.82 epsilon 0.01\n",
      "episode:  21200 score: 38.60  median score 32.40 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  21300 score: 14.60  median score 38.60 time 1.67 median time 2.87 epsilon 0.01\n",
      "episode:  21400 score: 19.00  median score 35.80 time 2.03 median time 2.87 epsilon 0.01\n",
      "episode:  21500 score: 38.60  median score 31.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  21600 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  21700 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  21800 score: 30.20  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  21900 score: 38.60  median score 35.00 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  22000 score: 29.80  median score 38.60 time 2.13 median time 2.87 epsilon 0.01\n",
      "episode:  22100 score: 25.00  median score 23.60 time 2.23 median time 2.13 epsilon 0.01\n",
      "episode:  22200 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  22300 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  22400 score: 14.60  median score 38.60 time 1.67 median time 2.87 epsilon 0.01\n",
      "episode:  22500 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  22600 score: 14.60  median score 38.60 time 1.67 median time 2.87 epsilon 0.01\n",
      "episode:  22700 score: 38.60  median score 32.80 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  22800 score: 29.80  median score 38.60 time 2.13 median time 2.87 epsilon 0.01\n",
      "episode:  22900 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  23000 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  23100 score: 14.60  median score 38.60 time 1.67 median time 2.87 epsilon 0.01\n",
      "episode:  23200 score: 14.60  median score 30.20 time 1.67 median time 2.23 epsilon 0.01\n",
      "episode:  23300 score: 39.40  median score 38.60 time 2.93 median time 2.87 epsilon 0.01\n",
      "episode:  23400 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  23500 score: 29.40  median score 38.60 time 2.10 median time 2.87 epsilon 0.01\n",
      "episode:  23600 score: 31.00  median score 38.60 time 2.23 median time 2.87 epsilon 0.01\n",
      "episode:  23700 score: 14.60  median score 38.60 time 1.67 median time 2.87 epsilon 0.01\n",
      "episode:  23800 score: 39.00  median score 38.60 time 2.90 median time 2.87 epsilon 0.01\n",
      "episode:  23900 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  24000 score: 41.80  median score 38.60 time 3.13 median time 2.87 epsilon 0.01\n",
      "episode:  24100 score: 63.40  median score 38.60 time 4.13 median time 2.87 epsilon 0.01\n",
      "episode:  24200 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  24300 score: 33.80  median score 36.20 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  24400 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  24500 score: 63.40  median score 38.60 time 4.13 median time 2.87 epsilon 0.01\n",
      "episode:  24600 score: 29.80  median score 38.60 time 2.13 median time 2.87 epsilon 0.01\n",
      "episode:  24700 score: 41.40  median score 38.60 time 3.10 median time 2.87 epsilon 0.01\n",
      "episode:  24800 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  24900 score: 41.00  median score 38.60 time 3.07 median time 2.87 epsilon 0.01\n",
      "episode:  25000 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  25100 score: 63.40  median score 38.60 time 4.13 median time 2.87 epsilon 0.01\n",
      "episode:  25200 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  25300 score: 14.60  median score 38.60 time 1.67 median time 2.87 epsilon 0.01\n",
      "episode:  25400 score: 40.60  median score 38.60 time 3.03 median time 2.87 epsilon 0.01\n",
      "episode:  25500 score: 14.60  median score 38.60 time 1.67 median time 2.87 epsilon 0.01\n",
      "episode:  25600 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  25700 score: 2.20  median score 38.60 time 0.63 median time 2.87 epsilon 0.01\n",
      "episode:  25800 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  25900 score: 55.00  median score 33.00 time 4.13 median time 2.87 epsilon 0.01\n",
      "episode:  26000 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  26100 score: 31.00  median score 38.60 time 2.23 median time 2.87 epsilon 0.01\n",
      "episode:  26200 score: 41.40  median score 38.60 time 3.10 median time 2.87 epsilon 0.01\n",
      "episode:  26300 score: 38.60  median score 31.00 time 2.87 median time 2.23 epsilon 0.01\n",
      "episode:  26400 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  26500 score: 40.20  median score 38.60 time 3.00 median time 2.87 epsilon 0.01\n",
      "episode:  26600 score: 14.60  median score 38.60 time 1.67 median time 2.87 epsilon 0.01\n",
      "episode:  26700 score: 31.40  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  26800 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  26900 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  27000 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  27100 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  27200 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  27300 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  27400 score: 33.80  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  27500 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  27600 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  27700 score: 14.60  median score 38.60 time 1.67 median time 2.87 epsilon 0.01\n",
      "episode:  27800 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  27900 score: 28.20  median score 31.40 time 2.80 median time 2.87 epsilon 0.01\n",
      "episode:  28000 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  28100 score: 17.80  median score 38.60 time 1.93 median time 2.87 epsilon 0.01\n",
      "episode:  28200 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  28300 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  28400 score: 15.80  median score 38.60 time 1.77 median time 2.87 epsilon 0.01\n",
      "episode:  28500 score: 41.40  median score 38.60 time 3.10 median time 2.87 epsilon 0.01\n",
      "episode:  28600 score: 35.80  median score 38.60 time 3.03 median time 2.87 epsilon 0.01\n",
      "episode:  28700 score: 15.80  median score 38.60 time 1.77 median time 2.87 epsilon 0.01\n",
      "episode:  28800 score: 43.00  median score 38.60 time 3.23 median time 2.87 epsilon 0.01\n",
      "episode:  28900 score: 72.60  median score 38.60 time 4.70 median time 2.87 epsilon 0.01\n",
      "episode:  29000 score: 14.60  median score 38.60 time 1.67 median time 2.87 epsilon 0.01\n",
      "episode:  29100 score: 63.40  median score 38.60 time 4.13 median time 2.87 epsilon 0.01\n",
      "episode:  29200 score: 15.40  median score 38.60 time 1.73 median time 2.87 epsilon 0.01\n",
      "episode:  29300 score: 130.20  median score 38.80 time 7.80 median time 2.90 epsilon 0.01\n",
      "episode:  29400 score: 55.40  median score 38.60 time 3.47 median time 2.87 epsilon 0.01\n",
      "episode:  29500 score: 64.20  median score 87.00 time 4.20 median time 5.30 epsilon 0.01\n",
      "episode:  29600 score: 185.00  median score 63.20 time 10.27 median time 4.13 epsilon 0.01\n",
      "episode:  29700 score: 53.00  median score 41.80 time 3.27 median time 3.13 epsilon 0.01\n",
      "episode:  29800 score: 38.60  median score 79.80 time 2.87 median time 4.70 epsilon 0.01\n",
      "episode:  29900 score: 161.00  median score 58.40 time 9.07 median time 4.10 epsilon 0.01\n",
      "episode:  30000 score: 17.00  median score 63.00 time 1.87 median time 4.10 epsilon 0.01\n",
      "episode:  30100 score: 14.60  median score 63.00 time 1.67 median time 4.10 epsilon 0.01\n",
      "episode:  30200 score: 38.60  median score 39.60 time 2.87 median time 2.97 epsilon 0.01\n",
      "episode:  30300 score: 38.60  median score 39.80 time 2.87 median time 2.97 epsilon 0.01\n",
      "episode:  30400 score: 15.00  median score 38.60 time 1.70 median time 2.87 epsilon 0.01\n",
      "episode:  30500 score: 127.40  median score 38.60 time 7.07 median time 2.87 epsilon 0.01\n",
      "episode:  30600 score: 87.40  median score 39.40 time 5.33 median time 2.93 epsilon 0.01\n",
      "episode:  30700 score: 87.80  median score 38.60 time 5.37 median time 2.87 epsilon 0.01\n",
      "episode:  30800 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  30900 score: 87.80  median score 38.60 time 5.37 median time 2.88 epsilon 0.01\n",
      "episode:  31000 score: 5.80  median score 38.60 time 1.73 median time 2.90 epsilon 0.01\n",
      "episode:  31100 score: 15.40  median score 38.60 time 1.73 median time 2.87 epsilon 0.01\n",
      "episode:  31200 score: 2.20  median score 38.60 time 0.63 median time 2.87 epsilon 0.01\n",
      "episode:  31300 score: 65.00  median score 38.60 time 4.27 median time 2.87 epsilon 0.01\n",
      "episode:  31400 score: 184.60  median score 39.40 time 10.23 median time 2.93 epsilon 0.01\n",
      "episode:  31500 score: 38.60  median score 63.20 time 2.87 median time 4.10 epsilon 0.01\n",
      "episode:  31600 score: 30.60  median score 39.00 time 2.20 median time 2.92 epsilon 0.01\n",
      "episode:  31700 score: 32.20  median score 39.60 time 2.33 median time 2.97 epsilon 0.01\n",
      "episode:  31800 score: 31.80  median score 39.40 time 2.30 median time 3.12 epsilon 0.01\n",
      "episode:  31900 score: 209.40  median score 65.00 time 11.50 median time 4.40 epsilon 0.01\n",
      "episode:  32000 score: 53.00  median score 41.40 time 3.27 median time 3.10 epsilon 0.01\n",
      "episode:  32100 score: 112.20  median score 53.00 time 6.60 median time 3.35 epsilon 0.01\n",
      "episode:  32200 score: 38.60  median score 64.00 time 2.87 median time 4.13 epsilon 0.01\n",
      "episode:  32300 score: 15.00  median score 54.40 time 1.70 median time 3.40 epsilon 0.01\n",
      "episode:  32400 score: 101.80  median score 55.20 time 5.73 median time 3.82 epsilon 0.01\n",
      "episode:  32500 score: 111.80  median score 63.00 time 6.57 median time 4.10 epsilon 0.01\n",
      "episode:  32600 score: 67.80  median score 53.40 time 4.50 median time 3.37 epsilon 0.01\n",
      "episode:  32700 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  32800 score: 88.60  median score 40.40 time 5.43 median time 3.13 epsilon 0.01\n",
      "episode:  32900 score: 112.60  median score 39.80 time 6.63 median time 2.97 epsilon 0.01\n",
      "episode:  33000 score: 31.80  median score 39.40 time 2.30 median time 2.93 epsilon 0.01\n",
      "episode:  33100 score: 39.00  median score 39.40 time 2.90 median time 2.93 epsilon 0.01\n",
      "episode:  33200 score: 42.20  median score 53.60 time 3.17 median time 3.37 epsilon 0.01\n",
      "episode:  33300 score: 38.60  median score 63.60 time 2.87 median time 4.15 epsilon 0.01\n",
      "episode:  33400 score: 38.60  median score 53.60 time 2.87 median time 3.32 epsilon 0.01\n",
      "episode:  33500 score: 89.80  median score 63.40 time 5.53 median time 4.27 epsilon 0.01\n",
      "episode:  33600 score: 38.60  median score 73.80 time 2.87 median time 4.50 epsilon 0.01\n",
      "episode:  33700 score: 101.80  median score 63.00 time 5.73 median time 4.10 epsilon 0.01\n",
      "episode:  33800 score: 59.40  median score 74.40 time 4.20 median time 4.67 epsilon 0.01\n",
      "episode:  33900 score: 227.40  median score 80.20 time 12.20 median time 4.73 epsilon 0.01\n",
      "episode:  34000 score: 61.40  median score 66.00 time 4.37 median time 4.32 epsilon 0.01\n",
      "episode:  34100 score: 405.40  median score 69.60 time 21.43 median time 4.58 epsilon 0.01\n",
      "episode:  34200 score: 6.20  median score 38.80 time 1.67 median time 2.88 epsilon 0.01\n",
      "episode:  34300 score: 396.20  median score 67.00 time 20.67 median time 4.53 epsilon 0.01\n",
      "episode:  34400 score: 23.80  median score 55.00 time 2.23 median time 3.42 epsilon 0.01\n",
      "episode:  34500 score: 116.20  median score 79.40 time 6.93 median time 4.68 epsilon 0.01\n",
      "episode:  34600 score: 87.40  median score 67.20 time 5.33 median time 4.45 epsilon 0.01\n",
      "episode:  34700 score: 2.20  median score 44.60 time 0.63 median time 3.23 epsilon 0.01\n",
      "episode:  34800 score: 184.60  median score 78.20 time 10.23 median time 4.70 epsilon 0.01\n",
      "episode:  34900 score: 15.00  median score 87.80 time 1.70 median time 5.03 epsilon 0.01\n",
      "episode:  35000 score: 29.00  median score 38.60 time 2.07 median time 2.90 epsilon 0.01\n",
      "episode:  35100 score: 78.20  median score 38.00 time 4.57 median time 2.87 epsilon 0.01\n",
      "episode:  35200 score: 49.00  median score 23.00 time 3.53 median time 2.12 epsilon 0.01\n",
      "episode:  35300 score: 18.60  median score 16.80 time 2.00 median time 1.93 epsilon 0.01\n",
      "episode:  35400 score: 29.40  median score 29.80 time 2.10 median time 2.13 epsilon 0.01\n",
      "episode:  35500 score: 31.80  median score 30.40 time 2.30 median time 2.20 epsilon 0.01\n",
      "episode:  35600 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  35700 score: 53.40  median score 87.40 time 3.30 median time 5.33 epsilon 0.01\n",
      "episode:  35800 score: 38.60  median score 67.60 time 2.87 median time 4.48 epsilon 0.01\n",
      "episode:  35900 score: 14.60  median score 72.20 time 1.67 median time 4.47 epsilon 0.01\n",
      "episode:  36000 score: 201.00  median score 67.20 time 10.80 median time 4.47 epsilon 0.01\n",
      "episode:  36100 score: 15.00  median score 42.60 time 1.70 median time 3.08 epsilon 0.01\n",
      "episode:  36200 score: 102.60  median score 38.80 time 5.80 median time 2.90 epsilon 0.01\n",
      "episode:  36300 score: 87.00  median score 66.00 time 5.30 median time 4.35 epsilon 0.01\n",
      "episode:  36400 score: 38.60  median score 64.00 time 2.87 median time 4.13 epsilon 0.01\n",
      "episode:  36500 score: 14.20  median score 63.00 time 1.83 median time 4.10 epsilon 0.01\n",
      "episode:  36600 score: 523.80  median score 114.40 time 27.50 median time 6.82 epsilon 0.01\n",
      "episode:  36700 score: 209.40  median score 87.00 time 11.50 median time 5.30 epsilon 0.01\n",
      "episode:  36800 score: 87.40  median score 104.20 time 5.33 median time 5.92 epsilon 0.01\n",
      "episode:  36900 score: 1.60  median score 55.60 time 1.80 median time 3.45 epsilon 0.01\n",
      "episode:  37000 score: 63.00  median score 55.00 time 4.10 median time 3.47 epsilon 0.01\n",
      "episode:  37100 score: 63.00  median score 128.20 time 4.50 median time 7.13 epsilon 0.01\n",
      "episode:  37200 score: 15.80  median score 63.40 time 1.77 median time 4.13 epsilon 0.01\n",
      "episode:  37300 score: 212.60  median score 137.40 time 11.77 median time 8.03 epsilon 0.01\n",
      "episode:  37400 score: 404.60  median score 181.70 time 21.37 median time 10.27 epsilon 0.01\n",
      "episode:  37500 score: 39.40  median score 114.20 time 2.93 median time 6.63 epsilon 0.01\n",
      "episode:  37600 score: 38.60  median score 80.00 time 2.87 median time 4.72 epsilon 0.01\n",
      "episode:  37700 score: 15.00  median score 87.20 time 1.70 median time 5.32 epsilon 0.01\n",
      "episode:  37800 score: 103.80  median score 71.60 time 5.90 median time 4.72 epsilon 0.01\n",
      "episode:  37900 score: 63.00  median score 63.00 time 4.10 median time 4.10 epsilon 0.01\n",
      "episode:  38000 score: 97.00  median score 38.60 time 5.93 median time 2.87 epsilon 0.01\n",
      "episode:  38100 score: 17.00  median score 83.40 time 1.87 median time 4.73 epsilon 0.01\n",
      "episode:  38200 score: 66.20  median score 80.40 time 4.37 median time 4.75 epsilon 0.01\n",
      "episode:  38300 score: 56.20  median score 83.60 time 3.53 median time 5.02 epsilon 0.01\n",
      "episode:  38400 score: 148.20  median score 55.00 time 8.30 median time 3.43 epsilon 0.01\n",
      "episode:  38500 score: 178.20  median score 54.60 time 9.70 median time 3.42 epsilon 0.01\n",
      "episode:  38600 score: 63.00  median score 80.60 time 4.10 median time 4.75 epsilon 0.01\n",
      "episode:  38700 score: 43.00  median score 66.80 time 3.23 median time 4.42 epsilon 0.01\n",
      "episode:  38800 score: 87.00  median score 55.80 time 5.30 median time 3.80 epsilon 0.01\n",
      "episode:  38900 score: 42.20  median score 56.20 time 3.17 median time 3.53 epsilon 0.01\n",
      "episode:  39000 score: 57.40  median score 81.00 time 4.13 median time 4.80 epsilon 0.01\n",
      "episode:  39100 score: 63.40  median score 38.60 time 4.13 median time 2.87 epsilon 0.01\n",
      "episode:  39200 score: 87.00  median score 38.60 time 5.30 median time 2.87 epsilon 0.01\n",
      "episode:  39300 score: 81.00  median score 38.60 time 4.80 median time 2.87 epsilon 0.01\n",
      "episode:  39400 score: 8.20  median score 17.80 time 1.73 median time 1.92 epsilon 0.01\n",
      "episode:  39500 score: 333.80  median score 18.40 time 17.87 median time 2.00 epsilon 0.01\n",
      "episode:  39600 score: 2.20  median score 63.20 time 0.63 median time 4.13 epsilon 0.01\n",
      "episode:  39700 score: 18.20  median score 63.00 time 1.97 median time 4.10 epsilon 0.01\n",
      "episode:  39800 score: 14.60  median score 63.00 time 1.67 median time 4.12 epsilon 0.01\n",
      "episode:  39900 score: 19.00  median score 42.00 time 2.03 median time 3.17 epsilon 0.01\n",
      "episode:  40000 score: 63.00  median score 53.60 time 4.10 median time 3.40 epsilon 0.01\n",
      "episode:  40100 score: 60.60  median score 40.80 time 4.10 median time 3.08 epsilon 0.01\n",
      "episode:  40200 score: 19.80  median score 38.60 time 2.20 median time 2.88 epsilon 0.01\n",
      "episode:  40300 score: 111.00  median score 80.60 time 6.70 median time 5.32 epsilon 0.01\n",
      "episode:  40400 score: 18.20  median score 71.40 time 1.97 median time 4.68 epsilon 0.01\n",
      "episode:  40500 score: 56.20  median score 66.60 time 3.53 median time 4.43 epsilon 0.01\n",
      "episode:  40600 score: 15.40  median score 41.40 time 1.73 median time 3.10 epsilon 0.01\n",
      "episode:  40700 score: 185.40  median score 40.20 time 10.30 median time 3.10 epsilon 0.01\n",
      "episode:  40800 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  40900 score: 2.20  median score 38.60 time 0.63 median time 2.87 epsilon 0.01\n",
      "episode:  41000 score: 63.80  median score 63.20 time 4.17 median time 4.13 epsilon 0.01\n",
      "episode:  41100 score: 105.00  median score 60.20 time 6.00 median time 4.10 epsilon 0.01\n",
      "episode:  41200 score: 38.60  median score 67.40 time 2.87 median time 4.43 epsilon 0.01\n",
      "episode:  41300 score: 56.20  median score 78.20 time 3.53 median time 5.05 epsilon 0.01\n",
      "episode:  41400 score: 194.20  median score 56.00 time 10.93 median time 3.80 epsilon 0.01\n",
      "episode:  41500 score: 63.00  median score 38.60 time 4.10 median time 2.87 epsilon 0.01\n",
      "episode:  41600 score: 32.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  41700 score: 38.60  median score 38.60 time 2.87 median time 2.92 epsilon 0.01\n",
      "episode:  41800 score: 237.80  median score 41.40 time 13.07 median time 3.12 epsilon 0.01\n",
      "episode:  41900 score: 184.60  median score 63.00 time 10.23 median time 4.10 epsilon 0.01\n",
      "episode:  42000 score: 31.80  median score 39.00 time 2.30 median time 2.90 epsilon 0.01\n",
      "episode:  42100 score: 91.40  median score 40.80 time 5.67 median time 3.23 epsilon 0.01\n",
      "episode:  42200 score: 276.20  median score 67.00 time 14.67 median time 4.43 epsilon 0.01\n",
      "episode:  42300 score: 90.60  median score 41.80 time 5.60 median time 3.13 epsilon 0.01\n",
      "episode:  42400 score: 188.60  median score 42.40 time 10.57 median time 3.18 epsilon 0.01\n",
      "episode:  42500 score: 64.20  median score 66.20 time 4.20 median time 4.37 epsilon 0.01\n",
      "episode:  42600 score: 90.20  median score 66.20 time 5.57 median time 4.43 epsilon 0.01\n",
      "episode:  42700 score: 111.80  median score 65.00 time 6.57 median time 4.37 epsilon 0.01\n",
      "episode:  42800 score: 88.60  median score 66.20 time 5.63 median time 4.38 epsilon 0.01\n",
      "episode:  42900 score: 63.00  median score 56.60 time 4.10 median time 3.82 epsilon 0.01\n",
      "episode:  43000 score: 14.60  median score 41.80 time 1.67 median time 3.12 epsilon 0.01\n",
      "episode:  43100 score: 38.60  median score 39.20 time 2.87 median time 3.05 epsilon 0.01\n",
      "episode:  43200 score: 15.40  median score 41.80 time 1.93 median time 3.13 epsilon 0.01\n",
      "episode:  43300 score: 83.00  median score 38.60 time 5.67 median time 2.87 epsilon 0.01\n",
      "episode:  43400 score: 87.00  median score 40.80 time 5.30 median time 3.07 epsilon 0.01\n",
      "episode:  43500 score: 262.60  median score 41.40 time 14.33 median time 3.15 epsilon 0.01\n",
      "episode:  43600 score: 42.20  median score 83.00 time 3.17 median time 5.05 epsilon 0.01\n",
      "episode:  43700 score: 233.40  median score 90.80 time 12.70 median time 5.63 epsilon 0.01\n",
      "episode:  43800 score: 38.60  median score 42.60 time 2.87 median time 3.17 epsilon 0.01\n",
      "episode:  43900 score: 42.20  median score 54.40 time 3.17 median time 3.45 epsilon 0.01\n",
      "episode:  44000 score: 42.20  median score 39.00 time 3.17 median time 2.93 epsilon 0.01\n",
      "episode:  44100 score: 29.80  median score 38.60 time 2.13 median time 2.87 epsilon 0.01\n",
      "episode:  44200 score: 18.60  median score 41.80 time 2.00 median time 3.20 epsilon 0.01\n",
      "episode:  44300 score: 11.00  median score 65.20 time 1.67 median time 4.28 epsilon 0.01\n",
      "episode:  44400 score: 371.00  median score 67.20 time 19.37 median time 4.47 epsilon 0.01\n",
      "episode:  44500 score: 25.80  median score 42.20 time 2.30 median time 3.20 epsilon 0.01\n",
      "episode:  44600 score: 14.60  median score 38.60 time 1.67 median time 2.87 epsilon 0.01\n",
      "episode:  44700 score: 43.00  median score 35.40 time 3.23 median time 2.87 epsilon 0.01\n",
      "episode:  44800 score: 119.00  median score 41.40 time 7.27 median time 3.13 epsilon 0.01\n",
      "episode:  44900 score: 38.60  median score 35.80 time 2.87 median time 3.07 epsilon 0.01\n",
      "episode:  45000 score: 41.80  median score 41.80 time 3.13 median time 3.13 epsilon 0.01\n",
      "episode:  45100 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  45200 score: 55.80  median score 38.60 time 4.10 median time 3.02 epsilon 0.01\n",
      "episode:  45300 score: 43.00  median score 29.20 time 3.23 median time 2.87 epsilon 0.01\n",
      "episode:  45400 score: 22.60  median score 30.20 time 2.33 median time 3.10 epsilon 0.01\n",
      "episode:  45500 score: 107.80  median score 31.80 time 6.83 median time 2.87 epsilon 0.01\n",
      "episode:  45600 score: 31.00  median score 32.00 time 2.23 median time 2.33 epsilon 0.01\n",
      "episode:  45700 score: 135.80  median score 40.80 time 7.77 median time 3.10 epsilon 0.01\n",
      "episode:  45800 score: 38.60  median score 38.60 time 2.87 median time 2.87 epsilon 0.01\n",
      "episode:  45900 score: 18.60  median score 38.60 time 2.00 median time 2.87 epsilon 0.01\n",
      "episode:  46000 score: 42.20  median score 40.40 time 3.17 median time 3.10 epsilon 0.01\n",
      "episode:  46100 score: 23.00  median score 42.00 time 2.27 median time 3.13 epsilon 0.01\n",
      "episode:  46200 score: 41.40  median score 38.60 time 3.10 median time 2.92 epsilon 0.01\n",
      "episode:  46300 score: 54.60  median score 41.40 time 3.40 median time 3.15 epsilon 0.01\n",
      "episode:  46400 score: 56.20  median score 31.80 time 3.53 median time 2.87 epsilon 0.01\n",
      "episode:  46500 score: 261.80  median score 79.40 time 14.27 median time 4.70 epsilon 0.01\n",
      "episode:  46600 score: 87.00  median score 42.60 time 5.30 median time 3.20 epsilon 0.01\n",
      "episode:  46700 score: 16.60  median score 40.60 time 1.83 median time 3.03 epsilon 0.01\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#run the DQL model training.\n",
    "agent = Agent()\n",
    "train(agent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results\n",
    "\n",
    "The DQL agent was able to learn how to play flappy bird up to the level of a below-average human within 50 000 games. It had periods of outlying success (passing 10 pipes or more) but generally the DQL model could only consistently pass 2 - 3 pipes when trained adequately. (20 - 80k games). Overally learning progress was slow but steady, however the model would often get stuck in local min/maxes for thousands of games (for example flapping into the ceiling and eventually dying yield better results than flopping to the ground, but is in no way a good result!)\n",
    "\n",
    "A good explanation of why it is so attracted to local maximas is due to the nature of exploration in the game and the way the game is structured. Flappy bird is a game where there are only two actions - one of which doesn't affect the game a lot (not flapping) and the other is extremely disruptive to the progress of the game (flapping). Additionally, the pipes are directly vertical which means that there is no difference in reward between a near miss and a far miss, making local maximas hard to escape and random movements often ineffective.\n",
    "\n",
    "To combat this, the team implemented behaviour cloning. It allowed a human to play a few games, before having the model train on the human's actions. This was an extremely successful method as it gave the model \"experiences\" which inform it that the local maxima is not the global maxima. In addition, the local maxima problem was so severe that the authors had to create a negative reward zone near the ceiling of the game to disincentivise that behaviour.\n",
    "\n",
    "Discussion\n",
    "\n",
    "Overall, DQL proved its reliability in succeeding (eventually) in solving problems. However, its reliance on the reward system means that in games such as flappy bird where successful exploration by random action is very difficult and rare means that effectively it creates its own \"dissapearing gradient\" problem. It would achieve a successful run - only for it to be drowned out by thousands of unsuccessful runs to local maximas which in turn doesn't teach the model on how to make a successful run, generating thousands more of unsuccessful runs to drown out successful ones and continuing the loop. Accordingly, the authors implemented a \"priority replay\" system where runs that were considered better than normal was more heavily studied and trained on by the agent - overcoming the vicious local maxima loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison vs NEAT\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
